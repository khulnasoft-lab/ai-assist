diff --git a/ai_gateway/__init__.py b/ai_gateway/__init__.py
index 033437b..fa66ae9 100644
--- a/ai_gateway/__init__.py
+++ b/ai_gateway/__init__.py
@@ -1,4 +1,4 @@
 # flake8: noqa
 
-from ai_gateway import api, auth, container, experimentation, main, models
+from ai_gateway import api, auth, container, experimentation, models
 from ai_gateway.config import *
diff --git a/ai_gateway/api/middleware.py b/ai_gateway/api/middleware.py
index 38ede02..5210549 100644
--- a/ai_gateway/api/middleware.py
+++ b/ai_gateway/api/middleware.py
@@ -14,6 +14,7 @@ from starlette.authentication import (
     BaseUser,
     HTTPConnection,
 )
+from starlette.concurrency import iterate_in_threadpool
 from starlette.middleware import Middleware
 from starlette.middleware.authentication import (
     AuthenticationBackend,
@@ -45,7 +46,6 @@ X_GITLAB_INSTANCE_ID_HEADER = "X-Gitlab-Instance-Id"
 X_GITLAB_GLOBAL_USER_ID_HEADER = "X-Gitlab-Global-User-Id"
 X_GITLAB_HOST_NAME_HEADER = "X-Gitlab-Host-Name"
 X_GITLAB_SAAS_NAMESPACE_IDS_HEADER = "X-Gitlab-Saas-Namespace-Ids"
-X_GITLAB_SAAS_DUO_PRO_NAMESPACE_IDS_HEADER = "X-Gitlab-Saas-Duo-Pro-Namespace-Ids"
 
 
 class _PathResolver:
@@ -126,6 +126,17 @@ class MiddlewareLogRequest(Middleware):
                 http_method = request.method
                 http_version = request.scope["http_version"]
 
+                if 400 <= status_code < 500:
+                    # StreamingResponse is received from the MiddlewareAuthentication, so
+                    # we need to read the response ourselves.
+                    response_body = [
+                        section async for section in response.body_iterator
+                    ]
+                    response.body_iterator = iterate_in_threadpool(iter(response_body))
+                    structlog.contextvars.bind_contextvars(
+                        response_body=response_body[0].decode()
+                    )
+
                 fields = dict(
                     url=str(request.url),
                     path=url,
@@ -143,9 +154,6 @@ class MiddlewareLogRequest(Middleware):
                         X_GITLAB_GLOBAL_USER_ID_HEADER
                     ),
                     gitlab_host_name=request.headers.get(X_GITLAB_HOST_NAME_HEADER),
-                    gitlab_saas_duo_pro_namespace_ids=request.headers.get(
-                        X_GITLAB_SAAS_DUO_PRO_NAMESPACE_IDS_HEADER
-                    ),
                     gitlab_saas_namespace_ids=request.headers.get(
                         X_GITLAB_SAAS_NAMESPACE_IDS_HEADER
                     ),
@@ -244,7 +252,6 @@ class MiddlewareAuthentication(Middleware):
     @staticmethod
     def on_auth_error(_: Request, e: Exception):
         content = jsonable_encoder({"error": str(e)})
-        context["auth_error_details"] = str(e)
         return JSONResponse(status_code=401, content=content)
 
     def __init__(
diff --git a/ai_gateway/api/server.py b/ai_gateway/api/server.py
index b4b8687..4e81686 100644
--- a/ai_gateway/api/server.py
+++ b/ai_gateway/api/server.py
@@ -1,6 +1,11 @@
-from dependency_injector.wiring import Provide, inject
-from fastapi import FastAPI
+import asyncio
+import logging
+from contextlib import asynccontextmanager
+
+from fastapi import APIRouter, FastAPI
 from fastapi.middleware.cors import CORSMiddleware
+from prometheus_client import start_http_server
+from prometheus_fastapi_instrumentator import Instrumentator, metrics
 from starlette.middleware import Middleware
 from starlette_context.middleware import RawContextMiddleware
 
@@ -13,53 +18,116 @@ from ai_gateway.api.monitoring import router as http_monitoring_router
 from ai_gateway.api.v1 import api_router as http_api_router_v1
 from ai_gateway.api.v2 import api_router as http_api_router_v2
 from ai_gateway.api.v3 import api_router as http_api_router_v3
+from ai_gateway.auth import GitLabOidcProvider
+from ai_gateway.config import Config
 from ai_gateway.container import ContainerApplication
+from ai_gateway.instrumentators.threads import monitor_threads
+from ai_gateway.profiling import setup_profiling
+from ai_gateway.structured_logging import setup_logging
 
 __all__ = [
     "create_fast_api_server",
 ]
 
+_PROBS_ENDPOINTS = ["/monitoring/healthz", "/metrics"]
 
-@inject
-def create_fast_api_server(
-    config: dict = Provide[ContainerApplication.config.fastapi],
-    auth_middleware: MiddlewareAuthentication = Provide[
-        ContainerApplication.fastapi.auth_middleware
-    ],
-    log_middleware: MiddlewareLogRequest = Provide[
-        ContainerApplication.fastapi.log_middleware
-    ],
-    telemetry_middleware: MiddlewareModelTelemetry = Provide[
-        ContainerApplication.fastapi.telemetry_middleware
-    ],
-):
-    context_middleware = Middleware(RawContextMiddleware)
-    cors_middleware = Middleware(
-        CORSMiddleware,
-        allow_origins=["*"],
-        allow_methods=["POST"],
-        allow_headers=["*"],
-    )
+
+def create_fast_api_server(config: Config):
+    @asynccontextmanager
+    async def lifespan(app: FastAPI):
+        container_application = ContainerApplication()
+        container_application.config.from_dict(config.model_dump())
+        container_application.init_resources()
+
+        if config.instrumentator.thread_monitoring_enabled:
+            loop = asyncio.get_running_loop()
+            loop.create_task(
+                monitor_threads(
+                    loop, interval=config.instrumentator.thread_monitoring_interval
+                )
+            )
+
+        # https://github.com/trallnag/prometheus-fastapi-instrumentator/issues/10
+        log = logging.getLogger("uvicorn.error")
+        log.info(
+            "Metrics HTTP server running on http://%s:%d",
+            config.fastapi.metrics_host,
+            config.fastapi.metrics_port,
+        )
+        start_http_server(
+            addr=config.fastapi.metrics_host, port=config.fastapi.metrics_port
+        )
+
+        yield
+
+        container_application.shutdown_resources()
 
     fastapi_app = FastAPI(
         title="GitLab Code Suggestions",
         description="GitLab Code Suggestions API to serve code completion predictions",
-        openapi_url=config["openapi_url"],
-        docs_url=config["docs_url"],
-        redoc_url=config["redoc_url"],
+        openapi_url=config.fastapi.openapi_url,
+        docs_url=config.fastapi.docs_url,
+        redoc_url=config.fastapi.redoc_url,
         swagger_ui_parameters={"defaultModelsExpandDepth": -1},
+        lifespan=lifespan,
         middleware=[
-            context_middleware,
-            cors_middleware,
-            log_middleware,
-            auth_middleware,
-            telemetry_middleware,
+            Middleware(RawContextMiddleware),
+            Middleware(
+                CORSMiddleware,
+                allow_origins=["*"],
+                allow_methods=["POST"],
+                allow_headers=["*"],
+            ),
+            MiddlewareLogRequest(skip_endpoints=_PROBS_ENDPOINTS),
+            MiddlewareAuthentication(
+                GitLabOidcProvider(
+                    oidc_providers={
+                        "Gitlab": config.gitlab_url,
+                        "CustomersDot": config.customer_portal_url,
+                    }
+                ),
+                bypass_auth=config.auth.bypass_external,
+                skip_endpoints=_PROBS_ENDPOINTS,
+            ),
+            MiddlewareModelTelemetry(skip_endpoints=_PROBS_ENDPOINTS),
         ],
     )
 
-    fastapi_app.include_router(http_api_router_v1, prefix="/v1")
-    fastapi_app.include_router(http_api_router_v2, prefix="/v2")
-    fastapi_app.include_router(http_api_router_v3, prefix="/v3")
-    fastapi_app.include_router(http_monitoring_router)
+    setup_router(fastapi_app)
+    setup_logging(fastapi_app, config.logging)
+    setup_prometheus_fastapi_instrumentator(fastapi_app)
+    setup_profiling(config.google_cloud_profiler)
 
     return fastapi_app
+
+
+def setup_router(app: FastAPI):
+    sub_router = APIRouter()
+    sub_router.include_router(http_api_router_v1, prefix="/v1")
+    sub_router.include_router(http_api_router_v2, prefix="/v2")
+    sub_router.include_router(http_api_router_v3, prefix="/v3")
+    sub_router.include_router(http_monitoring_router)
+
+    app.include_router(sub_router)
+    # Include alias routes for Cloudflare LoadBalancer cloud.gitlab.com
+    # https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/ai-assist/-/issues/358
+    app.include_router(sub_router, prefix="/ai")
+
+
+def setup_prometheus_fastapi_instrumentator(app: FastAPI):
+    instrumentator = Instrumentator(
+        should_group_status_codes=True,
+        should_ignore_untemplated=True,
+        should_respect_env_var=False,
+        should_instrument_requests_inprogress=False,
+        excluded_handlers=_PROBS_ENDPOINTS,
+    )
+    instrumentator.add(
+        metrics.latency(
+            should_include_handler=True,
+            should_include_method=True,
+            should_include_status=True,
+            buckets=(0.5, 1, 2.5, 5, 10, 30, 60),
+        )
+    )
+    instrumentator.instrument(app)
diff --git a/ai_gateway/api/v1/chat/agent.py b/ai_gateway/api/v1/chat/agent.py
index 4dd416d..f1e3276 100644
--- a/ai_gateway/api/v1/chat/agent.py
+++ b/ai_gateway/api/v1/chat/agent.py
@@ -1,8 +1,9 @@
 from time import time
-from typing import AsyncIterator, Union
+from typing import AsyncIterator
 
 import structlog
-from dependency_injector.providers import FactoryAggregate
+from dependency_injector.providers import Factory
+from dependency_injector.wiring import Provide, inject
 from fastapi import APIRouter, Depends, HTTPException, Request, status
 from starlette.authentication import requires
 
@@ -11,19 +12,16 @@ from ai_gateway.api.v1.chat.typing import (
     ChatRequest,
     ChatResponse,
     ChatResponseMetadata,
-    PromptPayload,
     StreamChatResponse,
 )
-from ai_gateway.async_dependency_resolver import (
-    get_chat_anthropic_claude_factory_provider,
-)
+from ai_gateway.container import ContainerApplication
 from ai_gateway.models import (
     AnthropicAPIConnectionError,
     AnthropicAPIStatusError,
     AnthropicAPITimeoutError,
+    AnthropicModel,
     KindModelProvider,
     TextGenModelChunk,
-    TextGenModelOutput,
 )
 from ai_gateway.tracking import log_exception
 
@@ -39,19 +37,29 @@ router = APIRouter()
 @router.post("/agent", response_model=ChatResponse, status_code=status.HTTP_200_OK)
 @requires("duo_chat")
 @feature_category("duo_chat")
+@inject
 async def chat(
     request: Request,
     chat_request: ChatRequest,
-    anthropic_claude_factory: FactoryAggregate = Depends(
-        get_chat_anthropic_claude_factory_provider
+    anthropic_claude_factory: Factory[AnthropicModel] = Depends(
+        Provide[ContainerApplication.chat.anthropic_claude_factory.provider]
     ),
 ):
     prompt_component = chat_request.prompt_components[0]
     payload = prompt_component.payload
 
+    anthropic_opts = {"name": payload.model}
+
+    if payload.params:
+        anthropic_opts.update(payload.params.dict())
+
+    model = anthropic_claude_factory(**anthropic_opts)
+
     try:
-        completion = await _generate_completion(
-            anthropic_claude_factory, payload, stream=chat_request.stream
+        completion = await model.generate(
+            prefix=payload.content,
+            _suffix="",
+            stream=chat_request.stream,
         )
 
         if isinstance(completion, AsyncIterator):
@@ -84,35 +92,6 @@ async def chat(
         )
 
 
-async def _generate_completion(
-    anthropic_claude_factory: FactoryAggregate,
-    prompt: PromptPayload,
-    stream: bool = False,
-) -> Union[TextGenModelOutput, AsyncIterator[TextGenModelChunk]]:
-    opts = prompt.params.dict() if prompt.params else {}
-
-    if isinstance(prompt.content, str):
-        factory_type = (
-            "llm"  # retrieve `AnthropicModel` from the FactoryAggregate object
-        )
-        opts.update({"prefix": prompt.content, "stream": stream})
-    else:  # otherwise, `list[Message]`
-        factory_type = (
-            "chat"  # retrieve `AnthropicChatModel` from the FactoryAggregate object
-        )
-        opts.update({"messages": prompt.content, "stream": stream})
-
-        # Hack: Anthropic renamed the `max_tokens_to_sample` arg to `max_tokens` for the new Message API
-        if max_tokens := opts.pop("max_tokens_to_sample", None):
-            opts["max_tokens"] = max_tokens
-
-    completion = await anthropic_claude_factory(
-        factory_type, name=prompt.model
-    ).generate(**opts)
-
-    return completion
-
-
 async def _handle_stream(
     response: AsyncIterator[TextGenModelChunk],
 ) -> StreamChatResponse:
diff --git a/ai_gateway/api/v1/chat/typing.py b/ai_gateway/api/v1/chat/typing.py
index 316932c..b01e72e 100644
--- a/ai_gateway/api/v1/chat/typing.py
+++ b/ai_gateway/api/v1/chat/typing.py
@@ -1,9 +1,9 @@
-from typing import Annotated, List, Literal, Optional, Union
+from typing import Annotated, List, Literal, Optional
 
 from fastapi.responses import StreamingResponse
 from pydantic import BaseModel, Field, StringConstraints
 
-from ai_gateway.models import KindAnthropicModel, KindModelProvider, Message
+from ai_gateway.models import KindAnthropicModel, KindModelProvider
 
 __all__ = [
     "ChatRequest",
@@ -31,13 +31,10 @@ class AnthropicParams(BaseModel):
 
 
 class PromptPayload(BaseModel):
-    content: Union[
-        Annotated[str, StringConstraints(max_length=400000)],
-        Annotated[list[Message], Field(min_length=1, max_length=100)],
-    ]
-    provider: Optional[Literal[KindModelProvider.ANTHROPIC]] = (
-        None  # We only support and expect Anthropic for now
-    )
+    content: Annotated[str, StringConstraints(max_length=400000)]
+    provider: Optional[
+        Literal[KindModelProvider.ANTHROPIC]
+    ] = None  # We only support and expect Anthropic for now
     model: Optional[KindAnthropicModel] = KindAnthropicModel.CLAUDE_2_0
     params: Optional[AnthropicParams] = None
 
diff --git a/ai_gateway/api/v1/x_ray/libraries.py b/ai_gateway/api/v1/x_ray/libraries.py
index 91ea13f..17dc064 100644
--- a/ai_gateway/api/v1/x_ray/libraries.py
+++ b/ai_gateway/api/v1/x_ray/libraries.py
@@ -1,10 +1,11 @@
 import structlog
+from dependency_injector.wiring import Provide, inject
 from fastapi import APIRouter, Depends, Request
 from starlette.authentication import requires
 
 from ai_gateway.api.feature_category import feature_category
 from ai_gateway.api.v1.x_ray.typing import XRayRequest, XRayResponse
-from ai_gateway.async_dependency_resolver import get_x_ray_anthropic_claude
+from ai_gateway.container import ContainerApplication
 from ai_gateway.models import (
     AnthropicAPIConnectionError,
     AnthropicAPIStatusError,
@@ -24,10 +25,13 @@ router = APIRouter()
 @router.post("/libraries", response_model=XRayResponse)
 @requires("code_suggestions")
 @feature_category("code_suggestions")
+@inject
 async def libraries(
     request: Request,
     payload: XRayRequest,
-    model: AnthropicModel = Depends(get_x_ray_anthropic_claude),
+    model: AnthropicModel = Depends(
+        Provide[ContainerApplication.x_ray.anthropic_claude]
+    ),
 ):
     package_file_prompt = payload.prompt_components[0].payload
 
diff --git a/ai_gateway/api/v2/code/completions.py b/ai_gateway/api/v2/code/completions.py
index 21de81d..9f693e0 100644
--- a/ai_gateway/api/v2/code/completions.py
+++ b/ai_gateway/api/v2/code/completions.py
@@ -4,6 +4,7 @@ from typing import Annotated, AsyncIterator, Union
 import anthropic
 import structlog
 from dependency_injector.providers import Factory
+from dependency_injector.wiring import Provide, inject
 from fastapi import APIRouter, Body, Depends, Request
 from starlette.datastructures import CommaSeparatedStrings
 
@@ -13,7 +14,6 @@ from ai_gateway.api.middleware import (
     X_GITLAB_HOST_NAME_HEADER,
     X_GITLAB_INSTANCE_ID_HEADER,
     X_GITLAB_REALM_HEADER,
-    X_GITLAB_SAAS_DUO_PRO_NAMESPACE_IDS_HEADER,
     X_GITLAB_SAAS_NAMESPACE_IDS_HEADER,
 )
 from ai_gateway.api.v2.code.typing import (
@@ -21,19 +21,10 @@ from ai_gateway.api.v2.code.typing import (
     CompletionsRequestV2,
     GenerationsRequestV1,
     GenerationsRequestV2,
-    GenerationsRequestV3,
     StreamSuggestionsResponse,
     SuggestionsRequest,
     SuggestionsResponse,
 )
-from ai_gateway.async_dependency_resolver import (
-    get_code_suggestions_completions_anthropic_provider,
-    get_code_suggestions_completions_vertex_legacy_provider,
-    get_code_suggestions_generations_anthropic_chat_factory_provider,
-    get_code_suggestions_generations_anthropic_factory_provider,
-    get_code_suggestions_generations_vertex_provider,
-    get_snowplow_instrumentator,
-)
 from ai_gateway.auth.authentication import requires
 from ai_gateway.code_suggestions import (
     CodeCompletions,
@@ -42,9 +33,9 @@ from ai_gateway.code_suggestions import (
     CodeSuggestionsChunk,
 )
 from ai_gateway.code_suggestions.processing.ops import lang_from_filename
+from ai_gateway.container import ContainerApplication
 from ai_gateway.instrumentators.base import TelemetryInstrumentator
 from ai_gateway.models import KindAnthropicModel, KindModelProvider
-from ai_gateway.tracking import RequestCount, SnowplowEvent, SnowplowEventContext
 from ai_gateway.tracking.errors import log_exception
 from ai_gateway.tracking.instrumentator import SnowplowInstrumentator
 
@@ -63,7 +54,7 @@ CompletionsRequestWithVersion = Annotated[
 ]
 
 GenerationsRequestWithVersion = Annotated[
-    Union[GenerationsRequestV1, GenerationsRequestV2, GenerationsRequestV3],
+    Union[GenerationsRequestV1, GenerationsRequestV2],
     Body(discriminator="prompt_version"),
 ]
 
@@ -72,23 +63,24 @@ GenerationsRequestWithVersion = Annotated[
 @router.post("/code/completions")
 @requires("code_suggestions")
 @feature_category("code_suggestions")
+@inject
 async def completions(
     request: Request,
     payload: CompletionsRequestWithVersion,
     completions_legacy_factory: Factory[CodeCompletionsLegacy] = Depends(
-        get_code_suggestions_completions_vertex_legacy_provider
+        Provide[
+            ContainerApplication.code_suggestions.completions.vertex_legacy.provider
+        ]
     ),
     completions_anthropic_factory: Factory[CodeCompletions] = Depends(
-        get_code_suggestions_completions_anthropic_provider
+        Provide[ContainerApplication.code_suggestions.completions.anthropic.provider]
     ),
     snowplow_instrumentator: SnowplowInstrumentator = Depends(
-        get_snowplow_instrumentator
+        Provide[ContainerApplication.snowplow.instrumentator]
     ),
 ):
     try:
-        snowplow_instrumentator.watch(
-            _suggestion_requested_snowplow_event(request, payload)
-        )
+        track_snowplow_event(request, payload, snowplow_instrumentator)
     except Exception as e:
         log_exception(e)
 
@@ -147,26 +139,24 @@ async def completions(
 @router.post("/code/generations")
 @requires("code_suggestions")
 @feature_category("code_suggestions")
+@inject
 async def generations(
     request: Request,
     payload: GenerationsRequestWithVersion,
     generations_vertex_factory: Factory[CodeGenerations] = Depends(
-        get_code_suggestions_generations_vertex_provider
+        Provide[ContainerApplication.code_suggestions.generations.vertex.provider]
     ),
     generations_anthropic_factory: Factory[CodeGenerations] = Depends(
-        get_code_suggestions_generations_anthropic_factory_provider
-    ),
-    generations_anthropic_chat_factory: Factory[CodeGenerations] = Depends(
-        get_code_suggestions_generations_anthropic_chat_factory_provider
+        Provide[
+            ContainerApplication.code_suggestions.generations.anthropic_factory.provider
+        ]
     ),
     snowplow_instrumentator: SnowplowInstrumentator = Depends(
-        get_snowplow_instrumentator
+        Provide[ContainerApplication.snowplow.instrumentator]
     ),
 ):
     try:
-        snowplow_instrumentator.watch(
-            _suggestion_requested_snowplow_event(request, payload)
-        )
+        track_snowplow_event(request, payload, snowplow_instrumentator)
     except Exception as e:
         log_exception(e)
 
@@ -180,20 +170,14 @@ async def generations(
     )
 
     if payload.model_provider == KindModelProvider.ANTHROPIC:
-        if payload.prompt_version == 3:
-            code_generations = _resolve_code_generations_anthropic_chat(
-                payload,
-                generations_anthropic_chat_factory,
-            )
-        else:
-            code_generations = _resolve_code_generations_anthropic(
-                payload,
-                generations_anthropic_factory,
-            )
+        code_generations = _resolve_code_generations_anthropic(
+            payload,
+            generations_anthropic_factory,
+        )
     else:
         code_generations = generations_vertex_factory()
 
-    if payload.prompt_version == 2 or payload.prompt_version == 3:
+    if payload.prompt_version == 2:
         code_generations.with_prompt_prepared(payload.prompt)
 
     with TelemetryInstrumentator().watch(payload.telemetry):
@@ -241,24 +225,15 @@ def _resolve_code_generations_anthropic(
     )
 
 
-def _resolve_code_generations_anthropic_chat(
-    payload: SuggestionsRequest,
-    generations_anthropic_chat_factory: Factory[CodeGenerations],
-) -> CodeGenerations:
-    return generations_anthropic_chat_factory(
-        model__name=payload.model_name,
-        model__stop_sequences=["</new_code>"],
-    )
-
-
 def _suggestion_choices(text: str) -> list:
     return [SuggestionsResponse.Choice(text=text)] if text else []
 
 
-def _suggestion_requested_snowplow_event(
+def track_snowplow_event(
     req: Request,
     payload: SuggestionsRequest,
-) -> SnowplowEvent:
+    snowplow_instrumentator: SnowplowInstrumentator,
+):
     language = lang_from_filename(payload.current_file.file_name) or ""
     if language:
         language = language.name.lower()
@@ -269,40 +244,21 @@ def _suggestion_requested_snowplow_event(
     if not gitlab_realm and req.user and req.user.claims:
         gitlab_realm = req.user.claims.gitlab_realm
 
-    request_counts = [
-        RequestCount(
-            requests=stats.requests,
-            accepts=stats.accepts,
-            errors=stats.errors,
-            lang=stats.lang,
-            model_engine=stats.model_engine,
-            model_name=stats.model_name,
-        )
-        for stats in payload.telemetry
-    ]
-
-    return SnowplowEvent(
-        context=SnowplowEventContext(
-            request_counts=request_counts,
-            prefix_length=len(payload.current_file.content_above_cursor),
-            suffix_length=len(payload.current_file.content_below_cursor),
-            language=language,
-            user_agent=req.headers.get("User-Agent", ""),
-            gitlab_realm=gitlab_realm if gitlab_realm else "",
-            gitlab_instance_id=req.headers.get(X_GITLAB_INSTANCE_ID_HEADER, ""),
-            gitlab_global_user_id=req.headers.get(X_GITLAB_GLOBAL_USER_ID_HEADER, ""),
-            gitlab_host_name=req.headers.get(X_GITLAB_HOST_NAME_HEADER, ""),
-            gitlab_saas_duo_pro_namespace_ids=list(
-                CommaSeparatedStrings(
-                    req.headers.get(X_GITLAB_SAAS_DUO_PRO_NAMESPACE_IDS_HEADER, "")
-                )
-            ),
-            gitlab_saas_namespace_ids=list(
-                CommaSeparatedStrings(
-                    req.headers.get(X_GITLAB_SAAS_NAMESPACE_IDS_HEADER, "")
-                )
-            ),
-        )
+    snowplow_instrumentator.watch(
+        telemetry=payload.telemetry,
+        prefix_length=len(payload.current_file.content_above_cursor),
+        suffix_length=len(payload.current_file.content_below_cursor),
+        language=language,
+        user_agent=req.headers.get("User-Agent", ""),
+        gitlab_realm=gitlab_realm if gitlab_realm else "",
+        gitlab_instance_id=req.headers.get(X_GITLAB_INSTANCE_ID_HEADER, ""),
+        gitlab_global_user_id=req.headers.get(X_GITLAB_GLOBAL_USER_ID_HEADER, ""),
+        gitlab_host_name=req.headers.get(X_GITLAB_HOST_NAME_HEADER, ""),
+        gitlab_saas_namespace_ids=list(
+            CommaSeparatedStrings(
+                req.headers.get(X_GITLAB_SAAS_NAMESPACE_IDS_HEADER, "")
+            )
+        ),
     )
 
 
diff --git a/ai_gateway/api/v2/code/typing.py b/ai_gateway/api/v2/code/typing.py
index ca5b74c..9138178 100644
--- a/ai_gateway/api/v2/code/typing.py
+++ b/ai_gateway/api/v2/code/typing.py
@@ -17,7 +17,6 @@ from ai_gateway.code_suggestions import (
 )
 from ai_gateway.experimentation import ExperimentTelemetry
 from ai_gateway.instrumentators.base import Telemetry
-from ai_gateway.models import KindModelProvider, Message
 
 __all__ = [
     "CompletionsRequestV1",
@@ -28,12 +27,14 @@ __all__ = [
     "StreamSuggestionsResponse",
 ]
 
+from ai_gateway.models import KindModelProvider
+
 
 class CurrentFile(BaseModel):
     file_name: Annotated[str, StringConstraints(strip_whitespace=True, max_length=255)]
-    language_identifier: Optional[Annotated[str, StringConstraints(max_length=255)]] = (
-        None  # https://code.visualstudio.com/docs/languages/identifiers
-    )
+    language_identifier: Optional[
+        Annotated[str, StringConstraints(max_length=255)]
+    ] = None  # https://code.visualstudio.com/docs/languages/identifiers
     content_above_cursor: Annotated[str, StringConstraints(max_length=100000)]
     content_below_cursor: Annotated[str, StringConstraints(max_length=100000)]
 
@@ -96,11 +97,6 @@ class GenerationsRequestV2(GenerationsRequest):
     prompt: str
 
 
-class GenerationsRequestV3(GenerationsRequest):
-    prompt_version: Literal[3]
-    prompt: list[Message]
-
-
 class SuggestionsResponse(BaseModel):
     class Choice(BaseModel):
         text: str
diff --git a/ai_gateway/api/v3/code/completions.py b/ai_gateway/api/v3/code/completions.py
index 4470957..c8dc12e 100644
--- a/ai_gateway/api/v3/code/completions.py
+++ b/ai_gateway/api/v3/code/completions.py
@@ -4,7 +4,7 @@ from typing import AsyncIterator
 import structlog
 from dependency_injector.providers import Factory
 from dependency_injector.wiring import Provide, inject
-from fastapi import APIRouter, Request
+from fastapi import APIRouter, Depends, Request
 
 from ai_gateway.api.feature_category import feature_category
 from ai_gateway.api.v3.code.typing import (
@@ -55,12 +55,14 @@ async def completions(
 @inject
 async def code_completion(
     payload: EditorContentCompletionPayload,
-    completions_legacy_factory: Factory[CodeCompletionsLegacy] = Provide[
-        ContainerApplication.code_suggestions.completions.vertex_legacy.provider
-    ],
-    completions_anthropic_factory: Factory[CodeCompletions] = Provide[
-        ContainerApplication.code_suggestions.completions.anthropic.provider
-    ],
+    completions_legacy_factory: Factory[CodeCompletionsLegacy] = Depends(
+        Provide[
+            ContainerApplication.code_suggestions.completions.vertex_legacy.provider
+        ]
+    ),
+    completions_anthropic_factory: Factory[CodeCompletions] = Depends(
+        Provide[ContainerApplication.code_suggestions.completions.anthropic.provider]
+    ),
 ):
     if payload.model_provider == ModelProvider.ANTHROPIC:
         engine = completions_anthropic_factory()
@@ -94,12 +96,14 @@ async def code_completion(
 @inject
 async def code_generation(
     payload: EditorContentGenerationPayload,
-    generations_vertex_factory: Factory[CodeGenerations] = Provide[
-        ContainerApplication.code_suggestions.generations.vertex.provider
-    ],
-    generations_anthropic_factory: Factory[CodeGenerations] = Provide[
-        ContainerApplication.code_suggestions.generations.anthropic_default.provider
-    ],
+    generations_vertex_factory: Factory[CodeGenerations] = Depends(
+        Provide[ContainerApplication.code_suggestions.generations.vertex.provider]
+    ),
+    generations_anthropic_factory: Factory[CodeGenerations] = Depends(
+        Provide[
+            ContainerApplication.code_suggestions.generations.anthropic_default.provider
+        ]
+    ),
 ):
     if payload.model_provider == KindModelProvider.ANTHROPIC:
         engine = generations_anthropic_factory()
diff --git a/ai_gateway/api/v3/code/typing.py b/ai_gateway/api/v3/code/typing.py
index a3b67dc..ac353d6 100644
--- a/ai_gateway/api/v3/code/typing.py
+++ b/ai_gateway/api/v3/code/typing.py
@@ -34,9 +34,9 @@ class EditorContentPayload(BaseModel):
     file_name: Annotated[str, StringConstraints(strip_whitespace=True, max_length=255)]
     content_above_cursor: Annotated[str, StringConstraints(max_length=100000)]
     content_below_cursor: Annotated[str, StringConstraints(max_length=100000)]
-    language_identifier: Optional[Annotated[str, StringConstraints(max_length=255)]] = (
-        None
-    )
+    language_identifier: Optional[
+        Annotated[str, StringConstraints(max_length=255)]
+    ] = None
     model_provider: Optional[
         Literal[ModelProvider.VERTEX_AI, ModelProvider.ANTHROPIC]
     ] = None
diff --git a/ai_gateway/app.py b/ai_gateway/app.py
index ba2035a..d57d017 100644
--- a/ai_gateway/app.py
+++ b/ai_gateway/app.py
@@ -1,24 +1,10 @@
-import asyncio
-import logging
 from logging.config import dictConfig
 
+import uvicorn
 from dotenv import load_dotenv
-from fastapi.exception_handlers import http_exception_handler
-from prometheus_client import start_http_server
-from prometheus_fastapi_instrumentator import Instrumentator, metrics
-from starlette.exceptions import HTTPException as StarletteHTTPException
-from starlette_context import context
 
 from ai_gateway.api import create_fast_api_server
 from ai_gateway.config import Config
-from ai_gateway.container import (
-    _METRICS_ENDPOINTS,
-    _PROBS_ENDPOINTS,
-    ContainerApplication,
-)
-from ai_gateway.instrumentators.threads import monitor_threads
-from ai_gateway.profiling import setup_profiling
-from ai_gateway.structured_logging import setup_logging
 
 # load env variables from .env if exists
 load_dotenv()
@@ -30,66 +16,16 @@ config = Config()
 dictConfig(config.fastapi.uvicorn_logger)
 
 
-def get_config():
-    return config
-
-
-def get_app():
-    container_application = ContainerApplication()
-    container_application.config.from_dict(config.model_dump())
-
-    app = create_fast_api_server()
-    setup_logging(app, config.logging)
-    log = logging.getLogger("uvicorn.error")
-
-    setup_profiling(config.google_cloud_profiler, log)
-
-    instrumentator = Instrumentator(
-        should_group_status_codes=True,
-        should_ignore_untemplated=True,
-        should_respect_env_var=False,
-        should_instrument_requests_inprogress=False,
-        excluded_handlers=_PROBS_ENDPOINTS + _METRICS_ENDPOINTS,
+def main():
+    # For now, trust all IPs for proxy headers until https://github.com/encode/uvicorn/pull/1611 is available.
+    uvicorn.run(
+        create_fast_api_server(config),
+        host=config.fastapi.api_host,
+        port=config.fastapi.api_port,
+        log_config=config.fastapi.uvicorn_logger,
+        forwarded_allow_ips="*",
     )
-    instrumentator.add(
-        metrics.latency(
-            should_include_handler=True,
-            should_include_method=True,
-            should_include_status=True,
-            buckets=(0.5, 1, 2.5, 5, 10, 30, 60),
-        )
-    )
-    instrumentator.instrument(app)
-
-    @app.on_event("startup")
-    def on_server_startup():
-        container_application.init_resources()
-
-        if config.instrumentator.thread_monitoring_enabled:
-            loop = asyncio.get_running_loop()
-            loop.create_task(
-                monitor_threads(
-                    loop, interval=config.instrumentator.thread_monitoring_interval
-                )
-            )
-
-        # https://github.com/trallnag/prometheus-fastapi-instrumentator/issues/10
-        log.info(
-            "Metrics HTTP server running on http://%s:%d",
-            config.fastapi.metrics_host,
-            config.fastapi.metrics_port,
-        )
-        start_http_server(
-            addr=config.fastapi.metrics_host, port=config.fastapi.metrics_port
-        )
-
-    @app.on_event("shutdown")
-    def on_server_shutdown():
-        container_application.shutdown_resources()
 
-    @app.exception_handler(StarletteHTTPException)
-    async def custom_http_exception_handler(request, exc):
-        context["http_exception_details"] = str(exc)
-        return await http_exception_handler(request, exc)
 
-    return app
+if __name__ == "__main__":
+    main()
diff --git a/ai_gateway/async_dependency_resolver.py b/ai_gateway/async_dependency_resolver.py
deleted file mode 100644
index 3bfa7f9..0000000
--- a/ai_gateway/async_dependency_resolver.py
+++ /dev/null
@@ -1,60 +0,0 @@
-from dependency_injector.wiring import Provide, inject
-
-from ai_gateway.container import ContainerApplication
-
-"""
-This module allows you to resolve the dependencies with Python async/coroutines.
-
-Do NOT use FastAPI's `Depends` and Dependency Injector's Wiring (a.k.a `Provide`/`@inject`) in the following way:
-
-```
-async def chat(
-    ...
-    some_factory: Factory[SomeModel] = Depends(Provide[ContainerApplication.some_factory.provider]),
-    ...
-```
-
-This is [an example usage](https://python-dependency-injector.ets-labs.org/examples/fastapi.html) provided by Dependency Injector.
-However, since `Provide` object is not async/coroutine compatible, FastAPI runs a new thread for resolving the dependency.
-
-See https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/ai-assist/-/merge_requests/606 for more information.
-"""
-
-
-@inject
-def get_container_application(
-    container: ContainerApplication = Provide[ContainerApplication],
-):
-    return container
-
-
-async def get_chat_anthropic_claude_factory_provider():
-    yield get_container_application().chat.anthropic_claude_factory
-
-
-async def get_x_ray_anthropic_claude():
-    yield get_container_application().x_ray.anthropic_claude()
-
-
-async def get_code_suggestions_completions_vertex_legacy_provider():
-    yield get_container_application().code_suggestions.completions.vertex_legacy
-
-
-async def get_code_suggestions_completions_anthropic_provider():
-    yield get_container_application().code_suggestions.completions.anthropic
-
-
-async def get_snowplow_instrumentator():
-    yield get_container_application().snowplow.instrumentator()
-
-
-async def get_code_suggestions_generations_vertex_provider():
-    yield get_container_application().code_suggestions.generations.vertex
-
-
-async def get_code_suggestions_generations_anthropic_factory_provider():
-    yield get_container_application().code_suggestions.generations.anthropic_factory
-
-
-async def get_code_suggestions_generations_anthropic_chat_factory_provider():
-    yield get_container_application().code_suggestions.generations.anthropic_chat_factory
diff --git a/ai_gateway/chat/container.py b/ai_gateway/chat/container.py
index 36acc45..e3e8a85 100644
--- a/ai_gateway/chat/container.py
+++ b/ai_gateway/chat/container.py
@@ -8,13 +8,6 @@ __all__ = [
 class ContainerChat(containers.DeclarativeContainer):
     models = providers.DependenciesContainer()
 
-    # The dependency injector does not allow us to override the FactoryAggregate provider directly.
-    # However, we can still override its internal sub-factories to achieve the same goal.
-    _anthropic_claude_llm_factory = providers.Factory(models.anthropic_claude)
-    _anthropic_claude_chat_factory = providers.Factory(models.anthropic_claude_chat)
-
     # We need to resolve the model based on model name provided in request payload
-    # Hence, `models._anthropic_claude` and `models._anthropic_claude_chat_factory` are only partially applied here.
-    anthropic_claude_factory = providers.FactoryAggregate(
-        llm=_anthropic_claude_llm_factory, chat=_anthropic_claude_chat_factory
-    )
+    # Hence, `models.anthropic_claude` is only partially applied here.
+    anthropic_claude_factory = providers.Factory(models.anthropic_claude)
diff --git a/ai_gateway/code_suggestions/base.py b/ai_gateway/code_suggestions/base.py
index c6bc245..1161f0f 100644
--- a/ai_gateway/code_suggestions/base.py
+++ b/ai_gateway/code_suggestions/base.py
@@ -51,9 +51,6 @@ USE_CASES_MODELS_MAP = {
         KindAnthropicModel.CLAUDE_2_0,
         KindAnthropicModel.CLAUDE_2_1,
         KindVertexTextModel.CODE_BISON_002,
-        KindAnthropicModel.CLAUDE_3_OPUS,
-        KindAnthropicModel.CLAUDE_3_SONNET,
-        KindAnthropicModel.CLAUDE_3_HAIKU,
     },
 }
 
diff --git a/ai_gateway/code_suggestions/completions.py b/ai_gateway/code_suggestions/completions.py
index 575fb58..4641542 100644
--- a/ai_gateway/code_suggestions/completions.py
+++ b/ai_gateway/code_suggestions/completions.py
@@ -13,10 +13,12 @@ from ai_gateway.code_suggestions.processing import (
     ModelEngineCompletions,
     ModelEngineOutput,
     Prompt,
-    TokenStrategyBase,
 )
 from ai_gateway.code_suggestions.processing.post.completions import PostProcessor
-from ai_gateway.code_suggestions.processing.pre import PromptBuilderPrefixBased
+from ai_gateway.code_suggestions.processing.pre import (
+    PromptBuilderPrefixBased,
+    TokenStrategyBase,
+)
 from ai_gateway.instrumentators import (
     KnownMetrics,
     TextGenModelInstrumentator,
@@ -29,8 +31,6 @@ from ai_gateway.models import (
     TextGenModelChunk,
     TextGenModelOutput,
 )
-from ai_gateway.tracking.instrumentator import SnowplowInstrumentator
-from ai_gateway.tracking.snowplow import SnowplowEvent
 
 __all__ = ["CodeCompletionsLegacy", "CodeCompletions"]
 
@@ -40,11 +40,9 @@ class CodeCompletionsLegacy:
         self,
         engine: ModelEngineCompletions,
         post_processor: Factory[PostProcessor],
-        snowplow_instrumentator: SnowplowInstrumentator,
     ):
         self.engine = engine
         self.post_processor = post_processor
-        self.instrumentator = snowplow_instrumentator
 
     async def execute(
         self,
@@ -56,24 +54,6 @@ class CodeCompletionsLegacy:
     ) -> ModelEngineOutput:
         response = await self.engine.generate(prefix, suffix, file_name, editor_lang)
 
-        self.instrumentator.watch(
-            SnowplowEvent(
-                context=None,
-                action="tokens_per_user_request_prompt",
-                label="code_completion",
-                value=response.tokens_consumption_metadata.input_tokens,
-            )
-        )
-
-        self.instrumentator.watch(
-            SnowplowEvent(
-                context=None,
-                action="tokens_per_user_request_response",
-                label="code_completion",
-                value=response.tokens_consumption_metadata.output_tokens,
-            )
-        )
-
         if not response.text:
             return response
 
@@ -94,7 +74,6 @@ class CodeCompletionsLegacy:
             model=response.model,
             lang_id=response.lang_id,
             metadata=response.metadata,
-            tokens_consumption_metadata=response.tokens_consumption_metadata,
         )
 
 
diff --git a/ai_gateway/code_suggestions/container.py b/ai_gateway/code_suggestions/container.py
index 8a44fd3..5a0d323 100644
--- a/ai_gateway/code_suggestions/container.py
+++ b/ai_gateway/code_suggestions/container.py
@@ -14,9 +14,7 @@ from ai_gateway.code_suggestions.processing.post.completions import (
 from ai_gateway.code_suggestions.processing.pre import TokenizerTokenStrategy
 from ai_gateway.experimentation import experiment_registry_provider
 from ai_gateway.models import KindAnthropicModel, KindVertexTextModel, TextGenBaseModel
-from ai_gateway.models.base_chat import ChatModelBase
 from ai_gateway.tokenizer import init_tokenizer
-from ai_gateway.tracking.instrumentator import SnowplowInstrumentator
 
 __all__ = [
     "ContainerCodeSuggestions",
@@ -27,8 +25,6 @@ class ContainerCodeGenerations(containers.DeclarativeContainer):
     tokenizer = providers.Dependency(instance_of=PreTrainedTokenizerFast)
     vertex_code_bison = providers.Dependency(instance_of=TextGenBaseModel)
     anthropic_claude = providers.Dependency(instance_of=TextGenBaseModel)
-    anthropic_claude_chat = providers.Dependency(instance_of=ChatModelBase)
-    snowplow_instrumentator = providers.Dependency(instance_of=SnowplowInstrumentator)
 
     vertex = providers.Factory(
         CodeGenerations,
@@ -38,7 +34,6 @@ class ContainerCodeGenerations(containers.DeclarativeContainer):
         tokenization_strategy=providers.Factory(
             TokenizerTokenStrategy, tokenizer=tokenizer
         ),
-        snowplow_instrumentator=snowplow_instrumentator,
     )
 
     # We need to resolve the model based on model name provided in request payload
@@ -52,16 +47,6 @@ class ContainerCodeGenerations(containers.DeclarativeContainer):
         tokenization_strategy=providers.Factory(
             TokenizerTokenStrategy, tokenizer=tokenizer
         ),
-        snowplow_instrumentator=snowplow_instrumentator,
-    )
-
-    anthropic_chat_factory = providers.Factory(
-        CodeGenerations,
-        model=providers.Factory(anthropic_claude_chat),
-        tokenization_strategy=providers.Factory(
-            TokenizerTokenStrategy, tokenizer=tokenizer
-        ),
-        snowplow_instrumentator=snowplow_instrumentator,
     )
 
     # Default use case with claude.2.0
@@ -75,7 +60,6 @@ class ContainerCodeCompletions(containers.DeclarativeContainer):
     tokenizer = providers.Dependency(instance_of=PreTrainedTokenizerFast)
     vertex_code_gecko = providers.Dependency(instance_of=TextGenBaseModel)
     anthropic_claude = providers.Dependency(instance_of=TextGenBaseModel)
-    snowplow_instrumentator = providers.Dependency(instance_of=SnowplowInstrumentator)
 
     config = providers.Configuration(strict=True)
 
@@ -86,16 +70,13 @@ class ContainerCodeCompletions(containers.DeclarativeContainer):
             model=providers.Factory(
                 vertex_code_gecko, name=KindVertexTextModel.CODE_GECKO_002
             ),
-            tokenization_strategy=providers.Factory(
-                TokenizerTokenStrategy, tokenizer=tokenizer
-            ),
+            tokenizer=tokenizer,
             experiment_registry=experiment_registry_provider(),
         ),
         post_processor=providers.Factory(
             PostProcessorCompletions,
             exclude=config.excl_post_proc,
         ).provider,
-        snowplow_instrumentator=snowplow_instrumentator,
     )
 
     anthropic = providers.Factory(
@@ -119,15 +100,11 @@ class ContainerCodeSuggestions(containers.DeclarativeContainer):
 
     tokenizer = providers.Resource(init_tokenizer)
 
-    snowplow = providers.DependenciesContainer()
-
     generations = providers.Container(
         ContainerCodeGenerations,
         tokenizer=tokenizer,
         vertex_code_bison=models.vertex_code_bison,
         anthropic_claude=models.anthropic_claude,
-        anthropic_claude_chat=models.anthropic_claude_chat,
-        snowplow_instrumentator=snowplow.instrumentator,
     )
 
     completions = providers.Container(
@@ -136,5 +113,4 @@ class ContainerCodeSuggestions(containers.DeclarativeContainer):
         vertex_code_gecko=models.vertex_code_gecko,
         anthropic_claude=models.anthropic_claude,
         config=config,
-        snowplow_instrumentator=snowplow.instrumentator,
     )
diff --git a/ai_gateway/code_suggestions/generations.py b/ai_gateway/code_suggestions/generations.py
index bbf4d02..08d06ef 100644
--- a/ai_gateway/code_suggestions/generations.py
+++ b/ai_gateway/code_suggestions/generations.py
@@ -8,25 +8,24 @@ from ai_gateway.code_suggestions.base import (
     increment_lang_counter,
     resolve_lang_id,
 )
-from ai_gateway.code_suggestions.processing import LanguageId, Prompt, TokenStrategyBase
+from ai_gateway.code_suggestions.processing import LanguageId, Prompt
 from ai_gateway.code_suggestions.processing.post.generations import (
     PostProcessor,
     PostProcessorAnthropic,
 )
-from ai_gateway.code_suggestions.processing.pre import PromptBuilderPrefixBased
+from ai_gateway.code_suggestions.processing.pre import (
+    PromptBuilderPrefixBased,
+    TokenStrategyBase,
+)
 from ai_gateway.instrumentators import TextGenModelInstrumentator
 from ai_gateway.models import (
-    Message,
     ModelAPICallError,
     ModelAPIError,
     TextGenBaseModel,
     TextGenModelChunk,
     TextGenModelOutput,
 )
-from ai_gateway.models.base_chat import ChatModelBase
 from ai_gateway.prompts import PromptTemplate
-from ai_gateway.tracking.instrumentator import SnowplowInstrumentator
-from ai_gateway.tracking.snowplow import SnowplowEvent
 
 __all__ = ["CodeGenerations"]
 
@@ -45,7 +44,6 @@ class CodeGenerations:
         self,
         model: TextGenBaseModel,
         tokenization_strategy: TokenStrategyBase,
-        snowplow_instrumentator: SnowplowInstrumentator,
     ):
         self.model = model
 
@@ -56,8 +54,6 @@ class CodeGenerations:
         self.prompt_builder = PromptBuilderPrefixBased(
             model.MAX_MODEL_LEN, tokenization_strategy
         )
-        self.tokenization_strategy = tokenization_strategy
-        self.snowplow_instrumentator = snowplow_instrumentator
 
     def _get_prompt(
         self, prefix: str, file_name: str, lang_id: Optional[LanguageId] = None
@@ -80,7 +76,7 @@ class CodeGenerations:
 
         return prompt
 
-    def with_prompt_prepared(self, prompt: str | list[Message]):
+    def with_prompt_prepared(self, prompt: str):
         self.prompt = self.prompt_builder.wrap(prompt)
 
     async def execute(
@@ -97,31 +93,13 @@ class CodeGenerations:
 
         prompt = self._get_prompt(prefix, file_name, lang_id)
 
-        self.snowplow_instrumentator.watch(
-            SnowplowEvent(
-                context=None,
-                action="tokens_per_user_request_prompt",
-                label="code_generation",
-                value=sum(
-                    md.length_tokens for md in prompt.metadata.components.values()
-                ),
-            )
-        )
-
         with self.instrumentator.watch(prompt) as watch_container:
             try:
                 watch_container.register_lang(lang_id, editor_lang)
 
-                if isinstance(self.model, ChatModelBase):
-                    res = await self.model.generate(
-                        prompt.prefix, stream=stream, **kwargs
-                    )
-                else:
-                    res = await self.model.generate(
-                        prompt.prefix, "", stream=stream, **kwargs
-                    )
-
-                if res:
+                if res := await self.model.generate(
+                    prompt.prefix, "", stream=stream, **kwargs
+                ):
                     if isinstance(res, AsyncIterator):
                         return self._handle_stream(res)
 
@@ -140,27 +118,18 @@ class CodeGenerations:
                 watch_container.register_model_exception(str(ex), -1)
 
         return CodeSuggestionsOutput(
-            text="", score=0, model=self.model.metadata, lang_id=lang_id
+            text="",
+            score=0,
+            model=self.model.metadata,
+            lang_id=lang_id,
         )
 
     async def _handle_stream(
         self, response: AsyncIterator[TextGenModelChunk]
     ) -> AsyncIterator[CodeSuggestionsChunk]:
-        chunks = []
-        try:
-            async for chunk in response:
-                chunk_content = CodeSuggestionsChunk(text=chunk.text)
-                chunks.append(chunk.text)
-                yield chunk_content
-        finally:
-            self.snowplow_instrumentator.watch(
-                SnowplowEvent(
-                    context=None,
-                    action="tokens_per_user_request_response",
-                    label="code_generation",
-                    value=sum(self.tokenization_strategy.estimate_length(chunks)),
-                )
-            )
+        async for chunk in response:
+            chunk_content = CodeSuggestionsChunk(text=chunk.text)
+            yield chunk_content
 
     async def _handle_sync(
         self,
@@ -181,15 +150,6 @@ class CodeGenerations:
         )
         generation = await processor(prefix).process(response.text)
 
-        self.snowplow_instrumentator.watch(
-            SnowplowEvent(
-                context=None,
-                action="tokens_per_user_request_response",
-                label="code_generation",
-                value=self.tokenization_strategy.estimate_length(response.text)[0],
-            )
-        )
-
         return CodeSuggestionsOutput(
             text=generation,
             score=response.score,
diff --git a/ai_gateway/code_suggestions/processing/base.py b/ai_gateway/code_suggestions/processing/base.py
index ab3fe36..11431b3 100644
--- a/ai_gateway/code_suggestions/processing/base.py
+++ b/ai_gateway/code_suggestions/processing/base.py
@@ -3,6 +3,7 @@ from pathlib import Path
 from typing import Any, NamedTuple, Optional
 
 from prometheus_client import Counter
+from transformers import PreTrainedTokenizer
 
 from ai_gateway.code_suggestions.processing.ops import (
     lang_from_editor_lang,
@@ -13,13 +14,10 @@ from ai_gateway.code_suggestions.processing.typing import (
     LanguageId,
     MetadataCodeContent,
     MetadataPromptBuilder,
-    Prompt,
-    TokenStrategyBase,
 )
 from ai_gateway.experimentation import ExperimentTelemetry
 from ai_gateway.instrumentators import TextGenModelInstrumentator
 from ai_gateway.models import ModelMetadata, PalmCodeGenBaseModel
-from ai_gateway.models.base import TokensConsumptionMetadata
 
 __all__ = [
     "ModelEngineOutput",
@@ -50,7 +48,6 @@ class ModelEngineOutput(NamedTuple):
     score: float
     model: ModelMetadata
     metadata: MetadataPromptBuilder
-    tokens_consumption_metadata: TokensConsumptionMetadata
     lang_id: Optional[LanguageId] = None
 
     @property
@@ -59,11 +56,9 @@ class ModelEngineOutput(NamedTuple):
 
 
 class ModelEngineBase(ABC):
-    def __init__(
-        self, model: PalmCodeGenBaseModel, tokenization_strategy: TokenStrategyBase
-    ):
+    def __init__(self, model: PalmCodeGenBaseModel, tokenizer: PreTrainedTokenizer):
         self.model = model
-        self.tokenization_strategy = tokenization_strategy
+        self.tokenizer = tokenizer
         self.instrumentator = TextGenModelInstrumentator(
             model.metadata.engine, model.metadata.name
         )
@@ -134,6 +129,12 @@ class ModelEngineBase(ABC):
             EXPERIMENT_COUNTER.labels(name=exp.name, variant=exp.variant).inc()
 
 
+class Prompt(NamedTuple):
+    prefix: str
+    metadata: MetadataPromptBuilder
+    suffix: Optional[str] = None
+
+
 class PromptBuilderBase(ABC):
     def __init__(
         self,
diff --git a/ai_gateway/code_suggestions/processing/completions.py b/ai_gateway/code_suggestions/processing/completions.py
index 0d71496..a2a1ef4 100644
--- a/ai_gateway/code_suggestions/processing/completions.py
+++ b/ai_gateway/code_suggestions/processing/completions.py
@@ -1,22 +1,25 @@
 from typing import Any, Callable, NamedTuple, Optional
 
 import structlog
+from transformers import PreTrainedTokenizer
 
 from ai_gateway.code_suggestions.processing.base import (
     MINIMIMUM_CONFIDENCE_SCORE,
     ModelEngineBase,
     ModelEngineOutput,
+    Prompt,
     PromptBuilderBase,
 )
-from ai_gateway.code_suggestions.processing.ops import remove_incomplete_block
+from ai_gateway.code_suggestions.processing.ops import (
+    remove_incomplete_block,
+    truncate_content,
+)
 from ai_gateway.code_suggestions.processing.typing import (
     CodeContent,
     LanguageId,
     MetadataCodeContent,
     MetadataExtraInfo,
     MetadataPromptBuilder,
-    Prompt,
-    TokenStrategyBase,
 )
 from ai_gateway.experimentation import ExperimentRegistry, ExperimentTelemetry
 from ai_gateway.instrumentators import TextGenModelInstrumentator
@@ -25,7 +28,6 @@ from ai_gateway.models import (
     VertexAPIConnectionError,
     VertexAPIStatusError,
 )
-from ai_gateway.models.base import TokensConsumptionMetadata
 from ai_gateway.prompts.parsers import CodeParser
 
 log = structlog.stdlib.get_logger("codesuggestions")
@@ -173,10 +175,10 @@ class ModelEngineCompletions(ModelEngineBase):
     def __init__(
         self,
         model: PalmCodeGenBaseModel,
-        tokenization_strategy: TokenStrategyBase,
+        tokenizer: PreTrainedTokenizer,
         experiment_registry: ExperimentRegistry,
     ):
-        super().__init__(model, tokenization_strategy)
+        super().__init__(model, tokenizer)
         self.experiment_registry = experiment_registry
 
     async def _generate(
@@ -196,9 +198,6 @@ class ModelEngineCompletions(ModelEngineBase):
             score=0,
             model=self.model.metadata,
             metadata=MetadataPromptBuilder(components={}),
-            tokens_consumption_metadata=TokensConsumptionMetadata(
-                input_tokens=0, output_tokens=0
-            ),
         )
 
         # TODO: keep watching the suffix length until logging ModelEngineOutput in the upper layer
@@ -227,33 +226,12 @@ class ModelEngineCompletions(ModelEngineBase):
                         watch_container.register_is_discarded()
                         completion = ""
 
-                    if res.metadata:
-                        log.debug(
-                            "token consumption metadata:",
-                            metadata=res.metadata,
-                        )
-                        tokens_consumption_metadata = res.metadata
-                    else:
-                        log.debug(
-                            "code completions: token consumption metadata is not available, using estimates"
-                        )
-                        tokens_consumption_metadata = TokensConsumptionMetadata(
-                            output_tokens=self.tokenization_strategy.estimate_length(
-                                completion
-                            )[0],
-                            input_tokens=sum(
-                                md.length_tokens
-                                for md in prompt.metadata.components.values()
-                            ),
-                        )
-
                     return ModelEngineOutput(
                         text=completion,
                         score=res.score,
                         model=self.model.metadata,
                         lang_id=lang_id,
                         metadata=prompt.metadata,
-                        tokens_consumption_metadata=tokens_consumption_metadata,
                     )
             except (VertexAPIConnectionError, VertexAPIStatusError) as ex:
                 watch_container.register_model_exception(str(ex), ex.code)
@@ -353,25 +331,32 @@ class ModelEngineCompletions(ModelEngineBase):
             comment_converter = COMMENT_GENERATOR[lang_id]
             contents = [comment_converter(content) for content in contents]
 
-        content_lengths = self.tokenization_strategy.estimate_length(contents)
+        contents_tokenized = self.tokenizer(
+            contents,
+            return_length=True,
+            return_attention_mask=False,
+            add_special_tokens=False,
+        )
 
         code_contents = [
             CodeContent(text=text, length_tokens=length)
-            for text, length in zip(contents, content_lengths)
+            for text, length in zip(contents, contents_tokenized["length"])
         ]
 
         return _CodeInfo(content=code_contents)
 
     def _get_body(self, prefix: str, suffix: str, max_length: int) -> _CodeBody:
         suffix_len = int(max_length * self.MAX_TOKENS_SUFFIX_PERCENT)
-        suffix_truncated = self.tokenization_strategy.truncate_content(
+        suffix_truncated = truncate_content(
+            self.tokenizer,
             suffix,
             max_length=suffix_len,
             truncation_side="right",
         )
 
         prefix_len = max_length - suffix_truncated.length_tokens
-        prefix_truncated = self.tokenization_strategy.truncate_content(
+        prefix_truncated = truncate_content(
+            self.tokenizer,
             prefix,
             max_length=prefix_len,
             truncation_side="left",
diff --git a/ai_gateway/code_suggestions/processing/ops.py b/ai_gateway/code_suggestions/processing/ops.py
index 2c274f3..ad0fcbf 100644
--- a/ai_gateway/code_suggestions/processing/ops.py
+++ b/ai_gateway/code_suggestions/processing/ops.py
@@ -4,9 +4,10 @@ from typing import Callable, NamedTuple, Optional, Union
 
 import numpy as np
 from Levenshtein import ratio as levenshtein_ratio
+from transformers import PreTrainedTokenizer
 from tree_sitter import Node
 
-from ai_gateway.code_suggestions.processing.typing import LanguageId
+from ai_gateway.code_suggestions.processing.typing import CodeContent, LanguageId
 
 __all__ = [
     "prepend_lang_id",
@@ -16,6 +17,7 @@ __all__ = [
     "trim_by_sep",
     "find_non_whitespace_point",
     "find_cursor_position",
+    "truncate_content",
     "find_newline_position",
     "compare_exact",
     "compare_levenshtein",
@@ -380,5 +382,31 @@ def convert_point_to_relative_point_in_node(
     return (row, col)
 
 
+def truncate_content(
+    tokenizer: PreTrainedTokenizer,
+    val: str,
+    max_length: int,
+    truncation_side: str = "left",
+) -> CodeContent:
+    prev_truncation_side = tokenizer.truncation_side
+    tokenizer.truncation_side = truncation_side
+
+    tokens = tokenizer(
+        val,
+        max_length=max_length,
+        truncation=True,
+        return_attention_mask=False,
+        add_special_tokens=False,
+    )
+
+    decoded = tokenizer.decode(tokens["input_ids"])
+    tokenizer.truncation_side = prev_truncation_side
+
+    return CodeContent(
+        text=decoded,
+        length_tokens=len(tokens["input_ids"]),
+    )
+
+
 async def strip_whitespaces(text: str) -> str:
     return "" if text.isspace() else text
diff --git a/ai_gateway/code_suggestions/processing/pre/base.py b/ai_gateway/code_suggestions/processing/pre/base.py
index 313c581..7092a36 100644
--- a/ai_gateway/code_suggestions/processing/pre/base.py
+++ b/ai_gateway/code_suggestions/processing/pre/base.py
@@ -1,20 +1,37 @@
 from abc import ABC, abstractmethod
-from typing import Any
+from typing import Any, NamedTuple, Union
 
-from ai_gateway.code_suggestions.processing.typing import (
+from ai_gateway.code_suggestions.processing import (
     MetadataCodeContent,
     MetadataPromptBuilder,
     Prompt,
-    TokenStrategyBase,
 )
-from ai_gateway.models import Message
 from ai_gateway.prompts import PromptTemplateBase
 
 __all__ = [
+    "CodeContent",
+    "TokenStrategyBase",
     "PromptBuilderBase",
 ]
 
 
+class CodeContent(NamedTuple):
+    text: str
+    length_tokens: int
+
+
+class TokenStrategyBase(ABC):
+    @abstractmethod
+    def truncate_content(
+        self, text: str, max_length: int, truncation_side: str = "left"
+    ) -> CodeContent:
+        pass
+
+    @abstractmethod
+    def estimate_length(self, *text: str) -> Union[int, list[int]]:
+        pass
+
+
 class PromptBuilderBase(ABC):
     def __init__(self, total_max_len: int, tkn_strategy: TokenStrategyBase):
         self.total_max_len = max(total_max_len, 0)
@@ -31,7 +48,7 @@ class PromptBuilderBase(ABC):
 
         # Apply all known arguments to get the number of reserved tokens
         tpl_raw = self.tpl.apply(**self.tpl_args)
-        tpl_len = self.tkn_strategy.estimate_length(tpl_raw)[0]
+        tpl_len = self.tkn_strategy.estimate_length(tpl_raw)
         if tpl_len > self.total_max_len:
             raise ValueError("the template size exceeds overall maximum length")
 
@@ -39,15 +56,9 @@ class PromptBuilderBase(ABC):
 
         return tpl_len
 
-    def wrap(
-        self, prompt: str | list[Message], ignore_exception: bool = False
-    ) -> Prompt:
-        if isinstance(prompt, list):
-            prompt_text = "".join(m.content for m in prompt)
-        else:
-            prompt_text = prompt
-        token_length = self.tkn_strategy.estimate_length(prompt_text)[0]
-        if token_length > self.total_max_len and not ignore_exception:
+    def wrap(self, prompt: str, ignore_exception: bool = False) -> Prompt:
+        length_tokens = self.tkn_strategy.estimate_length(prompt)
+        if length_tokens > self.total_max_len and not ignore_exception:
             raise ValueError("the prompt size exceeds overall maximum length")
 
         return Prompt(
@@ -55,8 +66,8 @@ class PromptBuilderBase(ABC):
             metadata=MetadataPromptBuilder(
                 components={
                     "prompt": MetadataCodeContent(
-                        length=len(prompt_text),
-                        length_tokens=token_length,
+                        length=len(prompt),
+                        length_tokens=length_tokens,
                     ),
                 }
             ),
diff --git a/ai_gateway/code_suggestions/processing/pre/prefix_based.py b/ai_gateway/code_suggestions/processing/pre/prefix_based.py
index 0faf89e..bf58cc3 100644
--- a/ai_gateway/code_suggestions/processing/pre/prefix_based.py
+++ b/ai_gateway/code_suggestions/processing/pre/prefix_based.py
@@ -1,12 +1,14 @@
 import math
 from typing import Any, Optional
 
-from ai_gateway.code_suggestions.processing.pre.base import PromptBuilderBase
-from ai_gateway.code_suggestions.processing.typing import (
-    CodeContent,
+from ai_gateway.code_suggestions.processing import (
     MetadataCodeContent,
     MetadataPromptBuilder,
     Prompt,
+)
+from ai_gateway.code_suggestions.processing.pre.base import (
+    CodeContent,
+    PromptBuilderBase,
     TokenStrategyBase,
 )
 
@@ -95,7 +97,7 @@ class PromptBuilderPrefixBased(PromptBuilderBase):
             length_tokens=max_length - length,
         )
 
-    def _build_suffix(self, max_length: int) -> Optional[CodeContent]:
+    def _build_suffix(self, max_length: float) -> Optional[CodeContent]:
         if not self.suffix:
             return None
 
diff --git a/ai_gateway/code_suggestions/processing/pre/tokens.py b/ai_gateway/code_suggestions/processing/pre/tokens.py
index 09b0b04..2fac86c 100644
--- a/ai_gateway/code_suggestions/processing/pre/tokens.py
+++ b/ai_gateway/code_suggestions/processing/pre/tokens.py
@@ -1,6 +1,11 @@
+from typing import Union
+
 from transformers import PreTrainedTokenizer
 
-from ai_gateway.code_suggestions.processing.typing import CodeContent, TokenStrategyBase
+from ai_gateway.code_suggestions.processing.pre.base import (
+    CodeContent,
+    TokenStrategyBase,
+)
 
 __all__ = [
     "TokenizerTokenStrategy",
@@ -14,6 +19,7 @@ class TokenizerTokenStrategy(TokenStrategyBase):
     def truncate_content(
         self, text: str, max_length: int, truncation_side: str = "left"
     ) -> CodeContent:
+        prev_truncation_side = self.tokenizer.truncation_side
         self.tokenizer.truncation_side = truncation_side
 
         tokens = self.tokenizer(
@@ -25,16 +31,22 @@ class TokenizerTokenStrategy(TokenStrategyBase):
         )
 
         decoded = self.tokenizer.decode(tokens["input_ids"])
+        self.tokenizer.truncation_side = prev_truncation_side
 
         return CodeContent(
             text=decoded,
             length_tokens=len(tokens["input_ids"]),
         )
 
-    def estimate_length(self, text: str | list[str]) -> list[int]:
-        return self.tokenizer(
-            text,
+    def estimate_length(self, *text: str) -> Union[int, list[int]]:
+        lengths = self.tokenizer(
+            list(text),
             return_length=True,
             return_attention_mask=False,
             add_special_tokens=False,
         )["length"]
+
+        if len(lengths) == 1:
+            return lengths.pop()
+
+        return lengths
diff --git a/ai_gateway/code_suggestions/processing/typing.py b/ai_gateway/code_suggestions/processing/typing.py
index 62c6587..d9c17a7 100644
--- a/ai_gateway/code_suggestions/processing/typing.py
+++ b/ai_gateway/code_suggestions/processing/typing.py
@@ -1,4 +1,3 @@
-from abc import ABC, abstractmethod
 from enum import Enum
 from typing import Mapping, NamedTuple, Optional
 
@@ -10,8 +9,6 @@ __all__ = [
     "MetadataExtraInfo",
     "MetadataPromptBuilder",
     "CodeContent",
-    "Prompt",
-    "TokenStrategyBase",
 ]
 
 
@@ -52,21 +49,3 @@ class MetadataPromptBuilder(NamedTuple):
 class CodeContent(NamedTuple):
     text: str
     length_tokens: int
-
-
-class Prompt(NamedTuple):
-    prefix: str | list
-    metadata: MetadataPromptBuilder
-    suffix: Optional[str] = None
-
-
-class TokenStrategyBase(ABC):
-    @abstractmethod
-    def truncate_content(
-        self, text: str, max_length: int, truncation_side: str = "left"
-    ) -> CodeContent:
-        pass
-
-    @abstractmethod
-    def estimate_length(self, text: str | list[str]) -> list[int]:
-        pass
diff --git a/ai_gateway/config.py b/ai_gateway/config.py
index 9f508d3..43110e6 100644
--- a/ai_gateway/config.py
+++ b/ai_gateway/config.py
@@ -1,7 +1,7 @@
 from typing import Annotated, Optional
 
 from dotenv import find_dotenv
-from pydantic import AliasChoices, BaseModel, Field, RootModel
+from pydantic import BaseModel, Field, RootModel
 from pydantic_settings import BaseSettings, SettingsConfigDict
 
 __all__ = [
@@ -18,8 +18,6 @@ __all__ = [
     "ConfigModelConcurrency",
 ]
 
-ENV_PREFIX = "AIGW"
-
 
 class ConfigLogging(BaseModel):
     level: str = "INFO"
@@ -36,7 +34,6 @@ class ConfigFastApi(BaseModel):
     docs_url: Optional[str] = None
     openapi_url: Optional[str] = None
     redoc_url: Optional[str] = None
-    reload: bool = False
 
 
 class ConfigAuth(BaseModel):
@@ -75,7 +72,6 @@ class ConfigVertexTextModel(BaseModel):
     project: str = "unreview-poc-390200e5"
     location: str = "us-central1"
     endpoint: str = "us-central1-aiplatform.googleapis.com"
-    json_key: str = ""
 
 
 class ConfigModelConcurrency(RootModel):
@@ -88,24 +84,17 @@ class ConfigModelConcurrency(RootModel):
 class Config(BaseSettings):
     model_config = SettingsConfigDict(
         env_nested_delimiter="__",
-        env_prefix=f"{ENV_PREFIX}_",
+        env_prefix="AIGW_",
         protected_namespaces=(),
         env_file=find_dotenv(),
         env_file_encoding="utf-8",
-        extra="ignore",
     )
 
     gitlab_url: str = "https://gitlab.com"
     gitlab_api_url: str = "https://gitlab.com/api/v4/"
     customer_portal_url: str = "https://customers.gitlab.com"
 
-    mock_model_responses: bool = Field(
-        validation_alias=AliasChoices(
-            f"{ENV_PREFIX.lower()}_mock_model_responses",
-            f"{ENV_PREFIX.lower()}_use_fake_models",  # Backward compatibility with the GitLab QA tests
-        ),
-        default=False,
-    )
+    use_fake_models: bool = False
 
     logging: Annotated[ConfigLogging, Field(default_factory=ConfigLogging)]
     fastapi: Annotated[ConfigFastApi, Field(default_factory=ConfigFastApi)]
diff --git a/ai_gateway/container.py b/ai_gateway/container.py
index 11b2fea..9d7ffd8 100644
--- a/ai_gateway/container.py
+++ b/ai_gateway/container.py
@@ -1,8 +1,6 @@
 from dependency_injector import containers, providers
 from py_grpc_prometheus.prometheus_client_interceptor import PromClientInterceptor
 
-from ai_gateway.api import middleware
-from ai_gateway.auth import GitLabOidcProvider
 from ai_gateway.chat.container import ContainerChat
 from ai_gateway.code_suggestions.container import ContainerCodeSuggestions
 from ai_gateway.models.container import ContainerModels
@@ -14,40 +12,6 @@ __all__ = [
 
 from ai_gateway.x_ray.container import ContainerXRay
 
-_PROBS_ENDPOINTS = ["/monitoring/healthz"]
-_METRICS_ENDPOINTS = ["/metrics"]
-
-
-class ContainerFastApi(containers.DeclarativeContainer):
-    config = providers.Configuration(strict=True)
-
-    oidc_provider = providers.Singleton(
-        GitLabOidcProvider,
-        oidc_providers=providers.Dict(
-            {
-                "Gitlab": config.gitlab_url,
-                "CustomersDot": config.customer_portal_url,
-            }
-        ),
-    )
-
-    auth_middleware = providers.Factory(
-        middleware.MiddlewareAuthentication,
-        oidc_provider,
-        bypass_auth=config.auth.bypass_external,
-        skip_endpoints=_PROBS_ENDPOINTS + _METRICS_ENDPOINTS,
-    )
-
-    log_middleware = providers.Factory(
-        middleware.MiddlewareLogRequest,
-        skip_endpoints=_METRICS_ENDPOINTS,
-    )
-
-    telemetry_middleware = providers.Factory(
-        middleware.MiddlewareModelTelemetry,
-        skip_endpoints=_PROBS_ENDPOINTS + _METRICS_ENDPOINTS,
-    )
-
 
 class ContainerApplication(containers.DeclarativeContainer):
     wiring_config = containers.WiringConfiguration(
@@ -58,7 +22,6 @@ class ContainerApplication(containers.DeclarativeContainer):
             "ai_gateway.api.v3.code.completions",
             "ai_gateway.api.server",
             "ai_gateway.api.monitoring",
-            "ai_gateway.async_dependency_resolver",
         ]
     )
 
@@ -71,8 +34,6 @@ class ContainerApplication(containers.DeclarativeContainer):
         enable_client_stream_send_time_histogram=True,
     )
 
-    snowplow = providers.Container(ContainerTracking, config=config.snowplow)
-
     pkg_models = providers.Container(
         ContainerModels,
         config=config,
@@ -81,7 +42,6 @@ class ContainerApplication(containers.DeclarativeContainer):
         ContainerCodeSuggestions,
         models=pkg_models,
         config=config.f.code_suggestions,
-        snowplow=snowplow,
     )
     x_ray = providers.Container(
         ContainerXRay,
@@ -92,7 +52,4 @@ class ContainerApplication(containers.DeclarativeContainer):
         models=pkg_models,
     )
 
-    fastapi = providers.Container(
-        ContainerFastApi,
-        config=config,
-    )
+    snowplow = providers.Container(ContainerTracking, config=config.snowplow)
diff --git a/ai_gateway/instrumentators/base.py b/ai_gateway/instrumentators/base.py
index c811758..0e63de4 100644
--- a/ai_gateway/instrumentators/base.py
+++ b/ai_gateway/instrumentators/base.py
@@ -26,7 +26,6 @@ INFERENCE_HISTOGRAM = Histogram(
     "code_suggestions_inference_request_duration_seconds",
     "Duration of the inference request in seconds",
     METRIC_LABELS,
-    buckets=(0.5, 1, 2.5, 5, 10, 30, 60),
 )
 
 INFERENCE_PROMPT_HISTOGRAM = Histogram(
diff --git a/ai_gateway/main.py b/ai_gateway/main.py
deleted file mode 100644
index 8d2ec35..0000000
--- a/ai_gateway/main.py
+++ /dev/null
@@ -1,21 +0,0 @@
-import uvicorn
-
-from ai_gateway.app import get_config
-
-
-def run_app():
-    config = get_config()
-    # For now, trust all IPs for proxy headers until https://github.com/encode/uvicorn/pull/1611 is available.
-    uvicorn.run(
-        "ai_gateway.app:get_app",
-        host=config.fastapi.api_host,
-        port=config.fastapi.api_port,
-        log_config=config.fastapi.uvicorn_logger,
-        forwarded_allow_ips="*",
-        reload=config.fastapi.reload,
-        factory=True,
-    )
-
-
-if __name__ == "__main__":
-    run_app()
diff --git a/ai_gateway/models/__init__.py b/ai_gateway/models/__init__.py
index afcc2e6..8758a14 100644
--- a/ai_gateway/models/__init__.py
+++ b/ai_gateway/models/__init__.py
@@ -1,7 +1,7 @@
 # flake8: noqa
 
-from ai_gateway.models import container, mock
+from ai_gateway.models import container
 from ai_gateway.models.anthropic import *
 from ai_gateway.models.base import *
-from ai_gateway.models.base_chat import *
+from ai_gateway.models.fake import *
 from ai_gateway.models.vertex_text import *
diff --git a/ai_gateway/models/anthropic.py b/ai_gateway/models/anthropic.py
index 2c05337..93bca9d 100644
--- a/ai_gateway/models/anthropic.py
+++ b/ai_gateway/models/anthropic.py
@@ -11,7 +11,6 @@ from anthropic import (
     AsyncStream,
 )
 from anthropic._types import NOT_GIVEN
-from anthropic.types import ContentBlockDeltaEvent
 
 from ai_gateway.models.base import (
     KindModelProvider,
@@ -23,14 +22,12 @@ from ai_gateway.models.base import (
     TextGenModelChunk,
     TextGenModelOutput,
 )
-from ai_gateway.models.base_chat import ChatModelBase, Message, Role
 
 __all__ = [
     "AnthropicAPIConnectionError",
-    "AnthropicAPIStatusError",
     "AnthropicAPITimeoutError",
+    "AnthropicAPIStatusError",
     "AnthropicModel",
-    "AnthropicChatModel",
     "KindAnthropicModel",
 ]
 
@@ -69,19 +66,10 @@ class KindAnthropicModel(str, Enum):
     CLAUDE_INSTANT_1_2 = "claude-instant-1.2"
     CLAUDE_2_0 = "claude-2.0"
     CLAUDE_2_1 = "claude-2.1"
-    CLAUDE_3_OPUS = "claude-3-opus-20240229"
-    CLAUDE_3_SONNET = "claude-3-sonnet-20240229"
-    CLAUDE_3_HAIKU = "claude-3-haiku-20240307"
 
 
 class AnthropicModel(TextGenBaseModel):
-    """This class uses the deprecated Completions API from Anthropic.
-    Claude models v3 and above should use AnthropicChatModel.
-
-    Ref: https://docs.anthropic.com/claude/reference/migrating-from-text-completions-to-messages
-    """
-
-    # Ref: https://docs.anthropic.com/claude/docs/models-overview#model-comparison
+    # Ref: https://docs.anthropic.com/claude/reference/selecting-a-model
     MAX_MODEL_LEN = 100_000
 
     # Ref: https://docs.anthropic.com/claude/reference/versioning
@@ -183,132 +171,7 @@ class AnthropicModel(TextGenBaseModel):
 
     @classmethod
     def from_model_name(
-        cls,
-        name: Union[str, KindAnthropicModel],
-        client: AsyncAnthropic,
-        **kwargs: Any,
-    ):
-        try:
-            kind_model = KindAnthropicModel(name)
-        except ValueError:
-            raise ValueError(f"no model found by the name '{name}'")
-
-        return cls(client, model_name=kind_model.value, **kwargs)
-
-
-class AnthropicChatModel(ChatModelBase):
-    # Ref: https://docs.anthropic.com/claude/docs/models-overview#model-comparison
-    MAX_MODEL_LEN = 200_000
-
-    # Ref: https://docs.anthropic.com/claude/reference/versioning
-    DEFAULT_VERSION = "2023-06-01"
-
-    OPTS_CLIENT = {
-        "default_headers": {},
-        "max_retries": 1,
-    }
-
-    OPTS_MODEL = {
-        "timeout": httpx.Timeout(30.0, connect=5.0),
-        "max_tokens": 2048,
-        "stop_sequences": NOT_GIVEN,
-        "temperature": 0.2,
-        "top_k": NOT_GIVEN,
-        "top_p": NOT_GIVEN,
-    }
-
-    def __init__(
-        self,
-        client: AsyncAnthropic,
-        version: str = DEFAULT_VERSION,
-        model_name: str = KindAnthropicModel.CLAUDE_3_HAIKU.value,
-        **kwargs: Any,
-    ):
-        client_opts = self._obtain_client_opts(version, **kwargs)
-
-        self.client = client.with_options(**client_opts)
-        self.model_opts = self._obtain_model_opts(**kwargs)
-
-        self._metadata = ModelMetadata(
-            name=model_name,
-            engine=KindModelProvider.ANTHROPIC.value,
-        )
-
-    @staticmethod
-    def _obtain_model_opts(**kwargs: Any):
-        return _obtain_opts(AnthropicChatModel.OPTS_MODEL, **kwargs)
-
-    @staticmethod
-    def _obtain_client_opts(version: str, **kwargs: Any):
-        opts = _obtain_opts(AnthropicChatModel.OPTS_CLIENT, **kwargs)
-
-        headers = opts["default_headers"]
-        if not headers.get("anthropic-version", None):
-            headers["anthropic-version"] = version
-
-        return opts
-
-    @property
-    def metadata(self) -> ModelMetadata:
-        return self._metadata
-
-    async def generate(
-        self,
-        messages: list[Message],
-        stream: bool = False,
-        **kwargs: Any,
-    ) -> Union[TextGenModelOutput, AsyncIterator[TextGenModelChunk]]:
-        opts = _obtain_opts(self.model_opts, **kwargs)
-        log.debug("codegen anthropic call:", **opts)
-
-        model_messages = _build_model_messages(messages)
-
-        with self.instrumentator.watch(stream=stream) as watcher:
-            try:
-                suggestion = await self.client.messages.create(
-                    model=self.metadata.name,
-                    stream=stream,
-                    **model_messages,
-                    **opts,
-                )
-            except APIStatusError as ex:
-                raise AnthropicAPIStatusError.from_exception(ex)
-            except APITimeoutError as ex:
-                raise AnthropicAPITimeoutError.from_exception(ex)
-            except APIConnectionError as ex:
-                raise AnthropicAPIConnectionError.from_exception(ex)
-
-            if stream:
-                return self._handle_stream(suggestion, lambda: watcher.finish())
-
-        return TextGenModelOutput(
-            text=suggestion.content[0].text,
-            # Give a high value, the model doesn't return scores.
-            score=10**5,
-            safety_attributes=SafetyAttributes(),
-        )
-
-    async def _handle_stream(
-        self, response: AsyncStream, after_callback: Callable
-    ) -> AsyncIterator[TextGenModelChunk]:
-        try:
-            async for event in response:
-                if isinstance(event, ContentBlockDeltaEvent):
-                    if not event.delta:
-                        yield TextGenModelChunk(text="")
-
-                    yield TextGenModelChunk(text=event.delta.text)
-                else:
-                    continue
-        finally:
-            after_callback()
-
-    @classmethod
-    def from_model_name(
-        cls,
-        name: Union[str, KindAnthropicModel],
-        client: AsyncAnthropic,
-        **kwargs: Any,
+        cls, name: Union[str, KindAnthropicModel], client: AsyncAnthropic, **kwargs: Any
     ):
         try:
             kind_model = KindAnthropicModel(name)
@@ -318,18 +181,6 @@ class AnthropicChatModel(ChatModelBase):
         return cls(client, model_name=kind_model.value, **kwargs)
 
 
-def _build_model_messages(messages: list[Message]) -> dict:
-    request: dict = {"system": NOT_GIVEN, "messages": []}
-
-    for message in messages:
-        if message.role == Role.SYSTEM:
-            request["system"] = message.content
-        else:
-            request["messages"].append(message.dict())
-
-    return request
-
-
 def _obtain_opts(default_opts: dict, **kwargs: Any) -> dict:
     return {
         opt_name: kwargs.pop(opt_name, opt_value) or opt_value
diff --git a/ai_gateway/models/base.py b/ai_gateway/models/base.py
index 3e0379c..6d29085 100644
--- a/ai_gateway/models/base.py
+++ b/ai_gateway/models/base.py
@@ -1,6 +1,6 @@
 from abc import ABC, abstractmethod
 from enum import Enum
-from typing import Any, AsyncIterator, NamedTuple, Optional, Union
+from typing import Any, AsyncIterator, NamedTuple, Union
 
 from anthropic import AsyncAnthropic
 from google.cloud.aiplatform.gapic import PredictionServiceAsyncClient
@@ -20,7 +20,6 @@ __all__ = [
     "ModelAPIError",
     "ModelAPICallError",
     "ModelMetadata",
-    "TokensConsumptionMetadata",
     "SafetyAttributes",
     "TextGenModelOutput",
     "TextGenModelChunk",
@@ -70,11 +69,6 @@ class ModelMetadata(NamedTuple):
     engine: str
 
 
-class TokensConsumptionMetadata(NamedTuple):
-    input_tokens: int
-    output_tokens: int
-
-
 class ModelInput(ABC):
     @abstractmethod
     def is_valid(self) -> bool:
@@ -98,7 +92,6 @@ class TextGenModelOutput(NamedTuple):
     text: str
     score: float
     safety_attributes: SafetyAttributes
-    metadata: Optional[TokensConsumptionMetadata] = None
 
 
 class TextGenModelChunk(NamedTuple):
diff --git a/ai_gateway/models/base_chat.py b/ai_gateway/models/base_chat.py
deleted file mode 100644
index 1b6e751..0000000
--- a/ai_gateway/models/base_chat.py
+++ /dev/null
@@ -1,56 +0,0 @@
-from abc import ABC, abstractmethod
-from enum import Enum
-from typing import Annotated, AsyncIterator, Union
-
-from pydantic import BaseModel, StringConstraints
-
-from ai_gateway.config import Config
-from ai_gateway.instrumentators.model_requests import ModelRequestInstrumentator
-from ai_gateway.models.base import ModelMetadata, TextGenModelChunk, TextGenModelOutput
-
-config = Config()
-
-
-__all__ = ["ChatModelBase", "Role", "Message"]
-
-
-class Role(str, Enum):
-    SYSTEM = "system"
-    USER = "user"
-    ASSISTANT = "assistant"
-
-
-class Message(BaseModel):
-    role: Role
-    content: Annotated[str, StringConstraints(max_length=400000)]
-
-
-class ChatModelBase(ABC):
-    MAX_MODEL_LEN = 2048
-
-    @property
-    def instrumentator(self) -> ModelRequestInstrumentator:
-        return ModelRequestInstrumentator(
-            model_engine=self.metadata.engine,
-            model_name=self.metadata.name,
-            concurrency_limit=config.model_engine_concurrency_limits.for_model(
-                engine=self.metadata.engine, name=self.metadata.name
-            ),
-        )
-
-    @property
-    @abstractmethod
-    def metadata(self) -> ModelMetadata:
-        pass
-
-    @abstractmethod
-    async def generate(
-        self,
-        messages: list[Message],
-        stream: bool = False,
-        temperature: float = 0.2,
-        max_output_tokens: int = 16,
-        top_p: float = 0.95,
-        top_k: int = 40,
-    ) -> Union[TextGenModelOutput, AsyncIterator[TextGenModelChunk]]:
-        pass
diff --git a/ai_gateway/models/container.py b/ai_gateway/models/container.py
index 3ade480..1a84654 100644
--- a/ai_gateway/models/container.py
+++ b/ai_gateway/models/container.py
@@ -1,13 +1,12 @@
-import os
 from typing import Iterator, Optional
 
 from anthropic import AsyncAnthropic
 from dependency_injector import containers, providers
 from google.cloud.aiplatform.gapic import PredictionServiceAsyncClient
 
-from ai_gateway.models import mock
-from ai_gateway.models.anthropic import AnthropicChatModel, AnthropicModel
+from ai_gateway.models.anthropic import AnthropicModel
 from ai_gateway.models.base import connect_anthropic, grpc_connect_vertex
+from ai_gateway.models.fake import FakePalmTextGenModel
 from ai_gateway.models.vertex_text import (
     PalmCodeBisonModel,
     PalmCodeGeckoModel,
@@ -19,27 +18,24 @@ __all__ = [
 ]
 
 
+def _real_or_fake(use_fake: bool) -> str:
+    return "fake" if use_fake else "real"
+
+
 def _init_vertex_grpc_client(
-    endpoint: str, mock_model_responses: bool, json_key: str = ""
+    endpoint: str, use_fake: bool
 ) -> Iterator[Optional[PredictionServiceAsyncClient]]:
-    if mock_model_responses:
+    if use_fake:
         yield None
         return
 
-    if json_key:
-        with open("/tmp/vertex-client.json", "w") as f:
-            f.write(json_key)
-            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/tmp/vertex-client.json"
-
     client = grpc_connect_vertex({"api_endpoint": endpoint})
     yield client
     client.transport.close()
 
 
-def _init_anthropic_client(
-    mock_model_responses: bool,
-) -> Iterator[Optional[AsyncAnthropic]]:
-    if mock_model_responses:
+def _init_anthropic_client(use_fake: bool) -> Iterator[Optional[AsyncAnthropic]]:
+    if use_fake:
         yield None
         return
 
@@ -54,69 +50,57 @@ class ContainerModels(containers.DeclarativeContainer):
 
     config = providers.Configuration(strict=True)
 
-    _mock_selector = providers.Callable(
-        lambda mock_model_responses: "mocked" if mock_model_responses else "original",
-        config.mock_model_responses,
-    )
+    real_or_fake = providers.Callable(_real_or_fake, config.use_fake_models)
 
     grpc_client_vertex = providers.Resource(
         _init_vertex_grpc_client,
         endpoint=config.vertex_text_model.endpoint,
-        mock_model_responses=config.mock_model_responses,
-        json_key=config.vertex_text_model.json_key,
+        use_fake=config.use_fake_models,
     )
 
     http_client_anthropic = providers.Resource(
-        _init_anthropic_client,
-        mock_model_responses=config.mock_model_responses,
+        _init_anthropic_client, use_fake=config.use_fake_models
     )
 
     vertex_text_bison = providers.Selector(
-        _mock_selector,
-        original=providers.Factory(
+        real_or_fake,
+        real=providers.Factory(
             PalmTextBisonModel.from_model_name,
             client=grpc_client_vertex,
             project=config.vertex_text_model.project,
             location=config.vertex_text_model.location,
         ),
-        mocked=providers.Factory(mock.LLM),
+        fake=providers.Factory(FakePalmTextGenModel),
     )
 
     vertex_code_bison = providers.Selector(
-        _mock_selector,
-        original=providers.Factory(
+        real_or_fake,
+        real=providers.Factory(
             PalmCodeBisonModel.from_model_name,
             client=grpc_client_vertex,
             project=config.vertex_text_model.project,
             location=config.vertex_text_model.location,
         ),
-        mocked=providers.Factory(mock.LLM),
+        fake=providers.Factory(FakePalmTextGenModel),
     )
 
     vertex_code_gecko = providers.Selector(
-        _mock_selector,
-        original=providers.Factory(
+        real_or_fake,
+        real=providers.Factory(
             PalmCodeGeckoModel.from_model_name,
             client=grpc_client_vertex,
             project=config.vertex_text_model.project,
             location=config.vertex_text_model.location,
         ),
-        mocked=providers.Factory(mock.LLM),
+        fake=providers.Factory(FakePalmTextGenModel),
     )
 
     anthropic_claude = providers.Selector(
-        _mock_selector,
-        original=providers.Factory(
-            AnthropicModel.from_model_name, client=http_client_anthropic
-        ),
-        mocked=providers.Factory(mock.LLM),
-    )
-
-    anthropic_claude_chat = providers.Selector(
-        _mock_selector,
-        original=providers.Factory(
-            AnthropicChatModel.from_model_name,
+        real_or_fake,
+        real=providers.Factory(
+            AnthropicModel.from_model_name,
             client=http_client_anthropic,
         ),
-        mocked=providers.Factory(mock.ChatModel),
+        # TODO: We need to update our fake models to make them generic
+        fake=providers.Factory(FakePalmTextGenModel),
     )
diff --git a/ai_gateway/models/fake.py b/ai_gateway/models/fake.py
new file mode 100644
index 0000000..e9b828a
--- /dev/null
+++ b/ai_gateway/models/fake.py
@@ -0,0 +1,37 @@
+from typing import Any, Optional
+
+from ai_gateway.models.base import (
+    ModelMetadata,
+    SafetyAttributes,
+    TextGenBaseModel,
+    TextGenModelOutput,
+)
+
+__all__ = [
+    "FakePalmTextGenModel",
+]
+
+
+class FakePalmTextGenModel(TextGenBaseModel):
+    def __init__(self, *args: Any, **kwargs: Any):
+        pass
+
+    @property
+    def metadata(self) -> ModelMetadata:
+        return ModelMetadata(name="fake-palm-model", engine="fake-palm-engine")
+
+    async def generate(
+        self,
+        prompt: str,
+        suffix: str,
+        stream: bool = False,
+        temperature: float = 0.0,
+        max_output_tokens: int = 0,
+        top_p: float = 0.0,
+        top_k: int = 0,
+    ) -> Optional[TextGenModelOutput]:
+        return TextGenModelOutput(
+            text="fake code suggestion from PaLM Text",
+            score=0,
+            safety_attributes=SafetyAttributes(),
+        )
diff --git a/ai_gateway/models/mock.py b/ai_gateway/models/mock.py
deleted file mode 100644
index fffa8bc..0000000
--- a/ai_gateway/models/mock.py
+++ /dev/null
@@ -1,128 +0,0 @@
-import json
-import re
-from typing import Any, AsyncIterator, Callable, Optional, TypeVar
-
-from anthropic.types import Message
-
-from ai_gateway.models.base import (
-    ModelMetadata,
-    SafetyAttributes,
-    TextGenBaseModel,
-    TextGenModelChunk,
-    TextGenModelOutput,
-)
-from ai_gateway.models.base_chat import ChatModelBase
-
-__all__ = [
-    "AsyncStream",
-    "LLM",
-    "ChatModel",
-]
-
-_T = TypeVar("_T")
-
-
-class AsyncStream(AsyncIterator[_T]):
-    def __init__(self, chunks: list[_T], callback_finish: Optional[Callable] = None):
-        self.chunks = chunks
-        self.callback_finish = callback_finish
-
-    def __aiter__(self) -> "AsyncStream[_T]":
-        return self
-
-    async def __anext__(self) -> _T:
-        if len(self.chunks) > 0:
-            return self.chunks.pop(0)
-
-        if self.callback_finish:
-            self.callback_finish()
-
-        raise StopAsyncIteration
-
-
-class LLM(TextGenBaseModel):
-    """
-    Implementation of the stub model that inherits the `TextGenBaseModel` interface.
-    Please, use this class if you require to mock such models as `AnthropicModel` or `PalmCodeGeckoModel`
-    """
-
-    def __init__(self, *_args: Any, **_kwargs: Any):
-        super().__init__()
-
-    @property
-    def metadata(self) -> ModelMetadata:
-        return ModelMetadata(name="llm-mocked", engine="llm-provider-mocked")
-
-    async def generate(
-        self,
-        prefix: str,
-        suffix: Optional[str] = None,
-        stream: bool = False,
-        **kwargs: Any,
-    ) -> TextGenModelOutput | AsyncIterator[TextGenModelChunk]:
-        scope = {
-            "prefix": prefix,
-            "suffix": suffix,
-            "stream": stream,
-            "kwargs": dict(kwargs),
-        }
-        suggestion = (
-            f"echo: {json.dumps(scope)}"  # echo the current scope's local variables
-        )
-
-        with self.instrumentator.watch(stream=stream) as watcher:
-            if stream:
-                chunks = [
-                    TextGenModelChunk(text=chunk)
-                    for chunk in re.split(r"(\s)", suggestion)
-                ]
-                return AsyncStream(chunks, lambda: watcher.finish())
-
-        return TextGenModelOutput(
-            text=suggestion,
-            score=0,
-            safety_attributes=SafetyAttributes(),
-        )
-
-
-class ChatModel(ChatModelBase):
-    """
-    Implementation of the stub model that inherits the `ChatModelBase` interface.
-    Please, use this class if you require to mock such models as `AnthropicChatModel`
-    """
-
-    def __init__(self, *_args: Any, **_kwargs: Any):
-        super().__init__()
-
-    @property
-    def metadata(self) -> ModelMetadata:
-        return ModelMetadata(
-            name="chat-model-mocked", engine="chat-model-provider-mocked"
-        )
-
-    async def generate(
-        self,
-        messages: list[Message],
-        stream: bool = False,
-        **kwargs: Any,
-    ) -> TextGenModelOutput | AsyncIterator[TextGenModelChunk]:
-        messages = [message.model_dump(mode="json") for message in messages]
-
-        scope = {"messages": messages, "stream": stream, "kwargs": dict(kwargs)}
-        suggestion = (
-            f"echo: {json.dumps(scope)}"  # echo the current scope's local variables
-        )
-
-        with self.instrumentator.watch(stream=stream) as watcher:
-            if stream:
-                chunks = [
-                    TextGenModelChunk(text=chunk)
-                    for chunk in re.split(r"(\s)", suggestion)
-                ]
-                return AsyncStream(chunks, lambda: watcher.finish())
-
-        return TextGenModelOutput(
-            text=suggestion,
-            score=0,
-            safety_attributes=SafetyAttributes(),
-        )
diff --git a/ai_gateway/models/vertex_text.py b/ai_gateway/models/vertex_text.py
index 71b7698..b55bf51 100644
--- a/ai_gateway/models/vertex_text.py
+++ b/ai_gateway/models/vertex_text.py
@@ -16,7 +16,6 @@ from ai_gateway.models.base import (
     SafetyAttributes,
     TextGenBaseModel,
     TextGenModelOutput,
-    TokensConsumptionMetadata,
 )
 
 __all__ = [
@@ -156,17 +155,7 @@ class PalmCodeGenBaseModel(TextGenBaseModel):
                     timeout=self.timeout,
                 )
                 response = PredictResponse.to_dict(response)
-                tokens_metatada = response.get("metadata", {}).get("tokenMetadata", {})
-                log.debug(
-                    "codegen vertex response:",
-                    tokens_metatada=tokens_metatada,
-                    input_tokens=tokens_metatada.get("inputTokenCount", {}).get(
-                        "totalTokens", None
-                    ),
-                    output_tokens=tokens_metatada.get("outputTokenCount", {}).get(
-                        "totalTokens", None
-                    ),
-                )
+
                 predictions = response.get("predictions", [])
             except GoogleAPICallError as ex:
                 raise VertexAPIStatusError.from_exception(ex)
@@ -180,14 +169,6 @@ class PalmCodeGenBaseModel(TextGenBaseModel):
                 safety_attributes=SafetyAttributes(
                     **prediction.get("safetyAttributes", {})
                 ),
-                metadata=TokensConsumptionMetadata(
-                    input_tokens=tokens_metatada.get("inputTokenCount", {}).get(
-                        "totalTokens", None
-                    ),
-                    output_tokens=tokens_metatada.get("outputTokenCount", {}).get(
-                        "totalTokens", None
-                    ),
-                ),
             )
 
     @property
diff --git a/ai_gateway/profiling.py b/ai_gateway/profiling.py
index 621ee1f..20717dc 100644
--- a/ai_gateway/profiling.py
+++ b/ai_gateway/profiling.py
@@ -3,9 +3,10 @@ import os
 import googlecloudprofiler
 
 from ai_gateway.config import ConfigGoogleCloudProfiler
+from ai_gateway.tracking import log_exception
 
 
-def setup_profiling(google_cloud_profiler: ConfigGoogleCloudProfiler, logger):
+def setup_profiling(google_cloud_profiler: ConfigGoogleCloudProfiler):
     if not google_cloud_profiler.enabled:
         return
 
@@ -21,4 +22,4 @@ def setup_profiling(google_cloud_profiler: ConfigGoogleCloudProfiler, logger):
             period_ms=google_cloud_profiler.period_ms,
         )
     except (ValueError, NotImplementedError) as exc:
-        logger.error("failed to setup Google Cloud Profiler: %s", exc)
+        log_exception(exc)
diff --git a/ai_gateway/prompts/parsers/treesitter.py b/ai_gateway/prompts/parsers/treesitter.py
index ac39369..1c3dc3d 100644
--- a/ai_gateway/prompts/parsers/treesitter.py
+++ b/ai_gateway/prompts/parsers/treesitter.py
@@ -1,8 +1,8 @@
 import asyncio
+import os
 from typing import Optional
 
-from tree_sitter import Node, Tree
-from tree_sitter_languages import get_parser
+from tree_sitter import Language, Node, Parser, Tree
 
 from ai_gateway.code_suggestions.processing.ops import (
     LanguageId,
@@ -122,24 +122,32 @@ class CodeParser(BaseCodeParser):
         cls,
         content: str,
         lang_id: LanguageId,
+        lib_path: Optional[str] = None,
     ):
-        return await asyncio.to_thread(cls._from_language_id, content, lang_id)
+        return await asyncio.to_thread(
+            cls._from_language_id, content, lang_id, lib_path
+        )
 
     @classmethod
     def _from_language_id(
         cls,
         content: str,
         lang_id: LanguageId,
+        lib_path: Optional[str] = None,
     ):
+        if lib_path is None:
+            lib_path = "%s/tree-sitter-languages.so" % os.getenv("LIB_DIR", "/usr/lib")
+
         if lang_id is None:
             raise ValueError(f"Unsupported language: {lang_id}")
 
         lang_def = ProgramLanguage.from_language_id(lang_id)
 
         try:
-            parser = get_parser(lang_def.grammar_name)
+            parser = Parser()
+            parser.set_language(Language(lib_path, lang_def.grammar_name))
             tree = parser.parse(bytes(content, "utf8"))
-        except (AttributeError, TypeError) as ex:
+        except TypeError as ex:
             raise ValueError(f"Unsupported code content: {str(ex)}")
 
         return cls(tree, lang_id)
diff --git a/ai_gateway/tracking/instrumentator.py b/ai_gateway/tracking/instrumentator.py
index 7263651..1617168 100644
--- a/ai_gateway/tracking/instrumentator.py
+++ b/ai_gateway/tracking/instrumentator.py
@@ -1,4 +1,10 @@
-from ai_gateway.tracking import Client, SnowplowEvent
+from ai_gateway.instrumentators.base import Telemetry
+from ai_gateway.tracking import (
+    Client,
+    RequestCount,
+    SnowplowEvent,
+    SnowplowEventContext,
+)
 
 __all__ = ["SnowplowInstrumentator"]
 
@@ -9,6 +15,43 @@ class SnowplowInstrumentator:
 
     def watch(
         self,
-        snowplow_event: SnowplowEvent,
+        telemetry: list[Telemetry],
+        prefix_length: int,
+        suffix_length: int,
+        language: str,
+        user_agent: str,
+        gitlab_realm: str,
+        gitlab_instance_id: str,
+        gitlab_global_user_id: str,
+        gitlab_host_name: str,
+        gitlab_saas_namespace_ids: list[str],
     ) -> None:
+        request_counts = []
+        for stats in telemetry:
+            request_count = RequestCount(
+                requests=stats.requests,
+                accepts=stats.accepts,
+                errors=stats.errors,
+                lang=stats.lang,
+                model_engine=stats.model_engine,
+                model_name=stats.model_name,
+            )
+
+            request_counts.append(request_count)
+
+        snowplow_event = SnowplowEvent(
+            context=SnowplowEventContext(
+                request_counts=request_counts,
+                prefix_length=prefix_length,
+                suffix_length=suffix_length,
+                language=language,
+                user_agent=user_agent,
+                gitlab_realm=gitlab_realm,
+                gitlab_instance_id=gitlab_instance_id,
+                gitlab_global_user_id=gitlab_global_user_id,
+                gitlab_host_name=gitlab_host_name,
+                gitlab_saas_namespace_ids=[int(id) for id in gitlab_saas_namespace_ids],
+            )
+        )
+
         self.client.track(snowplow_event)
diff --git a/ai_gateway/tracking/snowplow.py b/ai_gateway/tracking/snowplow.py
index b23d61d..485d797 100644
--- a/ai_gateway/tracking/snowplow.py
+++ b/ai_gateway/tracking/snowplow.py
@@ -52,7 +52,6 @@ class SnowplowEventContext:
     gitlab_global_user_id: str
     gitlab_host_name: str
     gitlab_saas_namespace_ids: list[int]
-    gitlab_saas_duo_pro_namespace_ids: list[int]
 
 
 @dataclass
@@ -62,8 +61,6 @@ class SnowplowEvent:
     context: Optional[SnowplowEventContext] = None
     category: str = "code_suggestions"
     action: str = "suggestion_requested"
-    label: str = None
-    value: int = None
 
 
 class Client(ABC):
@@ -101,15 +98,9 @@ class SnowplowClient(Client):
             event: A domain event which is transformed to Snowplow StructuredEvent for tracking.
         """
         structured_event = StructuredEvent(
-            context=(
-                [SelfDescribingJson(self.SCHEMA, asdict(event.context))]
-                if event.context
-                else None
-            ),
+            context=[SelfDescribingJson(self.SCHEMA, asdict(event.context))],
             category=event.category,
             action=event.action,
-            label=event.label,
-            value=event.value,
         )
 
         self.tracker.track(structured_event)
diff --git a/lints/__init__.py b/lints/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/lints/unsafe_dependency_resolution.py b/lints/unsafe_dependency_resolution.py
deleted file mode 100644
index ae7d564..0000000
--- a/lints/unsafe_dependency_resolution.py
+++ /dev/null
@@ -1,32 +0,0 @@
-from astroid import nodes
-from pylint.checkers import BaseChecker
-from pylint.lint import PyLinter
-
-
-class UnsafeDependencyResolution(BaseChecker):
-    name = "unsafe-dependency-resolution"
-    msgs = {
-        "W5001": (
-            "Unsafe dependency resolution detected.",
-            "unsafe-dependency-resolution",
-            "See https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/ai-assist/-/merge_requests/606 for more information.",
-        )
-    }
-
-    def visit_call(self, node: nodes.Call) -> None:
-        if (
-            hasattr(node, "func")
-            and isinstance(node.func, nodes.Name)
-            and node.func.name == "Depends"
-        ):
-            subscript = node.args[0]
-            if (
-                hasattr(subscript, "value")
-                and isinstance(subscript.value, nodes.Name)
-                and subscript.value.name == "Provide"
-            ):
-                self.add_message("unsafe-dependency-resolution", node=node)
-
-
-def register(linter: "PyLinter") -> None:
-    linter.register_checker(UnsafeDependencyResolution(linter))
diff --git a/scripts/build-tree-sitter-lib.py b/scripts/build-tree-sitter-lib.py
new file mode 100755
index 0000000..c992240
--- /dev/null
+++ b/scripts/build-tree-sitter-lib.py
@@ -0,0 +1,141 @@
+#!/usr/bin/env python3
+
+import contextlib
+import os
+import platform
+import subprocess
+import sys
+from collections.abc import Generator
+from pathlib import Path
+
+from tree_sitter import Language
+
+LANGS = [
+    (
+        "tree-sitter-c",
+        "https://github.com/tree-sitter/tree-sitter-c",
+        "25371f9448b97c55b853a6ee8bb0bfb1bca6da9f",
+    ),
+    (
+        "tree-sitter-c-sharp",
+        "https://github.com/tree-sitter/tree-sitter-c-sharp",
+        "9c494a503c8e2044bfffce57f70b480c01a82f03",
+    ),
+    (
+        "tree-sitter-cpp",
+        "https://github.com/tree-sitter/tree-sitter-cpp",
+        "a90f170f92d5d70e7c2d4183c146e61ba5f3a457",
+    ),
+    (
+        "tree-sitter-go",
+        "https://github.com/tree-sitter/tree-sitter-go",
+        "bbaa67a180cfe0c943e50c55130918be8efb20bd",
+    ),
+    (
+        "tree-sitter-java",
+        "https://github.com/tree-sitter/tree-sitter-java",
+        "2b57cd9541f9fd3a89207d054ce8fbe72657c444",
+    ),
+    (
+        "tree-sitter-javascript",
+        "https://github.com/tree-sitter/tree-sitter-javascript",
+        "f1e5a09b8d02f8209a68249c93f0ad647b228e6e",
+    ),
+    (
+        "tree-sitter-kotlin",
+        "https://github.com/fwcd/tree-sitter-kotlin",
+        "16de60e6588ad39afe274b13cd494f97e0f953c7",
+    ),
+    (
+        "tree-sitter-php",
+        "https://github.com/tree-sitter/tree-sitter-php",
+        "33e30169e6f9bb29845c80afaa62a4a87f23f6d6",
+    ),
+    (
+        "tree-sitter-python",
+        "https://github.com/tree-sitter/tree-sitter-python",
+        "82f5c9937fe4300b4bec3ee0e788d642c77aab2c",
+    ),
+    (
+        "tree-sitter-ruby",
+        "https://github.com/tree-sitter/tree-sitter-ruby",
+        "f257f3f57833d584050336921773738a3fd8ca22",
+    ),
+    (
+        "tree-sitter-rust",
+        "https://github.com/tree-sitter/tree-sitter-rust",
+        "48e053397b587de97790b055a1097b7c8a4ef846",
+    ),
+    (
+        "tree-sitter-scala",
+        "https://github.com/tree-sitter/tree-sitter-scala",
+        "1b4c2fa5c55c5fd83cbb0d2f818f916aba221a42",
+    ),
+    (
+        "tree-sitter-typescript",
+        "https://github.com/tree-sitter/tree-sitter-typescript",
+        "d847898fec3fe596798c9fda55cb8c05a799001a",
+    ),
+]
+
+
+@contextlib.contextmanager
+def working_directory(path: Path) -> Generator[None, None, None]:
+    """Changes working directory and returns to previous on exit."""
+    prev_cwd = Path.cwd()
+    os.chdir(path)
+    try:
+        yield
+    finally:
+        os.chdir(prev_cwd)
+
+
+def main() -> int:
+    """Clone and build treesitter language libraries."""
+    scripts_dir = Path(__file__).resolve().parent
+    vendor_dir = scripts_dir / "vendor" / platform.platform()
+    lib_dir = scripts_dir / "lib"
+    print(f"Checking out grammars in {vendor_dir}")
+
+    if not vendor_dir.exists():
+        vendor_dir.mkdir(parents=True)
+
+    with working_directory(vendor_dir):
+        for name, url, commit in LANGS:
+            if (vendor_dir / name).exists():
+                print(f"Updating {name}")
+
+                subprocess.check_call(
+                    ["git", "fetch", "--depth=1", "origin", commit], cwd=name
+                )
+                subprocess.check_call(["git", "reset", "--hard", commit], cwd=name)
+            else:
+                print(f"Initializing {name}")
+
+                os.mkdir(name)
+                subprocess.check_call(["git", "init"], cwd=name)
+                subprocess.check_call(["git", "remote", "add", "origin", url], cwd=name)
+                subprocess.check_call(
+                    ["git", "fetch", "--depth=1", "origin", commit], cwd=name
+                )
+                subprocess.check_call(["git", "reset", "--hard", commit], cwd=name)
+
+    language_directories = [
+        str(vendor_dir / name)
+        for name, _, _ in LANGS
+        if name != "tree-sitter-typescript"
+    ]
+    language_directories += [
+        str(vendor_dir / "tree-sitter-typescript/typescript"),
+        str(vendor_dir / "tree-sitter-typescript/tsx"),
+    ]
+
+    lib = str(lib_dir / "tree-sitter-languages.so")
+    print(f"Building {lib}")
+    Language.build_library(lib, language_directories)
+
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/tests/api/test_server.py b/tests/api/test_server.py
index 6d8251c..b74124e 100644
--- a/tests/api/test_server.py
+++ b/tests/api/test_server.py
@@ -12,16 +12,25 @@ from ai_gateway.container import ContainerApplication
 _ROUTES_V1 = [
     ("/v1/chat/agent", ["POST"]),
     ("/v1/x-ray/libraries", ["POST"]),
+    # with the `ai` prefix
+    ("/ai/v1/chat/agent", ["POST"]),
+    ("/ai/v1/x-ray/libraries", ["POST"]),
 ]
 
 _ROUTES_V2 = [
     ("/v2/code/completions", ["POST"]),
     ("/v2/completions", ["POST"]),  # legacy path
     ("/v2/code/generations", ["POST"]),
+    # with the `ai` prefix
+    ("/ai/v2/code/completions", ["POST"]),
+    ("/ai/v2/completions", ["POST"]),  # legacy path
+    ("/ai/v2/code/generations", ["POST"]),
 ]
 
 _ROUTES_V3 = [
     ("/v3/code/completions", ["POST"]),
+    # with the `ai` prefix
+    ("/ai/v3/code/completions", ["POST"]),
 ]
 
 
diff --git a/tests/api/v1/test_v1_chat_agent.py b/tests/api/v1/test_v1_chat_agent.py
index 562f6ac..b8d5330 100644
--- a/tests/api/v1/test_v1_chat_agent.py
+++ b/tests/api/v1/test_v1_chat_agent.py
@@ -1,8 +1,9 @@
-from typing import Any, AsyncIterator, Type
+from typing import AsyncIterator, Type
 from unittest import mock
 from unittest.mock import AsyncMock, patch
 
 import pytest
+from dependency_injector import providers
 from fastapi.testclient import TestClient
 from structlog.testing import capture_logs
 
@@ -13,10 +14,8 @@ from ai_gateway.models import (
     AnthropicAPIConnectionError,
     AnthropicAPIStatusError,
     AnthropicAPITimeoutError,
-    AnthropicChatModel,
     AnthropicModel,
     KindAnthropicModel,
-    Message,
     ModelAPIError,
     SafetyAttributes,
     TextGenModelChunk,
@@ -37,307 +36,182 @@ def auth_user():
     )
 
 
-@pytest.fixture()
-def mock_models():
-    model_output = TextGenModelOutput(
-        text="test completion",
-        score=10000,
-        safety_attributes=SafetyAttributes(),
-    )
-    mock_llm_model = mock.Mock(spec=AnthropicModel)
-    mock_llm_model.generate = AsyncMock(return_value=model_output)
-
-    mock_chat_model = mock.Mock(spec=AnthropicChatModel)
-    mock_chat_model.generate = AsyncMock(return_value=model_output)
-
-    container = ContainerApplication()
-    with (
-        container.chat._anthropic_claude_llm_factory.override(mock_llm_model),
-        container.chat._anthropic_claude_chat_factory.override(mock_chat_model),
-    ):
-        yield {"llm": mock_llm_model, "chat": mock_chat_model}
-
-
-@pytest.fixture()
-def mock_models_stream():
-    async def _stream(*args: Any, **kwargs: Any) -> AsyncIterator[TextGenModelChunk]:
-        for chunk in ["test", " ", "completion"]:
-            yield TextGenModelChunk(text=chunk)
-
-    mock_llm_model = mock.Mock(spec=AnthropicModel)
-    mock_llm_model.generate = AsyncMock(side_effect=_stream)
-
-    mock_chat_model = mock.Mock(spec=AnthropicChatModel)
-    mock_chat_model.generate = AsyncMock(side_effect=_stream)
-
-    container = ContainerApplication()
-    with (
-        container.chat._anthropic_claude_llm_factory.override(mock_llm_model),
-        container.chat._anthropic_claude_chat_factory.override(mock_chat_model),
-    ):
-        yield {"llm": mock_llm_model, "chat": mock_chat_model}
-
-
 class TestAgentSuccessfulRequest:
     @pytest.mark.asyncio
     @pytest.mark.parametrize(
-        "request_body",
+        ("request_body", "expected_provider_args"),
         [
-            {
-                "prompt_components": [
-                    {
-                        "type": "prompt",
-                        "metadata": {
-                            "source": "gitlab-rails-sm",
-                            "version": "16.5.0-ee",
-                        },
-                        "payload": {
-                            "content": "\n\nHuman: hello, what is your name?\n\nAssistant:",
-                            "provider": "anthropic",
-                            "model": "claude-2.0",
-                        },
-                    },
-                ]
-            },
-            {
-                "prompt_components": [
-                    {
-                        "type": "prompt",
-                        "metadata": {
-                            "source": "gitlab-rails-sm",
-                            "version": "16.5.0-ee",
-                        },
-                        "payload": {
-                            "content": "\n\nHuman: hello, what is your name?\n\nAssistant:",
-                            "provider": "anthropic",
-                            "model": "claude-2.1",
-                            "params": {
-                                "temperature": 0.3,
-                                "stop_sequences": ["\n\nHuman", "Observation:"],
-                                "max_tokens_to_sample": 2048,
+            (
+                {
+                    "prompt_components": [
+                        {
+                            "type": "prompt",
+                            "metadata": {
+                                "source": "gitlab-rails-sm",
+                                "version": "16.5.0-ee",
                             },
-                        },
-                    },
-                ]
-            },
-            {
-                "prompt_components": [
-                    {
-                        "type": "prompt",
-                        "metadata": {
-                            "source": "gitlab-rails-sm",
-                            "version": "16.5.0-ee",
-                        },
-                        "payload": {
-                            "content": [
-                                {
-                                    "role": "system",
-                                    "content": "You are a Python engineer",
-                                },
-                                {
-                                    "role": "user",
-                                    "content": "define a function that adds numbers together",
-                                },
-                            ],
-                            "provider": "anthropic",
-                            "model": "claude-3-opus-20240229",
-                            "params": {
-                                "temperature": 0.3,
-                                "stop_sequences": ["\n\nHuman", "Observation:"],
-                                "max_tokens_to_sample": 2048,
+                            "payload": {
+                                "content": "\n\nHuman: hello, what is your name?\n\nAssistant:",
+                                "provider": "anthropic",
+                                "model": "claude-2.0",
                             },
                         },
-                    },
-                ]
-            },
-            {
-                "prompt_components": [
-                    {
-                        "type": "prompt",
-                        "metadata": {
-                            "source": "gitlab-rails-sm",
-                            "version": "16.5.0-ee",
-                        },
-                        "payload": {
-                            "content": [
-                                {
-                                    "role": "system",
-                                    "content": "You are a Python engineer",
-                                },
-                                {
-                                    "role": "user",
-                                    "content": "define a function that adds numbers together",
-                                },
-                            ],
-                            "provider": "anthropic",
-                            "model": "claude-3-sonnet-20240229",
-                            "params": {
-                                "temperature": 0.3,
-                                "stop_sequences": ["\n\nHuman", "Observation:"],
-                                "max_tokens_to_sample": 2048,
+                    ]
+                },
+                {
+                    "name": KindAnthropicModel.CLAUDE_2_0,
+                },
+            ),
+            (
+                {
+                    "prompt_components": [
+                        {
+                            "type": "prompt",
+                            "metadata": {
+                                "source": "gitlab-rails-sm",
+                                "version": "16.5.0-ee",
                             },
-                        },
-                    },
-                ]
-            },
-            {
-                "prompt_components": [
-                    {
-                        "type": "prompt",
-                        "metadata": {
-                            "source": "gitlab-rails-sm",
-                            "version": "16.5.0-ee",
-                        },
-                        "payload": {
-                            "content": [
-                                {
-                                    "role": "system",
-                                    "content": "You are a Python engineer",
-                                },
-                                {
-                                    "role": "user",
-                                    "content": "define a function that adds numbers together",
+                            "payload": {
+                                "content": "\n\nHuman: hello, what is your name?\n\nAssistant:",
+                                "provider": "anthropic",
+                                "model": "claude-2.1",
+                                "params": {
+                                    "temperature": 0.3,
+                                    "stop_sequences": ["\n\nHuman", "Observation:"],
+                                    "max_tokens_to_sample": 1024,
                                 },
-                            ],
-                            "provider": "anthropic",
-                            "model": "claude-3-haiku-20240307",
-                            "params": {
-                                "temperature": 0.3,
-                                "stop_sequences": ["\n\nHuman", "Observation:"],
-                                "max_tokens_to_sample": 2048,
                             },
                         },
-                    },
-                ]
-            },
-            {
-                "prompt_components": [
-                    {
-                        "type": "prompt",
-                        "metadata": {
-                            "source": "gitlab-rails-sm",
-                            "version": "16.5.0-ee",
-                        },
-                        "payload": {
-                            "content": [
-                                {
-                                    "role": "system",
-                                    "content": "You are a Python engineer",
-                                },
-                                {
-                                    "role": "user",
-                                    "content": "define a function that adds numbers together",
-                                },
-                            ],
-                            "provider": "anthropic",
-                            "model": "claude-3-haiku-20240307",
-                        },
-                    },
-                ]
-            },
+                    ]
+                },
+                {
+                    "name": KindAnthropicModel.CLAUDE_2_1,
+                    "temperature": 0.3,
+                    "stop_sequences": ["\n\nHuman", "Observation:"],
+                    "max_tokens_to_sample": 1024,
+                },
+            ),
         ],
     )
     async def test_successful_response(
-        self,
-        mock_client: TestClient,
-        mock_models: dict,
-        request_body: dict,
+        self, mock_client: TestClient, request_body: dict, expected_provider_args: dict
     ):
-        response = mock_client.post(
-            "/chat/agent",
-            headers={
-                "Authorization": "Bearer 12345",
-                "X-Gitlab-Authentication-Type": "oidc",
-            },
-            json=request_body,
+        mock_model = mock.Mock(spec=AnthropicModel)
+        mock_model.generate = AsyncMock(
+            return_value=TextGenModelOutput(
+                text="test completion",
+                score=10000,
+                safety_attributes=SafetyAttributes(),
+            )
         )
 
+        container = ContainerApplication()
+        with container.chat.anthropic_claude_factory.override(mock_model):
+            response = mock_client.post(
+                "/chat/agent",
+                headers={
+                    "Authorization": "Bearer 12345",
+                    "X-Gitlab-Authentication-Type": "oidc",
+                },
+                json=request_body,
+            )
+
         assert response.status_code == 200
         assert response.json()["response"] == "test completion"
 
         response_metadata = response.json()["metadata"]
-        prompt_payload = request_body["prompt_components"][0]["payload"]
-        prompt_params = prompt_payload.get("params", {})
-
         assert response_metadata["provider"] == "anthropic"
-        assert response_metadata["model"] == prompt_payload["model"]
-
-        if isinstance(prompt_payload["content"], str):
-            mock_models["llm"].generate.assert_called_with(
-                prefix=prompt_payload["content"], stream=False, **prompt_params
-            )
-        else:
-            messages = [Message(**message) for message in prompt_payload["content"]]
-            if max_tokens := prompt_params.pop("max_tokens_to_sample", None):
-                prompt_params["max_tokens"] = max_tokens
+        assert (
+            response_metadata["model"]
+            == request_body["prompt_components"][0]["payload"]["model"]
+        )
 
-            mock_models["chat"].generate.assert_called_with(
-                messages=messages, stream=False, **prompt_params
-            )
+        mock_model.generate.assert_called_with(
+            prefix="\n\nHuman: hello, what is your name?\n\nAssistant:",
+            _suffix="",
+            stream=False,
+        )
 
 
 class TestAgentSuccessfulStream:
     @pytest.mark.asyncio
     @pytest.mark.parametrize(
-        "payload_content",
+        ("model_chunks", "expected_response"),
         [
-            "\n\nHuman: hello, what is your name?\n\nAssistant:",
-            [{"role": "user", "content": "hello, what is your name?"}],
+            (
+                [
+                    TextGenModelChunk(
+                        text="test",
+                    ),
+                    TextGenModelChunk(
+                        text=" ",
+                    ),
+                    TextGenModelChunk(
+                        text="completion",
+                    ),
+                ],
+                "test completion",
+            ),
         ],
     )
     async def test_successful_stream(
         self,
         mock_client: TestClient,
-        mock_models_stream: dict,
-        payload_content: str | list[dict],
+        model_chunks: list[TextGenModelChunk],
+        expected_response: str,
     ):
-        response = mock_client.post(
-            "/chat/agent",
-            headers={
-                "Authorization": "Bearer 12345",
-                "X-Gitlab-Authentication-Type": "oidc",
-            },
-            json={
-                "prompt_components": [
-                    {
-                        "type": "prompt",
-                        "metadata": {
-                            "source": "gitlab-rails-sm",
-                            "version": "16.5.0-ee",
-                        },
-                        "payload": {
-                            "content": payload_content,
-                            "provider": "anthropic",
-                            "model": KindAnthropicModel.CLAUDE_2_0.value,
+        async def _stream_generator(
+            prefix, _suffix, stream
+        ) -> AsyncIterator[TextGenModelChunk]:
+            for chunk in model_chunks:
+                yield chunk
+
+        model_name = KindAnthropicModel.CLAUDE_2_0
+        mock_model = mock.Mock(spec=AnthropicModel)
+        mock_model.generate = AsyncMock(side_effect=_stream_generator)
+
+        container = ContainerApplication()
+        with container.chat.anthropic_claude_factory.override(mock_model):
+            response = mock_client.post(
+                "/chat/agent",
+                headers={
+                    "Authorization": "Bearer 12345",
+                    "X-Gitlab-Authentication-Type": "oidc",
+                },
+                json={
+                    "prompt_components": [
+                        {
+                            "type": "prompt",
+                            "metadata": {
+                                "source": "gitlab-rails-sm",
+                                "version": "16.5.0-ee",
+                            },
+                            "payload": {
+                                "content": "\n\nHuman: hello, what is your name?\n\nAssistant:",
+                                "provider": "anthropic",
+                                "model": model_name.value,
+                            },
                         },
-                    },
-                ],
-                "stream": "True",
-            },
-        )
+                    ],
+                    "stream": "True",
+                },
+            )
 
         assert response.status_code == 200
-        assert response.text == "test completion"
+        assert response.text == expected_response
         assert response.headers["content-type"] == "text/event-stream; charset=utf-8"
 
-        if isinstance(payload_content, str):
-            mock_models_stream["llm"].generate.assert_called_with(
-                prefix=payload_content,
-                stream=True,
-            )
-        else:
-            messages = [Message(**content) for content in payload_content]
-            mock_models_stream["chat"].generate.assert_called_with(
-                messages=messages,
-                stream=True,
-            )
+        mock_model.generate.assert_called_with(
+            prefix="\n\nHuman: hello, what is your name?\n\nAssistant:",
+            _suffix="",
+            stream=True,
+        )
 
 
 class TestAgentUnsupportedProvider:
     def test_invalid_request(
         self,
         mock_client: TestClient,
-        mock_models: dict,
     ):
         response = mock_client.post(
             "/chat/agent",
@@ -367,7 +241,10 @@ class TestAgentUnsupportedProvider:
 
 
 class TestAgentUnsupportedModel:
-    def test_invalid_request(self, mock_client: TestClient, mock_models: dict):
+    def test_invalid_request(
+        self,
+        mock_client: TestClient,
+    ):
         response = mock_client.post(
             "/chat/agent",
             headers={
@@ -406,7 +283,6 @@ class TestAnthropicInvalidScope:
     def test_invalid_scope(
         self,
         mock_client: TestClient,
-        mock_models: dict,
     ):
         response = mock_client.post(
             "/chat/agent",
@@ -440,7 +316,6 @@ class TestAgentInvalidRequestMissingFields:
     def test_invalid_request_missing_fields(
         self,
         mock_client: TestClient,
-        mock_models: dict,
     ):
         response = mock_client.post(
             "/chat/agent",
@@ -470,14 +345,14 @@ class TestAgentInvalidRequestMissingFields:
                     "loc": ["body", "prompt_components", 0, "metadata", "version"],
                     "msg": "Field required",
                     "input": {"source": "gitlab-rails-sm"},
-                    "url": "https://errors.pydantic.dev/2.6/v/missing",
+                    "url": "https://errors.pydantic.dev/2.5/v/missing",
                 },
                 {
                     "type": "missing",
                     "loc": ["body", "prompt_components", 0, "payload", "content"],
                     "msg": "Field required",
                     "input": {"provider": "anthropic", "model": "claude-2.0"},
-                    "url": "https://errors.pydantic.dev/2.6/v/missing",
+                    "url": "https://errors.pydantic.dev/2.5/v/missing",
                 },
             ]
         }
@@ -487,7 +362,6 @@ class TestAgentInvalidRequestManyPromptComponents:
     def test_invalid_request_many_prompt_components(
         self,
         mock_client: TestClient,
-        mock_models: dict,
     ):
         response = mock_client.post(
             "/chat/agent",
@@ -556,7 +430,7 @@ class TestAgentInvalidRequestManyPromptComponents:
                         },
                     ],
                     "ctx": {"field_type": "List", "max_length": 1, "actual_length": 2},
-                    "url": "https://errors.pydantic.dev/2.6/v/too_long",
+                    "url": "https://errors.pydantic.dev/2.5/v/too_long",
                 }
             ]
         }
@@ -565,21 +439,15 @@ class TestAgentInvalidRequestManyPromptComponents:
 class TestAgentUnsuccessfulAnthropicRequest:
     @pytest.mark.asyncio
     @pytest.mark.parametrize(
-        ("model_class", "model_exception_type"),
+        "model_exception_type",
         [
-            (AnthropicModel, AnthropicAPIStatusError),
-            (AnthropicModel, AnthropicAPITimeoutError),
-            (AnthropicModel, AnthropicAPIConnectionError),
-            (AnthropicChatModel, AnthropicAPIStatusError),
-            (AnthropicChatModel, AnthropicAPITimeoutError),
-            (AnthropicChatModel, AnthropicAPIConnectionError),
+            AnthropicAPIStatusError,
+            AnthropicAPITimeoutError,
+            AnthropicAPIConnectionError,
         ],
     )
     async def test_fail_receiving_anthropic_response(
-        self,
-        mock_client: TestClient,
-        model_class: Type[AnthropicModel | AnthropicChatModel],
-        model_exception_type: Type[ModelAPIError],
+        self, mock_client: TestClient, model_exception_type: Type[ModelAPIError]
     ):
         def _side_effect(*_args, **_kwargs):
             raise exception
@@ -588,17 +456,20 @@ class TestAgentUnsuccessfulAnthropicRequest:
             model_exception_type.code = 404
         exception = model_exception_type("exception message")
 
-        mock_model = mock.Mock(spec=model_class)
-        mock_model.generate = AsyncMock(side_effect=_side_effect)
+        mock_model = mock.Mock(spec=AnthropicModel)
+        mock_model.generate = AsyncMock(
+            side_effect=_side_effect,
+            return_value=TextGenModelOutput(
+                text="test completion",
+                score=10000,
+                safety_attributes=SafetyAttributes(),
+            ),
+        )
 
         container = ContainerApplication()
-        with (
-            # override both models at the same time to avoid unnecessary if-else constructions
-            container.chat._anthropic_claude_llm_factory.override(mock_model),
-            container.chat._anthropic_claude_chat_factory.override(mock_model),
-            patch("ai_gateway.api.v1.chat.agent.log_exception") as mock_log_exception,
-            capture_logs(),
-        ):
+        with container.chat.anthropic_claude_factory.override(mock_model), patch(
+            "ai_gateway.api.v1.chat.agent.log_exception"
+        ) as mock_log_exception, capture_logs() as cap_logs:
             response = mock_client.post(
                 "/chat/agent",
                 headers={
diff --git a/tests/api/v1/test_v1_xray_libraries.py b/tests/api/v1/test_v1_xray_libraries.py
index e87bf6d..6e8061a 100644
--- a/tests/api/v1/test_v1_xray_libraries.py
+++ b/tests/api/v1/test_v1_xray_libraries.py
@@ -101,29 +101,26 @@ class TestUnauthorizedScopes:
 
     @pytest.mark.parametrize("path", ["/x-ray/libraries"])
     def test_failed_authorization_scope(self, mock_client, path):
-        container = ContainerApplication()
-
-        with container.x_ray.anthropic_claude.override(mock.Mock()):
-            response = mock_client.post(
-                path,
-                headers={
-                    "Authorization": "Bearer 12345",
-                    "X-Gitlab-Authentication-Type": "oidc",
-                },
-                json={
-                    "prompt_components": [
-                        {
-                            "type": "x_ray_package_file_prompt",
-                            "payload": {
-                                "prompt": "Human: Parse Gemfile content: `gem kaminari`. Respond using only valid JSON with list of libraries",
-                                "provider": "anthropic",
-                                "model": "claude-2.0",
-                            },
-                            "metadata": {"scannerVersion": "0.0.1"},
-                        }
-                    ]
-                },
-            )
+        response = mock_client.post(
+            path,
+            headers={
+                "Authorization": "Bearer 12345",
+                "X-Gitlab-Authentication-Type": "oidc",
+            },
+            json={
+                "prompt_components": [
+                    {
+                        "type": "x_ray_package_file_prompt",
+                        "payload": {
+                            "prompt": "Human: Parse Gemfile content: `gem kaminari`. Respond using only valid JSON with list of libraries",
+                            "provider": "anthropic",
+                            "model": "claude-2.0",
+                        },
+                        "metadata": {"scannerVersion": "0.0.1"},
+                    }
+                ]
+            },
+        )
 
         assert response.status_code == 403
         assert response.json() == {"detail": "Forbidden"}
diff --git a/tests/api/v2/test_v2_mocks.py b/tests/api/v2/test_fake_models.py
similarity index 80%
rename from tests/api/v2/test_v2_mocks.py
rename to tests/api/v2/test_fake_models.py
index 0d5268a..efe1836 100644
--- a/tests/api/v2/test_v2_mocks.py
+++ b/tests/api/v2/test_fake_models.py
@@ -10,16 +10,10 @@ from ai_gateway.code_suggestions import CodeCompletionsLegacy, CodeGenerations
 from ai_gateway.code_suggestions.processing import ModelEngineCompletions
 from ai_gateway.code_suggestions.processing.post.completions import PostProcessor
 from ai_gateway.code_suggestions.processing.pre import TokenizerTokenStrategy
+from ai_gateway.container import ContainerApplication
 from ai_gateway.experimentation import ExperimentRegistry
-from ai_gateway.models import (
-    AnthropicModel,
-    KindAnthropicModel,
-    KindVertexTextModel,
-    PalmCodeGeckoModel,
-)
-from ai_gateway.models.mock import LLM, ChatModel
+from ai_gateway.models import FakePalmTextGenModel
 from ai_gateway.tokenizer import init_tokenizer
-from ai_gateway.tracking.instrumentator import SnowplowInstrumentator
 
 
 @pytest.fixture(scope="class")
@@ -35,10 +29,10 @@ def auth_user():
     )
 
 
-class TestMockedModels:
-    # Verify mocked models with most used routes
+class TestFakeModels:
+    # Verify fake models with most used routes
 
-    def test_completions(
+    def test_fake_completions(
         self,
         mock_client: TestClient,
         mock_container: containers.DeclarativeContainer,
@@ -46,15 +40,14 @@ class TestMockedModels:
         """Completions: v1 with Vertex AI models."""
 
         engine = ModelEngineCompletions(
-            model=LLM(),
-            tokenization_strategy=TokenizerTokenStrategy(init_tokenizer()),
+            model=FakePalmTextGenModel(),
+            tokenizer=init_tokenizer(),
             experiment_registry=ExperimentRegistry(),
         )
 
         code_completions_mock = CodeCompletionsLegacy(
             engine=engine,
             post_processor=PostProcessor,
-            snowplow_instrumentator=mock.Mock(spec=SnowplowInstrumentator),
         )
 
         with mock_container.code_suggestions.completions.vertex_legacy.override(
@@ -81,7 +74,7 @@ class TestMockedModels:
         assert response.status_code == 200
 
         body = response.json()
-        assert body["choices"][0]["text"].startswith("echo:")
+        assert body["choices"][0]["text"] == "fake code suggestion from PaLM Text"
 
     def test_fake_generations(
         self, mock_client: TestClient, mock_container: containers.DeclarativeContainer
@@ -91,9 +84,7 @@ class TestMockedModels:
         tokenization_strategy = TokenizerTokenStrategy(init_tokenizer())
 
         code_generations_mock = CodeGenerations(
-            model=LLM(),
-            tokenization_strategy=tokenization_strategy,
-            snowplow_instrumentator=mock.Mock(spec=SnowplowInstrumentator),
+            model=FakePalmTextGenModel(), tokenization_strategy=tokenization_strategy
         )
 
         with mock_container.code_suggestions.generations.anthropic_factory.override(
@@ -122,4 +113,4 @@ class TestMockedModels:
         assert response.status_code == 200
 
         body = response.json()
-        assert body["choices"][0]["text"].startswith("echo:")
+        assert body["choices"][0]["text"] == "fake code suggestion from PaLM Text"
diff --git a/tests/api/v2/test_v2_code.py b/tests/api/v2/test_v2_code.py
index 82dbca7..933d209 100644
--- a/tests/api/v2/test_v2_code.py
+++ b/tests/api/v2/test_v2_code.py
@@ -1,13 +1,15 @@
-from typing import AsyncIterator, Dict, List, Union
+from typing import AsyncIterator
 from unittest import mock
 
 import pytest
 from dependency_injector import containers
+from fastapi import Request
 from fastapi.testclient import TestClient
 from snowplow_tracker import Snowplow
-from starlette.datastructures import CommaSeparatedStrings
 
 from ai_gateway.api.v2 import api_router
+from ai_gateway.api.v2.code.completions import track_snowplow_event
+from ai_gateway.api.v2.code.typing import CurrentFile, SuggestionsRequest
 from ai_gateway.auth import User, UserClaims
 from ai_gateway.code_suggestions import (
     CodeCompletions,
@@ -22,17 +24,11 @@ from ai_gateway.code_suggestions.processing.typing import (
     MetadataCodeContent,
     MetadataPromptBuilder,
 )
+from ai_gateway.container import ContainerApplication
 from ai_gateway.experimentation.base import ExperimentTelemetry
+from ai_gateway.instrumentators.base import Telemetry
 from ai_gateway.models import ModelMetadata
-from ai_gateway.models.base import TokensConsumptionMetadata
-from ai_gateway.models.base_chat import Message, Role
-from ai_gateway.tracking.container import ContainerTracking
 from ai_gateway.tracking.instrumentator import SnowplowInstrumentator
-from ai_gateway.tracking.snowplow import (
-    RequestCount,
-    SnowplowEvent,
-    SnowplowEventContext,
-)
 
 
 @pytest.fixture(scope="class")
@@ -49,11 +45,6 @@ def auth_user():
 
 
 class TestCodeCompletions:
-    def cleanup(self):
-        """Ensure Snowplow cache is reset between tests."""
-        yield
-        Snowplow.reset()
-
     @pytest.mark.parametrize(
         ("model_output", "expected_response"),
         [
@@ -73,9 +64,6 @@ class TestCodeCompletions:
                             ExperimentTelemetry(name="truncate_suffix", variant=1)
                         ],
                     ),
-                    tokens_consumption_metadata=TokensConsumptionMetadata(
-                        input_tokens=0, output_tokens=0
-                    ),
                 ),
                 {
                     "id": "id",
@@ -112,9 +100,6 @@ class TestCodeCompletions:
                             ExperimentTelemetry(name="truncate_suffix", variant=1)
                         ],
                     ),
-                    tokens_consumption_metadata=TokensConsumptionMetadata(
-                        input_tokens=0, output_tokens=0
-                    ),
                 ),
                 {
                     "id": "id",
@@ -140,6 +125,7 @@ class TestCodeCompletions:
     ):
         code_completions_mock = mock.Mock(spec=CodeCompletionsLegacy)
         code_completions_mock.execute = mock.AsyncMock(return_value=model_output)
+
         with mock_container.code_suggestions.completions.vertex_legacy.override(
             code_completions_mock
         ):
@@ -391,136 +377,151 @@ class TestCodeCompletions:
             stream=True,
         )
 
+
+class TestSnowplowInstrumentator:
+    @pytest.fixture(scope="class", autouse=True)
+    def cleanup(self):
+        """Ensure Snowplow cache is reset between tests."""
+        yield
+        Snowplow.reset()
+
     @pytest.mark.parametrize(
         (
-            "telemetry",
-            "current_file",
             "request_headers",
-            "expected_language",
+            "jwt_realm_claim",
+            "expected_instance_id",
+            "expected_user_id",
+            "expected_gitlab_host_name",
+            "expected_gitlab_saas_namespace_ids",
+            "expected_realm",
         ),
         [
             (
-                [
-                    {
-                        "model_engine": "vertex",
-                        "model_name": "code-gecko",
-                        "requests": 1,
-                        "accepts": 1,
-                        "errors": 0,
-                        "lang": None,
-                    }
-                ],
-                {
-                    "file_name": "main.py",
-                    "content_above_cursor": "# Create a fast binary search\n",
-                    "content_below_cursor": "\n",
-                },
                 {
                     "User-Agent": "vs-code",
                     "X-Gitlab-Instance-Id": "9ebada7a-f5e2-477a-8609-17797fa95cb9",
                     "X-Gitlab-Global-User-Id": "XTuMnZ6XTWkP3yh0ZwXualmOZvm2Gg/bk9jyfkL7Y6k=",
                     "X-Gitlab-Host-Name": "gitlab.com",
                     "X-Gitlab-Saas-Namespace-Ids": "1,2,3",
-                    "X-Gitlab-Saas-Duo-Pro-Namespace-Ids": "4,5,6",
                     "X-Gitlab-Realm": "saas",
                 },
-                "python",
-            )
+                None,
+                "9ebada7a-f5e2-477a-8609-17797fa95cb9",
+                "XTuMnZ6XTWkP3yh0ZwXualmOZvm2Gg/bk9jyfkL7Y6k=",
+                "gitlab.com",
+                ["1", "2", "3"],
+                "saas",
+            ),
+            (
+                {
+                    "User-Agent": "vs-code",
+                    "X-Gitlab-Instance-Id": "9ebada7a-f5e2-477a-8609-17797fa95cb9",
+                    "X-Gitlab-Global-User-Id": "XTuMnZ6XTWkP3yh0ZwXualmOZvm2Gg/bk9jyfkL7Y6k=",
+                    "X-Gitlab-Host-Name": "awesome-org.com",
+                    "X-Gitlab-Realm": "self-managed",
+                },
+                "saas",
+                "9ebada7a-f5e2-477a-8609-17797fa95cb9",
+                "XTuMnZ6XTWkP3yh0ZwXualmOZvm2Gg/bk9jyfkL7Y6k=",
+                "awesome-org.com",
+                [],
+                "self-managed",
+            ),
+            (
+                {
+                    "User-Agent": "vs-code",
+                    "X-Gitlab-Instance-Id": "9ebada7a-f5e2-477a-8609-17797fa95cb9",
+                    "X-Gitlab-Global-User-Id": "XTuMnZ6XTWkP3yh0ZwXualmOZvm2Gg/bk9jyfkL7Y6k=",
+                    "X-Gitlab-Host-Name": "gitlab.com",
+                    "X-Gitlab-Saas-Namespace-Ids": "1",
+                },
+                "saas",
+                "9ebada7a-f5e2-477a-8609-17797fa95cb9",
+                "XTuMnZ6XTWkP3yh0ZwXualmOZvm2Gg/bk9jyfkL7Y6k=",
+                "gitlab.com",
+                ["1"],
+                "saas",
+            ),
+            (
+                {
+                    "User-Agent": "vs-code",
+                },
+                None,
+                "",
+                "",
+                "",
+                [],
+                "",
+            ),
         ],
     )
-    def test_snowplow_tracking(
+    def test_track_snowplow_event(
         self,
-        mock_client: TestClient,
-        mock_container: containers.DeclarativeContainer,
-        telemetry: List[Dict[str, Union[str, int, None]]],
-        current_file: Dict[str, str],
-        expected_language: str,
-        request_headers: Dict[str, str],
+        request_headers,
+        jwt_realm_claim,
+        expected_instance_id,
+        expected_user_id,
+        expected_gitlab_host_name,
+        expected_gitlab_saas_namespace_ids,
+        expected_realm,
     ):
-        expected_event = SnowplowEvent(
-            context=SnowplowEventContext(
-                request_counts=[RequestCount(**rc) for rc in telemetry],
-                prefix_length=len(current_file.get("content_above_cursor", "")),
-                suffix_length=len(current_file.get("content_below_cursor", "")),
-                language=expected_language,
-                user_agent=request_headers.get("User-Agent", ""),
-                gitlab_realm=request_headers.get("X-Gitlab-Realm", ""),
-                gitlab_instance_id=request_headers.get("X-Gitlab-Instance-Id", ""),
-                gitlab_global_user_id=request_headers.get(
-                    "X-Gitlab-Global-User-Id", ""
-                ),
-                gitlab_host_name=request_headers.get("X-Gitlab-Host-Name", ""),
-                gitlab_saas_namespace_ids=list(
-                    CommaSeparatedStrings(
-                        request_headers.get("X-Gitlab-Saas-Namespace-Ids", "")
-                    )
-                ),
-                gitlab_saas_duo_pro_namespace_ids=list(
-                    CommaSeparatedStrings(
-                        request_headers.get("X-Gitlab-Saas-Duo-Pro-Namespace-Ids", "")
-                    )
-                ),
-            )
+        mock_request = mock.Mock(spec=Request)
+
+        mock_instrumentator = mock.Mock(spec=SnowplowInstrumentator)
+        mock_request.headers = request_headers
+        mock_request.user.claims.gitlab_realm = jwt_realm_claim
+
+        telemetry_1 = Telemetry(
+            requests=1,
+            accepts=2,
+            errors=3,
+            lang="python",
+            model_engine="vertex",
+            model_name="code-gecko",
         )
-
-        model_output = ModelEngineOutput(
-            text="def search",
-            score=0,
-            model=ModelMetadata(name="code-gecko", engine="vertex-ai"),
-            lang_id=LanguageId.PYTHON,
-            metadata=MetadataPromptBuilder(
-                components={
-                    "prefix": MetadataCodeContent(length=10, length_tokens=2),
-                    "suffix": MetadataCodeContent(length=10, length_tokens=2),
-                },
-                experiments=[ExperimentTelemetry(name="truncate_suffix", variant=1)],
-            ),
-            tokens_consumption_metadata=TokensConsumptionMetadata(
-                input_tokens=0, output_tokens=0
-            ),
+        telemetry_2 = Telemetry(
+            requests=4,
+            accepts=5,
+            errors=6,
+            lang="golang",
+            model_engine="vertex",
+            model_name="text-bison",
         )
 
-        code_completions_mock = mock.Mock(spec=CodeCompletionsLegacy)
-        code_completions_mock.execute = mock.AsyncMock(return_value=model_output)
-
-        snowplow_instrumentator_mock = mock.Mock(spec=SnowplowInstrumentator)
+        test_telemetry = [telemetry_1, telemetry_2]
 
-        snowplow_container_mock = mock.Mock(spec=ContainerTracking)
-        snowplow_container_mock.instrumentator = mock.Mock(
-            return_value=snowplow_instrumentator_mock
+        suggestion_request = SuggestionsRequest(
+            current_file=CurrentFile(
+                content_above_cursor="123",
+                content_below_cursor="123456",
+                file_name="foobar.py",
+            ),
+            telemetry=test_telemetry,
+        )
+        track_snowplow_event(
+            req=mock_request,
+            payload=suggestion_request,
+            snowplow_instrumentator=mock_instrumentator,
         )
 
-        with mock_container.code_suggestions.completions.vertex_legacy.override(
-            code_completions_mock
-        ), mock.patch.object(mock_container, "snowplow", snowplow_container_mock):
-            mock_client.post(
-                "/completions",
-                headers={
-                    "Authorization": "Bearer 12345",
-                    "X-Gitlab-Authentication-Type": "oidc",
-                    **request_headers,
-                },
-                json={
-                    "prompt_version": 1,
-                    "project_path": "gitlab-org/gitlab",
-                    "project_id": 278964,
-                    "current_file": current_file,
-                    "telemetry": telemetry,
-                },
-            )
-
-        snowplow_instrumentator_mock.watch.assert_called_once()
-        args = snowplow_instrumentator_mock.watch.call_args[0]
-        assert len(args) == 1
-        assert args[0] == expected_event
+        mock_instrumentator.watch.assert_called_once()
+        args = mock_instrumentator.watch.call_args[1]
+        assert len(args) == 10
+        assert len(args["telemetry"]) == 2
+        assert args["telemetry"][0].__dict__ == telemetry_1.__dict__
+        assert args["telemetry"][1].__dict__ == telemetry_2.__dict__
+        assert args["prefix_length"] == 3
+        assert args["suffix_length"] == 6
+        assert args["language"] == "python"
+        assert args["user_agent"] == "vs-code"
+        assert args["gitlab_realm"] == expected_realm
+        assert args["gitlab_instance_id"] == expected_instance_id
+        assert args["gitlab_global_user_id"] == expected_user_id
+        assert args["gitlab_host_name"] == expected_gitlab_host_name
+        assert args["gitlab_saas_namespace_ids"] == expected_gitlab_saas_namespace_ids
 
 
 class TestCodeGenerations:
-    def cleanup(self):
-        """Ensure Snowplow cache is reset between tests."""
-        yield
-        Snowplow.reset()
-
     @pytest.mark.parametrize(
         (
             "prompt_version",
@@ -531,7 +532,6 @@ class TestCodeGenerations:
             "model_output_text",
             "want_vertex_called",
             "want_anthropic_called",
-            "want_anthropic_chat_called",
             "want_vertex_prompt_prepared_called",
             "want_anthropic_prompt_prepared_called",
             "want_status",
@@ -550,7 +550,6 @@ class TestCodeGenerations:
                 False,
                 False,
                 False,
-                False,
                 200,
                 None,
                 [{"text": "foo", "index": 0, "finish_reason": "length"}],
@@ -566,7 +565,6 @@ class TestCodeGenerations:
                 True,
                 False,
                 False,
-                False,
                 200,
                 None,
                 [{"text": "foo", "index": 0, "finish_reason": "length"}],
@@ -582,7 +580,6 @@ class TestCodeGenerations:
                 False,
                 False,
                 False,
-                False,
                 200,
                 None,
                 [{"text": "foo", "index": 0, "finish_reason": "length"}],
@@ -598,7 +595,6 @@ class TestCodeGenerations:
                 False,
                 False,
                 False,
-                False,
                 200,
                 None,
                 [{"text": "foo", "index": 0, "finish_reason": "length"}],
@@ -614,7 +610,6 @@ class TestCodeGenerations:
                 False,
                 False,
                 False,
-                False,
                 200,
                 None,
                 [],
@@ -628,7 +623,6 @@ class TestCodeGenerations:
                 "foo",
                 True,
                 False,
-                False,
                 True,
                 False,
                 200,
@@ -645,7 +639,6 @@ class TestCodeGenerations:
                 False,
                 True,
                 False,
-                False,
                 True,
                 200,
                 "bar",
@@ -662,7 +655,6 @@ class TestCodeGenerations:
                 False,
                 False,
                 False,
-                False,
                 422,
                 None,
                 None,
@@ -677,34 +669,11 @@ class TestCodeGenerations:
                 False,
                 True,
                 False,
-                False,
                 True,
                 200,
                 "bar",
                 [],
             ),  # v2 empty suggestions from model
-            (
-                3,
-                "foo",
-                [
-                    {"role": "system", "content": "foo"},
-                    {"role": "user", "content": "bar"},
-                ],
-                "anthropic",
-                "claude-3-opus-20240229",
-                "foo",
-                False,
-                False,
-                True,
-                False,
-                True,
-                200,
-                [
-                    Message(role=Role.SYSTEM, content="foo"),
-                    Message(role=Role.USER, content="bar"),
-                ],
-                [{"text": "foo", "index": 0, "finish_reason": "length"}],
-            ),  # v3 with prompt - anthropic
         ],
     )
     def test_non_stream_response(
@@ -719,7 +688,6 @@ class TestCodeGenerations:
         model_output_text,
         want_vertex_called,
         want_anthropic_called,
-        want_anthropic_chat_called,
         want_vertex_prompt_prepared_called,
         want_anthropic_prompt_prepared_called,
         want_status,
@@ -741,17 +709,10 @@ class TestCodeGenerations:
             return_value=model_output
         )
 
-        code_generations_anthropic_chat_mock = mock.Mock(spec=CodeGenerations)
-        code_generations_anthropic_chat_mock.execute = mock.AsyncMock(
-            return_value=model_output
-        )
-
         with mock_container.code_suggestions.generations.vertex.override(
             code_generations_vertex_mock
         ), mock_container.code_suggestions.generations.anthropic_factory.override(
             code_generations_anthropic_mock
-        ), mock_container.code_suggestions.generations.anthropic_chat_factory.override(
-            code_generations_anthropic_chat_mock
         ):
             response = mock_client.post(
                 "/code/generations",
@@ -777,26 +738,17 @@ class TestCodeGenerations:
         assert response.status_code == want_status
         assert code_generations_vertex_mock.execute.called == want_vertex_called
         assert code_generations_anthropic_mock.execute.called == want_anthropic_called
-        assert (
-            code_generations_anthropic_chat_mock.execute.called
-            == want_anthropic_chat_called
-        )
 
         if want_vertex_prompt_prepared_called:
             code_generations_vertex_mock.with_prompt_prepared.assert_called_with(
                 want_prompt
             )
 
-        if want_anthropic_prompt_prepared_called and want_anthropic_called:
+        if want_anthropic_prompt_prepared_called:
             code_generations_anthropic_mock.with_prompt_prepared.assert_called_with(
                 want_prompt
             )
 
-        if want_anthropic_prompt_prepared_called and want_anthropic_chat_called:
-            code_generations_anthropic_chat_mock.with_prompt_prepared.assert_called_with(
-                want_prompt
-            )
-
         if want_status == 200:
             body = response.json()
             assert body["choices"] == want_choices
@@ -864,122 +816,6 @@ class TestCodeGenerations:
         assert response.text == expected_response
         assert response.headers["content-type"] == "text/event-stream; charset=utf-8"
 
-    @pytest.mark.parametrize(
-        (
-            "telemetry",
-            "current_file",
-            "request_headers",
-            "expected_language",
-        ),
-        [
-            (
-                [
-                    {
-                        "model_engine": "vertex",
-                        "model_name": "code-gecko",
-                        "requests": 1,
-                        "accepts": 1,
-                        "errors": 0,
-                        "lang": None,
-                    }
-                ],
-                {
-                    "file_name": "main.py",
-                    "content_above_cursor": "# Create a fast binary search\n",
-                    "content_below_cursor": "\n",
-                },
-                {
-                    "User-Agent": "vs-code",
-                    "X-Gitlab-Instance-Id": "9ebada7a-f5e2-477a-8609-17797fa95cb9",
-                    "X-Gitlab-Global-User-Id": "XTuMnZ6XTWkP3yh0ZwXualmOZvm2Gg/bk9jyfkL7Y6k=",
-                    "X-Gitlab-Host-Name": "gitlab.com",
-                    "X-Gitlab-Saas-Namespace-Ids": "1,2,3",
-                    "X-Gitlab-Saas-Duo-Pro-Namespace-Ids": "4,5,6",
-                    "X-Gitlab-Realm": "saas",
-                },
-                "python",
-            )
-        ],
-    )
-    def test_snowplow_tracking(
-        self,
-        mock_client: TestClient,
-        mock_container: containers.DeclarativeContainer,
-        telemetry: List[Dict[str, Union[str, int, None]]],
-        current_file: Dict[str, str],
-        expected_language: str,
-        request_headers: Dict[str, str],
-    ):
-        expected_event = SnowplowEvent(
-            context=SnowplowEventContext(
-                request_counts=[RequestCount(**rc) for rc in telemetry],
-                prefix_length=len(current_file.get("content_above_cursor", "")),
-                suffix_length=len(current_file.get("content_below_cursor", "")),
-                language=expected_language,
-                user_agent=request_headers.get("User-Agent", ""),
-                gitlab_realm=request_headers.get("X-Gitlab-Realm", ""),
-                gitlab_instance_id=request_headers.get("X-Gitlab-Instance-Id", ""),
-                gitlab_global_user_id=request_headers.get(
-                    "X-Gitlab-Global-User-Id", ""
-                ),
-                gitlab_host_name=request_headers.get("X-Gitlab-Host-Name", ""),
-                gitlab_saas_namespace_ids=list(
-                    CommaSeparatedStrings(
-                        request_headers.get("X-Gitlab-Saas-Namespace-Ids", "")
-                    )
-                ),
-                gitlab_saas_duo_pro_namespace_ids=list(
-                    CommaSeparatedStrings(
-                        request_headers.get("X-Gitlab-Saas-Duo-Pro-Namespace-Ids", "")
-                    )
-                ),
-            )
-        )
-
-        model_output = CodeSuggestionsOutput(
-            text="some code",
-            score=0,
-            model=ModelMetadata(name="some-model", engine="some-engine"),
-            lang_id=LanguageId.PYTHON,
-        )
-
-        code_generations_vertex_mock = mock.Mock(spec=CodeGenerations)
-        code_generations_vertex_mock.execute = mock.AsyncMock(return_value=model_output)
-
-        snowplow_instrumentator_mock = mock.Mock(spec=SnowplowInstrumentator)
-
-        snowplow_container_mock = mock.Mock(spec=ContainerTracking)
-        snowplow_container_mock.instrumentator = mock.Mock(
-            return_value=snowplow_instrumentator_mock
-        )
-
-        with mock_container.code_suggestions.generations.vertex.override(
-            code_generations_vertex_mock
-        ), mock.patch.object(mock_container, "snowplow", snowplow_container_mock):
-            response = mock_client.post(
-                "/code/generations",
-                headers={
-                    "Authorization": "Bearer 12345",
-                    "X-Gitlab-Authentication-Type": "oidc",
-                    **request_headers,
-                },
-                json={
-                    "prompt_version": 1,
-                    "project_path": "gitlab-org/gitlab",
-                    "project_id": 278964,
-                    "current_file": current_file,
-                    "prompt": "some prompt",
-                    "model_provider": "vertex-ai",
-                    "model_name": "code-bison@002",
-                    "telemetry": telemetry,
-                },
-            )
-
-        snowplow_instrumentator_mock.watch.assert_called_once()
-        args = snowplow_instrumentator_mock.watch.call_args[0]
-        assert len(args) == 1
-        assert args[0] == expected_event
-
 
 class TestUnauthorizedScopes:
     @pytest.fixture
diff --git a/tests/code_suggestions/models/test_anthropic.py b/tests/code_suggestions/models/test_anthropic.py
index bc81dcc..b8d55a9 100644
--- a/tests/code_suggestions/models/test_anthropic.py
+++ b/tests/code_suggestions/models/test_anthropic.py
@@ -3,879 +3,375 @@ from unittest.mock import AsyncMock, Mock, patch
 
 import pytest
 from anthropic import (
-    NOT_GIVEN,
     APIConnectionError,
     APITimeoutError,
     AsyncAnthropic,
     BadRequestError,
     UnprocessableEntityError,
 )
-from anthropic.types import (
-    Completion,
-    ContentBlock,
-    ContentBlockDeltaEvent,
-    ContentBlockStartEvent,
-    ContentBlockStopEvent,
-)
-from anthropic.types import Message as AMessage
-from anthropic.types import (
-    MessageDeltaEvent,
-    MessageDeltaUsage,
-    MessageStartEvent,
-    MessageStopEvent,
-    TextDelta,
-    Usage,
-    message_delta_event,
-)
+from anthropic.types import Completion
 
 from ai_gateway.models import (
     AnthropicAPIConnectionError,
     AnthropicAPIStatusError,
     AnthropicAPITimeoutError,
-    AnthropicChatModel,
     AnthropicModel,
     KindAnthropicModel,
-    Message,
-    Role,
     SafetyAttributes,
     TextGenModelOutput,
 )
 
 
-class TestAnthropicModel:
-    @pytest.mark.parametrize(
-        "model_name_version",
-        ["claude-instant-1.2", "claude-2.0"],
-    )
-    def test_anthropic_model_from_name(self, model_name_version: str):
-        model = AnthropicModel.from_model_name(model_name_version, Mock())
-
-        assert model.metadata.name == model_name_version
-        assert model.metadata.engine == "anthropic"
-
-    @pytest.mark.parametrize(
-        ("model_name_version", "opts", "opts_client", "opts_model"),
-        [
-            (
-                "claude-instant-1.2",
-                {},
-                AnthropicModel.OPTS_CLIENT,
-                AnthropicModel.OPTS_MODEL,
-            ),
-            (
-                "claude-instant-1.2",
-                {"version": "2020-10-10"},
-                AnthropicModel.OPTS_CLIENT,
-                AnthropicModel.OPTS_MODEL,
-            ),
-            (
-                "claude-instant-1.2",
-                {
-                    "timeout": 6,
-                    "max_tokens_to_sample": 5,
-                    "stop_sequences": ["\n\n"],
-                    "temperature": 0.1,
-                    "top_k": 40,
-                    "top_p": 0.95,
-                    "version": "2020-10-10",
-                    "default_headers": {
-                        "Custom Header": "custom",
-                        "anthropic-version": "2010-10-10",
-                    },
-                    "max_retries": 2,
-                },
-                {
-                    "default_headers": {
-                        "Custom Header": "custom",
-                        "anthropic-version": "2010-10-10",
-                    },
-                    "max_retries": 2,
-                },
-                {
-                    "timeout": 6,
-                    "max_tokens_to_sample": 5,
-                    "stop_sequences": ["\n\n"],
-                    "temperature": 0.1,
-                    "top_k": 40,
-                    "top_p": 0.95,
-                },
-            ),
-        ],
-    )
-    def test_anthropic_provider_opts(
-        self,
-        model_name_version: str,
-        opts: dict,
-        opts_client: dict,
-        opts_model: dict,
-    ):
-        client = Mock()
-        model = AnthropicModel.from_model_name(model_name_version, client, **opts)
-
-        headers = opts_client["default_headers"]
-        if not headers.get("anthropic-version", None):
-            headers["anthropic-version"] = opts.get(
-                "version", AnthropicModel.DEFAULT_VERSION
-            )
-
-        client.with_options.assert_called_with(**opts_client)
-        assert model.model_opts == opts_model
-
-    @pytest.mark.asyncio
-    @pytest.mark.parametrize(
-        ("model_name_version", "exception", "expected_exception"),
-        [
-            ("claude-instant-1.2", BadRequestError, AnthropicAPIStatusError),
-            ("claude-instant-1.2", UnprocessableEntityError, AnthropicAPIStatusError),
-            ("claude-instant-1.2", APIConnectionError, AnthropicAPIConnectionError),
-            ("claude-instant-1.2", APITimeoutError, AnthropicAPITimeoutError),
-        ],
-    )
-    async def test_anthropic_model_error(
-        self, model_name_version: str, exception: Type, expected_exception: Type
-    ):
-        def _client_predict(*_args, **_kwargs):
-            if issubclass(exception, APITimeoutError):
-                raise exception(request=Mock())
-
-            if issubclass(exception, APIConnectionError):
-                raise exception(message="exception", request=Mock())
-
-            raise exception(message="exception", response=Mock(), body=Mock())
+@pytest.mark.parametrize(
+    "model_name_version",
+    ["claude-instant-1.2", "claude-2.0"],
+)
+def test_anthropic_model_from_name(model_name_version: str):
+    model = AnthropicModel.from_model_name(model_name_version, Mock())
 
-        model = AnthropicModel.from_model_name(
-            model_name_version, Mock(spec=AsyncAnthropic)
-        )
-        model.client.completions.create = AsyncMock(side_effect=_client_predict)
+    assert model.metadata.name == model_name_version
+    assert model.metadata.engine == "anthropic"
 
-        with pytest.raises(expected_exception):
-            _ = await model.generate("prefix", "suffix")
 
-    @pytest.mark.asyncio
-    @pytest.mark.parametrize(
+@pytest.mark.parametrize(
+    ("model_name_version", "opts", "opts_client", "opts_model"),
+    [
         (
-            "model_name_version",
-            "prompt",
-            "suggestion",
-            "opts",
-            "opts_model",
-            "expected_opts_model",
-            "expected_output",
+            "claude-instant-1.2",
+            {},
+            AnthropicModel.OPTS_CLIENT,
+            AnthropicModel.OPTS_MODEL,
         ),
-        [
-            (
-                "claude-instant-1.2",
-                "random_prompt",
-                "random_text",
-                {},
-                {},
-                {**AnthropicModel.OPTS_MODEL, **{"stream": False}},
-                TextGenModelOutput(
-                    text="random_text",
-                    score=10_000,
-                    safety_attributes=SafetyAttributes(),
-                ),
-            ),
-            (
-                "claude-instant-1.2",
-                "random_prompt",
-                "random_text",
-                {"top_k": 10},
-                {},
-                {**AnthropicModel.OPTS_MODEL, **{"top_k": 10, "stream": False}},
-                TextGenModelOutput(
-                    text="random_text",
-                    score=10_000,
-                    safety_attributes=SafetyAttributes(),
-                ),
-            ),
-            (
-                "claude-instant-1.2",
-                "random_prompt",
-                "random_text",
-                {"temperature": 1},
-                {},
-                {
-                    **AnthropicModel.OPTS_MODEL,
-                    **{"temperature": 1, "stream": False},
-                },
-                TextGenModelOutput(
-                    text="random_text",
-                    score=10_000,
-                    safety_attributes=SafetyAttributes(),
-                ),
-            ),
-            (
-                "claude-instant-1.2",
-                "random_prompt",
-                "random_text",
-                {"temperature": 1},
-                {"temperature": 0.1},  # Override the temperature when calling the model
-                {
-                    **AnthropicModel.OPTS_MODEL,
-                    **{"temperature": 0.1, "stream": False},
+        (
+            "claude-instant-1.2",
+            {"version": "2020-10-10"},
+            AnthropicModel.OPTS_CLIENT,
+            AnthropicModel.OPTS_MODEL,
+        ),
+        (
+            "claude-instant-1.2",
+            {
+                "timeout": 6,
+                "max_tokens_to_sample": 5,
+                "stop_sequences": ["\n\n"],
+                "temperature": 0.1,
+                "top_k": 40,
+                "top_p": 0.95,
+                "version": "2020-10-10",
+                "default_headers": {
+                    "Custom Header": "custom",
+                    "anthropic-version": "2010-10-10",
                 },
-                TextGenModelOutput(
-                    text="random_text",
-                    score=10_000,
-                    safety_attributes=SafetyAttributes(),
-                ),
-            ),
-            (
-                "claude-instant-1.2",
-                "random_prompt",
-                "random_text",
-                {},
-                {
-                    "temperature": 0.1,
-                    "top_p": 0.95,
-                },  # Override the temperature when calling the model
-                {
-                    **AnthropicModel.OPTS_MODEL,
-                    **{"temperature": 0.1, "top_p": 0.95, "stream": False},
+                "max_retries": 2,
+            },
+            {
+                "default_headers": {
+                    "Custom Header": "custom",
+                    "anthropic-version": "2010-10-10",
                 },
-                TextGenModelOutput(
-                    text="random_text",
-                    score=10_000,
-                    safety_attributes=SafetyAttributes(),
-                ),
-            ),
-        ],
-    )
-    async def test_anthropic_model_generate(
-        self,
-        model_name_version: str,
-        prompt: str,
-        suggestion: str,
-        opts: dict,
-        opts_model: dict,
-        expected_opts_model: dict,
-        expected_output: TextGenModelOutput,
-    ):
-        def _client_predict(*_args, **_kwargs):
-            return Completion(
-                id="compl_01CtvorJWMstkmATFkR7qVYM",
-                completion=suggestion,
-                model=model_name_version,
-                stop_reason="max_tokens",
-                type="completion",
-            )
-
-        model = AnthropicModel.from_model_name(
-            model_name_version,
-            Mock(spec=AsyncAnthropic),
-            **opts,
-        )
-
-        model.client.completions.create = AsyncMock(side_effect=_client_predict)
-
-        actual_output = await model.generate(prompt, "", **opts_model)
-        # Create another dictionary to avoid modifying the original one.
-        expected_opts_model = {
-            **expected_opts_model,
-            **{
-                "model": model_name_version,
-                "prompt": prompt,
+                "max_retries": 2,
             },
-        }
-
-        model.client.completions.create.assert_called_with(**expected_opts_model)
-        assert actual_output.text == expected_output.text
-
-    @pytest.mark.asyncio
-    async def test_anthropic_model_generate_instrumented(self):
-        model = AnthropicModel(
-            model_name=KindAnthropicModel.CLAUDE_2_0.value,
-            client=Mock(spec=AsyncAnthropic),
+            {
+                "timeout": 6,
+                "max_tokens_to_sample": 5,
+                "stop_sequences": ["\n\n"],
+                "temperature": 0.1,
+                "top_k": 40,
+                "top_p": 0.95,
+            },
+        ),
+    ],
+)
+def test_anthropic_provider_opts(
+    model_name_version: str,
+    opts: dict,
+    opts_client: dict,
+    opts_model: dict,
+):
+    client = Mock()
+    model = AnthropicModel.from_model_name(model_name_version, client, **opts)
+
+    headers = opts_client["default_headers"]
+    if not headers.get("anthropic-version", None):
+        headers["anthropic-version"] = opts.get(
+            "version", AnthropicModel.DEFAULT_VERSION
         )
-        model.client.completions.create = AsyncMock()
 
-        with patch(
-            "ai_gateway.instrumentators.model_requests.ModelRequestInstrumentator.watch"
-        ) as mock_watch:
-            await model.generate("Wolf, what time is it?")
+    client.with_options.assert_called_with(**opts_client)
+    assert model.model_opts == opts_model
 
-            mock_watch.assert_called_once_with(stream=False)
 
-    @pytest.mark.asyncio
-    async def test_anthropic_model_generate_stream_instrumented(self):
-        async def mock_stream(*args, **kwargs):
-            completions = [
-                Completion(
-                    id="compl_01CtvorJWMstkmATFkR7qVYM",
-                    completion="hello",
-                    model=KindAnthropicModel.CLAUDE_2_0.value,
-                    stop_reason="stop_sequence",
-                    type="completion",
-                ),
-                Completion(
-                    id="compl_02CtvorJWMstkmATFkR7qVYM",
-                    completion="world",
-                    model=KindAnthropicModel.CLAUDE_2_0.value,
-                    stop_reason="stop_sequence",
-                    type="completion",
-                ),
-                "break here",
-            ]
-            for item in completions:
-                if item == "break here":
-                    raise ValueError("broken")
-                yield item
-
-        model = AnthropicModel(
-            model_name=KindAnthropicModel.CLAUDE_2_0.value,
-            client=Mock(spec=AsyncAnthropic),
-        )
-        model.client.completions.create = AsyncMock(side_effect=mock_stream)
+@pytest.mark.asyncio
+@pytest.mark.parametrize(
+    ("model_name_version", "exception", "expected_exception"),
+    [
+        ("claude-instant-1.2", BadRequestError, AnthropicAPIStatusError),
+        ("claude-instant-1.2", UnprocessableEntityError, AnthropicAPIStatusError),
+        ("claude-instant-1.2", APIConnectionError, AnthropicAPIConnectionError),
+        ("claude-instant-1.2", APITimeoutError, AnthropicAPITimeoutError),
+    ],
+)
+async def test_anthropic_model_error(
+    model_name_version: str, exception: Type, expected_exception: Type
+):
+    def _client_predict(*_args, **_kwargs):
+        if issubclass(exception, APITimeoutError):
+            raise exception(request=Mock())
 
-        with patch(
-            "ai_gateway.instrumentators.model_requests.ModelRequestInstrumentator.watch"
-        ) as mock_watch:
-            watcher = Mock()
-            mock_watch.return_value.__enter__.return_value = watcher
+        if issubclass(exception, APIConnectionError):
+            raise exception(message="exception", request=Mock())
 
-            r = await model.generate("Wolf, what time is it?", stream=True)
+        raise exception(message="exception", response=Mock(), body=Mock())
 
-            # Make sure we haven't called finish before completions are consumed
-            watcher.finish.assert_not_called()
+    model = AnthropicModel.from_model_name(
+        model_name_version, Mock(spec=AsyncAnthropic)
+    )
+    model.client.completions.create = AsyncMock(side_effect=_client_predict)
 
-            with pytest.raises(ValueError):
-                _ = [item async for item in r]
+    with pytest.raises(expected_exception):
+        _ = await model.generate("prefix", "suffix")
 
-            mock_watch.assert_called_once_with(stream=True)
-            watcher.finish.assert_called_once()
 
-    @pytest.mark.asyncio
-    @pytest.mark.parametrize(
+@pytest.mark.asyncio
+@pytest.mark.parametrize(
+    (
+        "model_name_version",
+        "prompt",
+        "suggestion",
+        "opts",
+        "opts_model",
+        "expected_opts_model",
+        "expected_output",
+    ),
+    [
         (
-            "model_name_version",
-            "prompt",
-            "completion_chunks",
-            "expected_chunks",
+            "claude-instant-1.2",
+            "random_prompt",
+            "random_text",
+            {},
+            {},
+            {**AnthropicModel.OPTS_MODEL, **{"stream": False}},
+            TextGenModelOutput(
+                text="random_text", score=10_000, safety_attributes=SafetyAttributes()
+            ),
         ),
-        [
-            (
-                "claude-instant-1.2",
-                "random_prompt",
-                [
-                    Completion(
-                        id="compl_01CtvorJWMstkmATFkR7qVYM",
-                        completion="def hello_",
-                        stop_reason="stop_sequence",
-                        model="claude-instant-1.2",
-                        type="completion",
-                    ),
-                    Completion(
-                        id="compl_02CtvorJWMstkmATFkR7qVYM",
-                        completion="world():",
-                        stop_reason="stop_sequence",
-                        model="claude-instant-1.2",
-                        type="completion",
-                    ),
-                ],
-                [
-                    "def hello_",
-                    "world():",
-                ],
+        (
+            "claude-instant-1.2",
+            "random_prompt",
+            "random_text",
+            {"top_k": 10},
+            {},
+            {**AnthropicModel.OPTS_MODEL, **{"top_k": 10, "stream": False}},
+            TextGenModelOutput(
+                text="random_text", score=10_000, safety_attributes=SafetyAttributes()
             ),
-        ],
-    )
-    async def test_anthropic_model_generate_stream(
-        self,
-        model_name_version: str,
-        prompt: str,
-        completion_chunks: list[Completion],
-        expected_chunks: list[str],
-    ):
-        async def _stream_generator(*args, **kwargs):
-            for chunk in completion_chunks:
-                yield chunk
-
-        model = AnthropicModel.from_model_name(
-            model_name_version,
-            Mock(spec=AsyncAnthropic),
+        ),
+        (
+            "claude-instant-1.2",
+            "random_prompt",
+            "random_text",
+            {"temperature": 1},
+            {},
+            {**AnthropicModel.OPTS_MODEL, **{"temperature": 1, "stream": False}},
+            TextGenModelOutput(
+                text="random_text", score=10_000, safety_attributes=SafetyAttributes()
+            ),
+        ),
+        (
+            "claude-instant-1.2",
+            "random_prompt",
+            "random_text",
+            {"temperature": 1},
+            {"temperature": 0.1},  # Override the temperature when calling the model
+            {**AnthropicModel.OPTS_MODEL, **{"temperature": 0.1, "stream": False}},
+            TextGenModelOutput(
+                text="random_text", score=10_000, safety_attributes=SafetyAttributes()
+            ),
+        ),
+        (
+            "claude-instant-1.2",
+            "random_prompt",
+            "random_text",
+            {},
+            {
+                "temperature": 0.1,
+                "top_p": 0.95,
+            },  # Override the temperature when calling the model
+            {
+                **AnthropicModel.OPTS_MODEL,
+                **{"temperature": 0.1, "top_p": 0.95, "stream": False},
+            },
+            TextGenModelOutput(
+                text="random_text", score=10_000, safety_attributes=SafetyAttributes()
+            ),
+        ),
+    ],
+)
+async def test_anthropic_model_generate(
+    model_name_version: str,
+    prompt: str,
+    suggestion: str,
+    opts: dict,
+    opts_model: dict,
+    expected_opts_model: dict,
+    expected_output: TextGenModelOutput,
+):
+    def _client_predict(*_args, **_kwargs):
+        return Completion(
+            id="compl_01CtvorJWMstkmATFkR7qVYM",
+            completion=suggestion,
+            model=model_name_version,
+            stop_reason="max_tokens",
+            type="completion",
         )
 
-        model.client.completions.create = AsyncMock(side_effect=_stream_generator)
-
-        actual_output = await model.generate(prompt, stream=True)
-        expected_opts_model = {
-            **AnthropicModel.OPTS_MODEL,
-            **{"model": model_name_version, "prompt": prompt, "stream": True},
-        }
+    model = AnthropicModel.from_model_name(
+        model_name_version,
+        Mock(spec=AsyncAnthropic),
+        **opts,
+    )
 
-        chunks = []
-        async for content in actual_output:
-            chunks += content
+    model.client.completions.create = AsyncMock(side_effect=_client_predict)
 
-        assert chunks == expected_chunks
+    actual_output = await model.generate(prompt, "", **opts_model)
+    # Create another dictionary to avoid modifying the original one.
+    expected_opts_model = {
+        **expected_opts_model,
+        **{
+            "model": model_name_version,
+            "prompt": prompt,
+        },
+    }
 
-        model.client.completions.create.assert_called_with(**expected_opts_model)
+    model.client.completions.create.assert_called_with(**expected_opts_model)
+    assert actual_output.text == expected_output.text
 
 
-class TestAnthropicChatModel:
-    @pytest.mark.parametrize(
-        "model_name_version",
-        [
-            "claude-3-opus-20240229",
-            "claude-3-sonnet-20240229",
-            "claude-3-haiku-20240307",
-        ],
-    )
-    def test_anthropic_model_from_name(self, model_name_version: str):
-        model = AnthropicChatModel.from_model_name(model_name_version, Mock())
-
-        assert model.metadata.name == model_name_version
-        assert model.metadata.engine == "anthropic"
-
-    @pytest.mark.parametrize(
-        ("model_name_version", "opts", "opts_client", "opts_model"),
-        [
-            (
-                "claude-3-haiku-20240307",
-                {},
-                AnthropicChatModel.OPTS_CLIENT,
-                AnthropicChatModel.OPTS_MODEL,
-            ),
-            (
-                "claude-3-haiku-20240307",
-                {"version": "2020-10-10"},
-                AnthropicChatModel.OPTS_CLIENT,
-                AnthropicChatModel.OPTS_MODEL,
-            ),
-            (
-                "claude-3-haiku-20240307",
-                {
-                    "timeout": 6,
-                    "max_tokens": 5,
-                    "stop_sequences": ["\n\n"],
-                    "temperature": 0.1,
-                    "top_k": 40,
-                    "top_p": 0.95,
-                    "version": "2020-10-10",
-                    "default_headers": {
-                        "Custom Header": "custom",
-                        "anthropic-version": "2010-10-10",
-                    },
-                    "max_retries": 2,
-                },
-                {
-                    "default_headers": {
-                        "Custom Header": "custom",
-                        "anthropic-version": "2010-10-10",
-                    },
-                    "max_retries": 2,
-                },
-                {
-                    "timeout": 6,
-                    "max_tokens": 5,
-                    "stop_sequences": ["\n\n"],
-                    "temperature": 0.1,
-                    "top_k": 40,
-                    "top_p": 0.95,
-                },
-            ),
-        ],
-    )
-    def test_anthropic_provider_opts(
-        self,
-        model_name_version: str,
-        opts: dict,
-        opts_client: dict,
-        opts_model: dict,
-    ):
-        client = Mock()
-        model = AnthropicChatModel.from_model_name(model_name_version, client, **opts)
-
-        headers = opts_client["default_headers"]
-        if not headers.get("anthropic-version", None):
-            headers["anthropic-version"] = opts.get(
-                "version", AnthropicChatModel.DEFAULT_VERSION
-            )
-
-        client.with_options.assert_called_with(**opts_client)
-        assert model.model_opts == opts_model
-
-    @pytest.mark.asyncio
-    @pytest.mark.parametrize(
-        ("model_name_version", "exception", "expected_exception"),
-        [
-            ("claude-3-haiku-20240307", BadRequestError, AnthropicAPIStatusError),
-            (
-                "claude-3-haiku-20240307",
-                UnprocessableEntityError,
-                AnthropicAPIStatusError,
-            ),
-            (
-                "claude-3-haiku-20240307",
-                APIConnectionError,
-                AnthropicAPIConnectionError,
-            ),
-            ("claude-3-haiku-20240307", APITimeoutError, AnthropicAPITimeoutError),
-        ],
+@pytest.mark.asyncio
+async def test_anthropic_model_generate_instrumented():
+    model = AnthropicModel(
+        model_name=KindAnthropicModel.CLAUDE_2_0.value, client=Mock(spec=AsyncAnthropic)
     )
-    async def test_anthropic_model_error(
-        self, model_name_version: str, exception: Type, expected_exception: Type
-    ):
-        def _client_predict(*_args, **_kwargs):
-            if issubclass(exception, APITimeoutError):
-                raise exception(request=Mock())
+    model.client.completions.create = AsyncMock()
 
-            if issubclass(exception, APIConnectionError):
-                raise exception(message="exception", request=Mock())
+    with patch(
+        "ai_gateway.instrumentators.model_requests.ModelRequestInstrumentator.watch"
+    ) as mock_watch:
+        await model.generate("Wolf, what time is it?")
 
-            raise exception(message="exception", response=Mock(), body=Mock())
+        mock_watch.assert_called_once_with(stream=False)
 
-        model = AnthropicChatModel.from_model_name(
-            model_name_version, Mock(spec=AsyncAnthropic)
-        )
-        model.client.messages.create = AsyncMock(side_effect=_client_predict)
-
-        with pytest.raises(expected_exception):
-            _ = await model.generate(
-                messages=[
-                    Message(role=Role.SYSTEM, content="hello"),
-                    Message(role=Role.USER, content="bye"),
-                ]
-            )
-
-    @pytest.mark.asyncio
-    @pytest.mark.parametrize(
-        (
-            "model_name_version",
-            "messages",
-            "suggestion",
-            "opts",
-            "opts_model",
-            "expected_opts_model",
-            "expected_output",
-        ),
-        [
-            (
-                "claude-3-haiku-20240307",
-                [
-                    Message(role=Role.SYSTEM, content="nice human"),
-                    Message(role=Role.USER, content="write code"),
-                ],
-                "random_text",
-                {},
-                {},
-                {**AnthropicChatModel.OPTS_MODEL, **{"stream": False}},
-                TextGenModelOutput(
-                    text="random_text",
-                    score=10_000,
-                    safety_attributes=SafetyAttributes(),
-                ),
-            ),
-            (
-                "claude-3-haiku-20240307",
-                [
-                    Message(role=Role.SYSTEM, content="nice human"),
-                    Message(role=Role.USER, content="write code"),
-                ],
-                "random_text",
-                {"top_k": 10},
-                {},
-                {**AnthropicChatModel.OPTS_MODEL, **{"top_k": 10, "stream": False}},
-                TextGenModelOutput(
-                    text="random_text",
-                    score=10_000,
-                    safety_attributes=SafetyAttributes(),
-                ),
-            ),
-            (
-                "claude-3-haiku-20240307",
-                [
-                    Message(role=Role.SYSTEM, content="nice human"),
-                    Message(role=Role.USER, content="write code"),
-                ],
-                "random_text",
-                {"temperature": 1},
-                {},
-                {
-                    **AnthropicChatModel.OPTS_MODEL,
-                    **{"temperature": 1, "stream": False},
-                },
-                TextGenModelOutput(
-                    text="random_text",
-                    score=10_000,
-                    safety_attributes=SafetyAttributes(),
-                ),
-            ),
-            (
-                "claude-3-haiku-20240307",
-                [
-                    Message(role=Role.USER, content="write code"),
-                ],
-                "random_text",
-                {"temperature": 1},
-                {"temperature": 0.1},  # Override the temperature when calling the model
-                {
-                    **AnthropicChatModel.OPTS_MODEL,
-                    **{"temperature": 0.1, "stream": False},
-                },
-                TextGenModelOutput(
-                    text="random_text",
-                    score=10_000,
-                    safety_attributes=SafetyAttributes(),
-                ),
+
+@pytest.mark.asyncio
+async def test_anthropic_model_generate_stream_instrumented():
+    async def mock_stream(*args, **kwargs):
+        completions = [
+            Completion(
+                id="compl_01CtvorJWMstkmATFkR7qVYM",
+                completion="hello",
+                model=KindAnthropicModel.CLAUDE_2_0.value,
+                stop_reason="stop_sequence",
+                type="completion",
             ),
-            (
-                "claude-3-haiku-20240307",
-                [
-                    Message(role=Role.USER, content="write code"),
-                    Message(role=Role.ASSISTANT, content="Writing code:"),
-                ],
-                "random_text",
-                {},
-                {
-                    "temperature": 0.1,
-                    "top_p": 0.95,
-                },  # Override the temperature when calling the model
-                {
-                    **AnthropicChatModel.OPTS_MODEL,
-                    **{"temperature": 0.1, "top_p": 0.95, "stream": False},
-                },
-                TextGenModelOutput(
-                    text="random_text",
-                    score=10_000,
-                    safety_attributes=SafetyAttributes(),
-                ),
+            Completion(
+                id="compl_02CtvorJWMstkmATFkR7qVYM",
+                completion="world",
+                model=KindAnthropicModel.CLAUDE_2_0.value,
+                stop_reason="stop_sequence",
+                type="completion",
             ),
-        ],
+            "break here",
+        ]
+        for item in completions:
+            if item == "break here":
+                raise ValueError("broken")
+            yield item
+
+    model = AnthropicModel(
+        model_name=KindAnthropicModel.CLAUDE_2_0.value, client=Mock(spec=AsyncAnthropic)
     )
-    async def test_anthropic_model_generate(
-        self,
-        model_name_version: str,
-        messages: list[Message],
-        suggestion: str,
-        opts: dict,
-        opts_model: dict,
-        expected_opts_model: dict,
-        expected_output: TextGenModelOutput,
-    ):
-        def _client_predict(*_args, **_kwargs):
-            return AMessage(
-                id="msg_01PE3CarfxWEG2taV9AygzH9",
-                content=[ContentBlock(text=suggestion, type="text")],
-                model=model_name_version,
-                role="assistant",
-                stop_reason="end_turn",
-                stop_sequence=None,
-                type="message",
-                usage=Usage(input_tokens=21, output_tokens=81),
-            )
-
-        model = AnthropicChatModel.from_model_name(
-            model_name_version,
-            Mock(spec=AsyncAnthropic),
-            **opts,
-        )
+    model.client.completions.create = AsyncMock(side_effect=mock_stream)
 
-        model.client.messages.create = AsyncMock(side_effect=_client_predict)
-
-        actual_output = await model.generate(messages, **opts_model)
-        # Create another dictionary to avoid modifying the original one.
-        expected_opts_model = {
-            **expected_opts_model,
-            **{
-                "model": model_name_version,
-                "system": (
-                    [
-                        message.content
-                        for message in messages
-                        if message.role == Role.SYSTEM
-                    ]
-                    + [NOT_GIVEN]  # Use NOT_GIVEN by default
-                ).pop(0),
-                "messages": [
-                    message.dict()
-                    for message in messages
-                    if message.role in (Role.USER, Role.ASSISTANT)
-                ],
-            },
-        }
-
-        model.client.messages.create.assert_called_with(**expected_opts_model)
-        assert actual_output.text == expected_output.text
-
-    @pytest.mark.asyncio
-    async def test_anthropic_model_generate_instrumented(self):
-        model = AnthropicChatModel(
-            model_name=KindAnthropicModel.CLAUDE_3_HAIKU.value,
-            client=Mock(spec=AsyncAnthropic),
-        )
-        model.client.messages.create = AsyncMock()
-
-        with patch(
-            "ai_gateway.instrumentators.model_requests.ModelRequestInstrumentator.watch"
-        ) as mock_watch:
-            await model.generate(
-                [
-                    Message(role=Role.SYSTEM, content="nice human"),
-                    Message(role=Role.USER, content="write code"),
-                ]
-            )
-
-            mock_watch.assert_called_once_with(stream=False)
-
-    @pytest.mark.asyncio
-    async def test_anthropic_model_generate_stream_instrumented(self):
-        async def mock_stream(*args, **kwargs):
-            completions = [
-                MessageStartEvent(
-                    message=AMessage(
-                        id="msg_01PE3CarfxWEG2taV9AygzH9",
-                        content=[],
-                        model=KindAnthropicModel.CLAUDE_3_HAIKU.value,
-                        role="assistant",
-                        stop_reason=None,
-                        stop_sequence=None,
-                        type="message",
-                        usage=Usage(input_tokens=21, output_tokens=1),
-                    ),
-                    type="message_start",
-                ),
-                ContentBlockStartEvent(
-                    content_block=ContentBlock(text="", type="text"),
-                    index=0,
-                    type="content_block_start",
-                ),
-                ContentBlockDeltaEvent(
-                    delta=TextDelta(text="It's", type="text_delta"),
-                    index=0,
-                    type="content_block_delta",
-                ),
-                "break here",
-            ]
-            for item in completions:
-                if item == "break here":
-                    raise ValueError("broken")
-                yield item
-
-        model = AnthropicChatModel(
-            model_name=KindAnthropicModel.CLAUDE_3_HAIKU.value,
-            client=Mock(spec=AsyncAnthropic),
-        )
-        model.client.messages.create = AsyncMock(side_effect=mock_stream)
+    with patch(
+        "ai_gateway.instrumentators.model_requests.ModelRequestInstrumentator.watch"
+    ) as mock_watch:
+        watcher = Mock()
+        mock_watch.return_value.__enter__.return_value = watcher
 
-        with patch(
-            "ai_gateway.instrumentators.model_requests.ModelRequestInstrumentator.watch"
-        ) as mock_watch:
-            watcher = Mock()
-            mock_watch.return_value.__enter__.return_value = watcher
+        r = await model.generate("Wolf, what time is it?", stream=True)
 
-            r = await model.generate(
-                messages=[
-                    Message(role=Role.SYSTEM, content="nice human"),
-                    Message(role=Role.USER, content="write code"),
-                ],
-                stream=True,
-            )
+        # Make sure we haven't called finish before completions are consumed
+        watcher.finish.assert_not_called()
 
-            # Make sure we haven't called finish before completions are consumed
-            watcher.finish.assert_not_called()
+        with pytest.raises(ValueError):
+            _ = [item async for item in r]
 
-            with pytest.raises(ValueError):
-                _ = [item async for item in r]
+        mock_watch.assert_called_once_with(stream=True)
+        watcher.finish.assert_called_once()
 
-            mock_watch.assert_called_once_with(stream=True)
-            watcher.finish.assert_called_once()
 
-    @pytest.mark.asyncio
-    @pytest.mark.parametrize(
+@pytest.mark.asyncio
+@pytest.mark.parametrize(
+    (
+        "model_name_version",
+        "prompt",
+        "completion_chunks",
+        "expected_chunks",
+    ),
+    [
         (
-            "model_name_version",
-            "messages",
-            "completion_chunks",
-            "expected_chunks",
+            "claude-instant-1.2",
+            "random_prompt",
+            [
+                Completion(
+                    id="compl_01CtvorJWMstkmATFkR7qVYM",
+                    completion="def hello_",
+                    stop_reason="stop_sequence",
+                    model="claude-instant-1.2",
+                    type="completion",
+                ),
+                Completion(
+                    id="compl_02CtvorJWMstkmATFkR7qVYM",
+                    completion="world():",
+                    stop_reason="stop_sequence",
+                    model="claude-instant-1.2",
+                    type="completion",
+                ),
+            ],
+            [
+                "def hello_",
+                "world():",
+            ],
         ),
-        [
-            (
-                "claude-3-haiku-20240307",
-                [
-                    Message(role=Role.SYSTEM, content="nice human"),
-                    Message(role=Role.USER, content="write code"),
-                ],
-                [
-                    MessageStartEvent(
-                        message=AMessage(
-                            id="msg_01PE3CarfxWEG2taV9AygzH9",
-                            content=[],
-                            model=KindAnthropicModel.CLAUDE_3_HAIKU,
-                            role="assistant",
-                            stop_reason=None,
-                            stop_sequence=None,
-                            type="message",
-                            usage=Usage(input_tokens=21, output_tokens=1),
-                        ),
-                        type="message_start",
-                    ),
-                    ContentBlockStartEvent(
-                        content_block=ContentBlock(text="", type="text"),
-                        index=0,
-                        type="content_block_start",
-                    ),
-                    ContentBlockDeltaEvent(
-                        delta=TextDelta(text="def hello_", type="text_delta"),
-                        index=0,
-                        type="content_block_delta",
-                    ),
-                    ContentBlockDeltaEvent(
-                        delta=TextDelta(text="world():", type="text_delta"),
-                        index=0,
-                        type="content_block_delta",
-                    ),
-                    ContentBlockStopEvent(index=0, type="content_block_stop"),
-                    MessageDeltaEvent(
-                        delta=message_delta_event.Delta(
-                            stop_reason="end_turn", stop_sequence=None
-                        ),
-                        type="message_delta",
-                        usage=MessageDeltaUsage(output_tokens=57),
-                    ),
-                    MessageStopEvent(type="message_stop"),
-                ],
-                [
-                    "def hello_",
-                    "world():",
-                ],
-            ),
-        ],
+    ],
+)
+async def test_anthropic_model_generate_stream(
+    model_name_version: str,
+    prompt: str,
+    completion_chunks: list[Completion],
+    expected_chunks: list[str],
+):
+    async def _stream_generator(*args, **kwargs):
+        for chunk in completion_chunks:
+            yield chunk
+
+    model = AnthropicModel.from_model_name(
+        model_name_version,
+        Mock(spec=AsyncAnthropic),
     )
-    async def test_anthropic_model_generate_stream(
-        self,
-        model_name_version: str,
-        messages: list[Message],
-        completion_chunks: list,
-        expected_chunks: list[str],
-    ):
-        async def _stream_generator(*args, **kwargs):
-            for chunk in completion_chunks:
-                yield chunk
-
-        model = AnthropicChatModel.from_model_name(
-            model_name_version,
-            Mock(spec=AsyncAnthropic),
-        )
 
-        model.client.messages.create = AsyncMock(side_effect=_stream_generator)
+    model.client.completions.create = AsyncMock(side_effect=_stream_generator)
 
-        actual_output = await model.generate(messages=messages, stream=True)
-        expected_opts_model = {
-            **AnthropicChatModel.OPTS_MODEL,
-            **{
-                "model": model_name_version,
-                "system": "nice human",
-                "messages": [{"role": Role.USER, "content": "write code"}],
-                "stream": True,
-            },
-        }
+    actual_output = await model.generate(prompt, stream=True)
+    expected_opts_model = {
+        **AnthropicModel.OPTS_MODEL,
+        **{"model": model_name_version, "prompt": prompt, "stream": True},
+    }
 
-        chunks = []
-        async for content in actual_output:
-            chunks += content
+    chunks = []
+    async for content in actual_output:
+        chunks += content
 
-        assert chunks == expected_chunks
+    assert chunks == expected_chunks
 
-        model.client.messages.create.assert_called_with(**expected_opts_model)
+    model.client.completions.create.assert_called_with(**expected_opts_model)
diff --git a/tests/code_suggestions/models/test_mock.py b/tests/code_suggestions/models/test_mock.py
deleted file mode 100644
index 7334bdd..0000000
--- a/tests/code_suggestions/models/test_mock.py
+++ /dev/null
@@ -1,91 +0,0 @@
-import json
-
-import pytest
-
-from ai_gateway.models import Message, ModelMetadata, Role, SafetyAttributes, mock
-
-
-@pytest.mark.asyncio
-class TestLLM:
-    TEST_CASES = [
-        ("long custom prefix", "long custom suffix", {}),
-        ("long custom prefix", "long custom suffix", {"temperature": 0.1}),
-        ("long custom prefix", None, {}),
-        ("long custom prefix", None, {"temperature": 0.1}),
-    ]
-
-    async def test_metadata(self):
-        model = mock.LLM()
-        assert model.metadata == ModelMetadata(
-            name="llm-mocked", engine="llm-provider-mocked"
-        )
-
-    @pytest.mark.parametrize(("prefix", "suffix", "kwargs"), TEST_CASES)
-    async def test_non_stream(self, prefix: str, suffix: str, kwargs: dict):
-        model = mock.LLM()
-
-        response = await model.generate(prefix, suffix, stream=False, **kwargs)
-        expected_substrings = [prefix, suffix] if suffix else [prefix]
-        expected_substrings += [json.dumps(kwargs)]
-
-        assert all([substring in response.text for substring in expected_substrings])
-        assert response.text.startswith("echo:")
-        assert response.score == 0
-        assert response.safety_attributes == SafetyAttributes()
-
-    @pytest.mark.parametrize(("prefix", "suffix", "kwargs"), TEST_CASES)
-    async def test_stream(self, prefix: str, suffix: str, kwargs: dict):
-        model = mock.LLM()
-
-        response = await model.generate(prefix, suffix, stream=True, **kwargs)
-        actual_text = "".join([chunk.text async for chunk in response])
-        expected_substrings = [prefix, suffix] if suffix else [prefix]
-        expected_substrings += [json.dumps(kwargs)]
-
-        assert all([substring in actual_text for substring in expected_substrings])
-
-
-@pytest.mark.asyncio
-class TestChatModel:
-    TEST_CASES = [
-        ([Message(role=Role.SYSTEM, content="long custom system prompt")], {}),
-        (
-            [
-                Message(role=Role.SYSTEM, content="long custom system prompt"),
-                Message(role=Role.USER, content="long custom user prompt"),
-            ],
-            {"temperature": 0.1},
-        ),
-    ]
-
-    async def test_metadata(self):
-        model = mock.ChatModel()
-        assert model.metadata == ModelMetadata(
-            name="chat-model-mocked", engine="chat-model-provider-mocked"
-        )
-
-    @pytest.mark.parametrize(("messages", "kwargs"), TEST_CASES)
-    async def test_non_stream(self, messages: list[Message], kwargs: dict):
-        model = mock.ChatModel()
-
-        response = await model.generate(messages, stream=False, **kwargs)
-        messages = [message.model_dump(mode="json") for message in messages]
-        expected_substrings = [json.dumps(s) for s in (messages, kwargs)]
-
-        assert response.score == 0
-        assert response.safety_attributes == SafetyAttributes()
-        assert response.text.startswith("echo:")
-        assert all([substring in response.text for substring in expected_substrings])
-
-    @pytest.mark.parametrize(("messages", "kwargs"), TEST_CASES)
-    async def test_stream(self, messages: list[Message], kwargs: dict):
-        model = mock.ChatModel()
-
-        response = await model.generate(messages, stream=True, **kwargs)
-        actual_text = "".join([chunk.text async for chunk in response])
-
-        messages = [message.model_dump(mode="json") for message in messages]
-        expected_substrings = [json.dumps(s) for s in (messages, kwargs)]
-
-        assert actual_text.startswith("echo:")
-        assert all([substring in actual_text for substring in expected_substrings])
diff --git a/tests/code_suggestions/pre_processing/test_prefix_based.py b/tests/code_suggestions/pre_processing/test_prefix_based.py
index fc010e1..465bf2f 100644
--- a/tests/code_suggestions/pre_processing/test_prefix_based.py
+++ b/tests/code_suggestions/pre_processing/test_prefix_based.py
@@ -12,7 +12,6 @@ from ai_gateway.code_suggestions.processing.pre import (
     PromptBuilderPrefixBased,
     TokenizerTokenStrategy,
 )
-from ai_gateway.models.base_chat import Message, Role
 from ai_gateway.prompts import PromptTemplate
 
 # This template takes 4 tokens (ignore placeholders)
@@ -209,25 +208,6 @@ class TestPromptBuilderPrefixBased:
                     ),
                 ),
             ),
-            (
-                [
-                    Message(role=Role.SYSTEM, content="random_text"),
-                    Message(role=Role.USER, content="random_another_text"),
-                ],
-                1,
-                True,
-                Prompt(
-                    prefix=[
-                        Message(role=Role.SYSTEM, content="random_text"),
-                        Message(role=Role.USER, content="random_another_text"),
-                    ],
-                    metadata=MetadataPromptBuilder(
-                        components={
-                            "prompt": MetadataCodeContent(length=30, length_tokens=8)
-                        }
-                    ),
-                ),
-            ),
         ],
     )
     def test_with_prompt_wrapped(
diff --git a/tests/code_suggestions/pre_processing/test_tokens.py b/tests/code_suggestions/pre_processing/test_tokens.py
index f382d7a..d75c5be 100644
--- a/tests/code_suggestions/pre_processing/test_tokens.py
+++ b/tests/code_suggestions/pre_processing/test_tokens.py
@@ -36,18 +36,21 @@ class TestTokenizerTokenStrategy:
     @pytest.mark.parametrize(
         ("text", "expected_length"),
         [
-            ("random_text", [3]),
+            ("random_text", 3),
             (["random_text", "random"], [3, 1]),
             (["random_text", "random"], [3, 1]),
-            ("", [0]),
+            ("", 0),
             (["", ""], [0, 0]),
         ],
     )
     def test_estimate_length(
-        self, text: Union[str, list[str]], expected_length: list[int]
+        self, text: Union[str, list[str]], expected_length: Union[int, list[int]]
     ):
         strategy = TokenizerTokenStrategy(self.tokenizer)
 
-        actual = strategy.estimate_length(text)
+        if isinstance(text, str):
+            actual = strategy.estimate_length(text)
+        else:
+            actual = strategy.estimate_length(*text)
 
         assert actual == expected_length
diff --git a/tests/code_suggestions/test_authentication.py b/tests/code_suggestions/test_authentication.py
index ff44991..78de8ee 100644
--- a/tests/code_suggestions/test_authentication.py
+++ b/tests/code_suggestions/test_authentication.py
@@ -56,7 +56,6 @@ expected_log_keys = [
     "gitlab_global_user_id",
     "gitlab_host_name",
     "gitlab_saas_namespace_ids",
-    "gitlab_saas_duo_pro_namespace_ids",
 ]
 
 invalid_authentication_token_type_error = {
@@ -83,7 +82,7 @@ invalid_authentication_token_type_error = {
                 claims=UserClaims(scopes=["feature1", "feature3"]),
             ),
             {"error": "No authorization header presented"},
-            ["auth_error_details"],
+            [],
         ),
         (
             {"Authorization": "invalid"},
@@ -94,7 +93,7 @@ invalid_authentication_token_type_error = {
                 claims=UserClaims(scopes=["feature1", "feature3"]),
             ),
             {"error": "Invalid authorization header"},
-            ["auth_error_details"],
+            [],
         ),
         (
             {"Authorization": "Bearer 12345"},
@@ -105,7 +104,7 @@ invalid_authentication_token_type_error = {
                 claims=UserClaims(scopes=["feature1", "feature3"]),
             ),
             invalid_authentication_token_type_error,
-            ["auth_duration_s", "auth_error_details"],
+            ["auth_duration_s"],
         ),
         (
             {
@@ -236,7 +235,7 @@ invalid_authentication_token_type_error = {
                 claims=UserClaims(scopes=["feature1", "feature3"]),
             ),
             {"error": "Forbidden by auth provider"},
-            ["auth_duration_s", "auth_error_details"],
+            ["auth_duration_s"],
         ),
     ],
 )
diff --git a/tests/code_suggestions/test_completions.py b/tests/code_suggestions/test_completions.py
index 62da60d..099a740 100644
--- a/tests/code_suggestions/test_completions.py
+++ b/tests/code_suggestions/test_completions.py
@@ -1,6 +1,6 @@
 from contextlib import contextmanager
 from typing import Any, Type
-from unittest.mock import AsyncMock, Mock, call, patch
+from unittest.mock import AsyncMock, Mock, patch
 
 import pytest
 
@@ -10,12 +10,14 @@ from ai_gateway.code_suggestions.processing import (
     ModelEngineOutput,
 )
 from ai_gateway.code_suggestions.processing.post.completions import PostProcessor
-from ai_gateway.code_suggestions.processing.pre import PromptBuilderPrefixBased
+from ai_gateway.code_suggestions.processing.pre import (
+    PromptBuilderPrefixBased,
+    TokenStrategyBase,
+)
 from ai_gateway.code_suggestions.processing.typing import (
     LanguageId,
     MetadataCodeContent,
     MetadataPromptBuilder,
-    TokenStrategyBase,
 )
 from ai_gateway.instrumentators import KnownMetrics, TextGenModelInstrumentator
 from ai_gateway.models import (
@@ -30,9 +32,6 @@ from ai_gateway.models import (
     TextGenModelChunk,
     TextGenModelOutput,
 )
-from ai_gateway.models.base import TokensConsumptionMetadata
-from ai_gateway.tracking.instrumentator import SnowplowInstrumentator
-from ai_gateway.tracking.snowplow import SnowplowEvent
 
 
 class InstrumentorMock(Mock):
@@ -91,9 +90,6 @@ class TestCodeCompletionsLegacy:
                     "suffix": MetadataCodeContent(length=10, length_tokens=2),
                 },
             ),
-            tokens_consumption_metadata=TokensConsumptionMetadata(
-                input_tokens=1, output_tokens=2
-            ),
         )
         engine = Mock(spec=ModelEngineCompletions)
         engine.generate = AsyncMock(return_value=engine_response)
@@ -108,9 +104,7 @@ class TestCodeCompletionsLegacy:
             "ai_gateway.code_suggestions.completions.benchmark"
         ) as mock_benchmark:
             use_case = CodeCompletionsLegacy(
-                engine=engine,
-                post_processor=post_processor_factory,
-                snowplow_instrumentator=Mock(spec=SnowplowInstrumentator),
+                engine=engine, post_processor=post_processor_factory
             )
             actual = await use_case.execute(
                 prefix=prefix,
@@ -175,9 +169,6 @@ class TestCodeCompletionsLegacy:
                     "suffix": MetadataCodeContent(length=10, length_tokens=2),
                 },
             ),
-            tokens_consumption_metadata=TokensConsumptionMetadata(
-                input_tokens=1, output_tokens=2
-            ),
         )
         engine = Mock(spec=ModelEngineCompletions)
         engine.generate = AsyncMock(return_value=engine_response)
@@ -196,11 +187,9 @@ class TestCodeCompletionsLegacy:
             "ai_gateway.code_suggestions.completions.benchmark"
         ) as mock_benchmark:
             use_case = CodeCompletionsLegacy(
-                engine=engine,
-                post_processor=post_processor_factory,
-                snowplow_instrumentator=Mock(spec=SnowplowInstrumentator),
+                engine=engine, post_processor=post_processor_factory
             )
-            await use_case.execute(
+            _ = await use_case.execute(
                 prefix=prefix,
                 suffix=suffix,
                 file_name=file_name,
@@ -210,70 +199,6 @@ class TestCodeCompletionsLegacy:
         mock_benchmark.assert_not_called()
         post_processor.process.assert_not_called()
 
-    async def test_snowplow_instrumentation(
-        self,
-    ):
-        snowplow_instrumentator_mock = Mock(spec=SnowplowInstrumentator)
-        expected_event_1 = SnowplowEvent(
-            context=None,
-            action="tokens_per_user_request_prompt",
-            label="code_completion",
-            value=1,
-        )
-
-        expected_event_2 = SnowplowEvent(
-            context=None,
-            action="tokens_per_user_request_response",
-            label="code_completion",
-            value=2,
-        )
-
-        engine_response = ModelEngineOutput(
-            text="",
-            score=0,
-            model=ModelMetadata(name="code-gecko", engine="vertex-ai"),
-            lang_id=LanguageId.PYTHON,
-            metadata=MetadataPromptBuilder(
-                components={
-                    "prefix": MetadataCodeContent(length=10, length_tokens=2),
-                    "suffix": MetadataCodeContent(length=10, length_tokens=2),
-                },
-            ),
-            tokens_consumption_metadata=TokensConsumptionMetadata(
-                input_tokens=1, output_tokens=2
-            ),
-        )
-        engine = Mock(spec=ModelEngineCompletions)
-        engine.generate = AsyncMock(return_value=engine_response)
-        engine.model = PalmCodeGeckoModel(
-            Mock(),
-            "gl",
-            "us-central-1",
-        )
-
-        post_processor = Mock(spec=PostProcessor)
-        post_processor_factory = Mock()
-        post_processor_factory.return_value = post_processor
-
-        with patch(
-            "ai_gateway.code_suggestions.completions.benchmark"
-        ) as mock_benchmark:
-            use_case = CodeCompletionsLegacy(
-                engine=engine,
-                post_processor=post_processor_factory,
-                snowplow_instrumentator=snowplow_instrumentator_mock,
-            )
-            _ = await use_case.execute(
-                prefix="random_prefix",
-                suffix="random_suffix",
-                file_name="file_name",
-                editor_lang="python",
-            )
-
-        snowplow_instrumentator_mock.watch.assert_has_calls(
-            [call(expected_event_1), call(expected_event_2)]
-        )
-
 
 @pytest.mark.asyncio
 class TestCodeCompletions:
diff --git a/tests/code_suggestions/test_engine.py b/tests/code_suggestions/test_engine.py
index 7b06d4d..497a512 100644
--- a/tests/code_suggestions/test_engine.py
+++ b/tests/code_suggestions/test_engine.py
@@ -13,7 +13,6 @@ from ai_gateway.code_suggestions.processing import (
     ModelEngineCompletions,
     ops,
 )
-from ai_gateway.code_suggestions.processing.pre import TokenizerTokenStrategy
 from ai_gateway.experimentation import ExperimentRegistry
 from ai_gateway.models import (
     ModelMetadata,
@@ -23,11 +22,8 @@ from ai_gateway.models import (
     VertexAPIConnectionError,
     VertexAPIStatusError,
 )
-from ai_gateway.models.base import TokensConsumptionMetadata
 
-tokenization_strategy = TokenizerTokenStrategy(
-    tokenizer=AutoTokenizer.from_pretrained("Salesforce/codegen2-16B")
-)
+tokenizer = AutoTokenizer.from_pretrained("Salesforce/codegen2-16B")
 
 
 class MockInstrumentor:
@@ -128,8 +124,7 @@ def _side_effect_with_suffix(
     def _fn(prompt: str, suffix: str):
         assert original_suffix.startswith(suffix)
         assert (
-            tokenization_strategy.estimate_length(prompt)[0]
-            + tokenization_strategy.estimate_length(suffix)[0]
+            token_length(prompt) + token_length(suffix)
             <= PalmCodeGenBaseModel.MAX_MODEL_LEN
         )
 
@@ -157,24 +152,6 @@ def _side_effect_with_imports(
     return _fn
 
 
-def _side_effect_with_tokens_consumption_metadata(
-    content: str,
-    suffix: str,
-    filename: str,
-    model_output: str,
-    safety_attributes: SafetyAttributes,
-):
-    def _fn(prompt: str, suffix: str):
-        return TextGenModelOutput(
-            text=model_output,
-            score=-1,
-            safety_attributes=safety_attributes,
-            metadata=TokensConsumptionMetadata(input_tokens=1, output_tokens=2),
-        )
-
-    return _fn
-
-
 def _side_effect_with_connection_exception(
     content: str,
     suffix: str,
@@ -203,11 +180,14 @@ def _side_effect_with_status_exception(
     return _fn
 
 
+def token_length(s: str):
+    return len(tokenizer(s)["input_ids"])
+
+
 @pytest.mark.asyncio
 @pytest.mark.parametrize(
     "prefix,suffix,file_name,editor_language,model_gen_func,successful_predict,model_output,safety_attributes,"
-    "language,prompt_builder_metadata,expected_completion,expected_prompt_symbol_counts,expected_safety_attributes,"
-    "expected_input_tokens,expected_output_tokens,estimate_tokens_consumption",
+    "language,prompt_builder_metadata,expected_completion,expected_prompt_symbol_counts,expected_safety_attributes",
     [
         (
             "prompt",
@@ -238,42 +218,6 @@ def _side_effect_with_status_exception(
             "random completion",
             None,
             SafetyAttributes(),
-            None,
-            None,
-            True,
-        ),
-        (
-            "prompt",
-            "",
-            "f.unk",
-            None,
-            _side_effect_with_tokens_consumption_metadata,
-            True,
-            "random completion",
-            SafetyAttributes(),
-            None,
-            MetadataPromptBuilder(
-                components={
-                    "prefix": MetadataCodeContent(length=6, length_tokens=2),
-                    "suffix": MetadataCodeContent(length=0, length_tokens=0),
-                },
-                imports=MetadataExtraInfo(
-                    name="imports",
-                    pre=MetadataCodeContent(length=0, length_tokens=0),
-                    post=MetadataCodeContent(length=0, length_tokens=0),
-                ),
-                function_signatures=MetadataExtraInfo(
-                    name="function_signatures",
-                    pre=MetadataCodeContent(length=0, length_tokens=0),
-                    post=MetadataCodeContent(length=0, length_tokens=0),
-                ),
-            ),
-            "random completion",
-            None,
-            SafetyAttributes(),
-            1,
-            2,
-            False,
         ),
         (
             "prompt",
@@ -304,42 +248,6 @@ def _side_effect_with_status_exception(
             "random completion",
             None,
             SafetyAttributes(),
-            None,
-            None,
-            False,
-        ),
-        (
-            "prompt",
-            "",
-            "f.unk",
-            None,
-            _side_effect_unknown_tpl_palm,
-            True,
-            "random completion",
-            SafetyAttributes(),
-            None,
-            MetadataPromptBuilder(
-                components={
-                    "prefix": MetadataCodeContent(length=6, length_tokens=2),
-                    "suffix": MetadataCodeContent(length=0, length_tokens=0),
-                },
-                imports=MetadataExtraInfo(
-                    name="imports",
-                    pre=MetadataCodeContent(length=0, length_tokens=0),
-                    post=MetadataCodeContent(length=0, length_tokens=0),
-                ),
-                function_signatures=MetadataExtraInfo(
-                    name="function_signatures",
-                    pre=MetadataCodeContent(length=0, length_tokens=0),
-                    post=MetadataCodeContent(length=0, length_tokens=0),
-                ),
-            ),
-            "random completion",
-            None,
-            SafetyAttributes(),
-            None,
-            None,
-            False,
         ),
         (
             "prompt",
@@ -370,9 +278,6 @@ def _side_effect_with_status_exception(
             "random completion",
             {"comment": 1},
             SafetyAttributes(),
-            None,
-            None,
-            False,
         ),
         (
             "prompt",
@@ -403,9 +308,6 @@ def _side_effect_with_status_exception(
             "random completion\nnew line",
             None,
             SafetyAttributes(),
-            None,
-            None,
-            False,
         ),
         (
             "prompt " * 2048,
@@ -436,9 +338,6 @@ def _side_effect_with_status_exception(
             "random completion\nnew line",
             {"comment": 1},
             SafetyAttributes(),
-            None,
-            None,
-            False,
         ),
         (
             "prompt " * 2048,
@@ -459,9 +358,6 @@ def _side_effect_with_status_exception(
             "random completion\nnew line",
             None,
             SafetyAttributes(),
-            None,
-            None,
-            False,
         ),
         (
             "import os\nimport pytest\n" + "prompt" * 2048,
@@ -492,9 +388,6 @@ def _side_effect_with_status_exception(
             "random completion",
             {"comment": 1, "import_statement": 2},
             SafetyAttributes(),
-            None,
-            None,
-            False,
         ),
         (
             "random_prefix",
@@ -510,9 +403,6 @@ def _side_effect_with_status_exception(
             "",
             None,
             None,
-            None,
-            None,
-            False,
         ),
         (
             "random_prefix",
@@ -528,9 +418,6 @@ def _side_effect_with_status_exception(
             "",
             None,
             None,
-            None,
-            None,
-            False,
         ),
         (
             "",
@@ -546,9 +433,6 @@ def _side_effect_with_status_exception(
             "",
             {"comment": 1},
             SafetyAttributes(),
-            None,
-            None,
-            False,
         ),
     ],
 )
@@ -567,9 +451,6 @@ async def test_model_engine_palm(
     expected_completion,
     expected_prompt_symbol_counts,
     expected_safety_attributes,
-    expected_input_tokens,
-    expected_output_tokens,
-    estimate_tokens_consumption,
 ):
     model_name = "palm-model"
     model_engine = "vertex-ai"
@@ -583,7 +464,7 @@ async def test_model_engine_palm(
 
     engine = ModelEngineCompletions(
         model=text_gen_base_model,
-        tokenization_strategy=tokenization_strategy,
+        tokenizer=tokenizer,
         experiment_registry=ExperimentRegistry(),
     )
     engine.instrumentator = MockInstrumentor()
@@ -613,23 +494,6 @@ async def test_model_engine_palm(
         max_prefix_len = body_len - components["suffix"].length_tokens
         assert 0 <= components["prefix"].length_tokens <= max_prefix_len
 
-    if estimate_tokens_consumption:
-        assert completion.tokens_consumption_metadata.output_tokens == (
-            tokenization_strategy.estimate_length(model_output)[0]
-        )
-        assert completion.tokens_consumption_metadata.input_tokens == (
-            completion.metadata.components["prefix"].length_tokens
-            + completion.metadata.components["suffix"].length_tokens
-        )
-
-    if expected_input_tokens is not None and expected_output_tokens is not None:
-        assert completion.tokens_consumption_metadata.output_tokens == (
-            expected_output_tokens
-        )
-        assert completion.tokens_consumption_metadata.input_tokens == (
-            expected_input_tokens
-        )
-
     if not prefix and completion.metadata:
         assert 0 == completion.metadata.components["prefix"].length
         assert 0 == completion.metadata.components["suffix"].length
@@ -765,7 +629,7 @@ async def test_prompt_building_model_engine_palm(
 ):
     engine = ModelEngineCompletions(
         model=text_gen_base_model,
-        tokenization_strategy=tokenization_strategy,
+        tokenizer=tokenizer,
         experiment_registry=ExperimentRegistry(),
     )
     prompt = await engine._build_prompt(
diff --git a/tests/code_suggestions/test_generation.py b/tests/code_suggestions/test_generation.py
index 0d4eebc..1268623 100644
--- a/tests/code_suggestions/test_generation.py
+++ b/tests/code_suggestions/test_generation.py
@@ -1,21 +1,18 @@
 from contextlib import contextmanager
 from typing import Any, AsyncIterator
-from unittest.mock import AsyncMock, Mock, call, patch
+from unittest.mock import AsyncMock, Mock, patch
 
 import pytest
-from snowplow_tracker import Snowplow
 
 from ai_gateway.code_suggestions import CodeGenerations, ModelProvider
-from ai_gateway.code_suggestions.processing import LanguageId, TokenStrategyBase
+from ai_gateway.code_suggestions.processing import LanguageId
 from ai_gateway.code_suggestions.processing.post.generations import (
     PostProcessor,
     PostProcessorAnthropic,
 )
-from ai_gateway.code_suggestions.processing.pre import PromptBuilderBase
-from ai_gateway.code_suggestions.processing.typing import (
-    MetadataCodeContent,
-    MetadataPromptBuilder,
-    Prompt,
+from ai_gateway.code_suggestions.processing.pre import (
+    PromptBuilderBase,
+    TokenStrategyBase,
 )
 from ai_gateway.instrumentators import TextGenModelInstrumentator
 from ai_gateway.models import (
@@ -24,8 +21,6 @@ from ai_gateway.models import (
     TextGenModelChunk,
     TextGenModelOutput,
 )
-from ai_gateway.tracking.instrumentator import SnowplowInstrumentator
-from ai_gateway.tracking.snowplow import SnowplowEvent
 
 
 class InstrumentorMock(Mock):
@@ -41,36 +36,14 @@ class InstrumentorMock(Mock):
 
 @pytest.mark.asyncio
 class TestCodeGeneration:
-    def cleanup(self):
-        """Ensure Snowplow cache is reset between tests."""
-        yield
-        Snowplow.reset()
-
     @pytest.fixture(scope="class")
     def use_case(self):
         model = Mock(spec=TextGenBaseModel)
         model.MAX_MODEL_LEN = 2048
-        tokenization_strategy_mock = Mock(spec=TokenStrategyBase)
-        tokenization_strategy_mock.estimate_length = Mock(return_value=[1, 2])
-        prompt_builder_mock = Mock(spec=PromptBuilderBase)
-        prompt = Prompt(
-            prefix="prompt",
-            metadata=MetadataPromptBuilder(
-                components={
-                    "prompt": MetadataCodeContent(
-                        length=len("prompt"),
-                        length_tokens=1,
-                    ),
-                }
-            ),
-        )
-        prompt_builder_mock.build = Mock(return_value=prompt)
 
-        use_case = CodeGenerations(
-            model, tokenization_strategy_mock, Mock(spec=SnowplowInstrumentator)
-        )
+        use_case = CodeGenerations(model, Mock(spec=TokenStrategyBase))
         use_case.instrumentator = InstrumentorMock(spec=TextGenModelInstrumentator)
-        use_case.prompt_builder = prompt_builder_mock
+        use_case.prompt_builder = Mock(spec=PromptBuilderBase)
 
         yield use_case
 
@@ -151,73 +124,3 @@ class TestCodeGeneration:
             "",
             stream=True,
         )
-
-    @pytest.mark.parametrize(
-        ("stream", "response_token_length"),
-        [
-            (True, 9),
-            (False, 4),
-        ],
-    )
-    async def test_snowplow_instrumentation(
-        self,
-        use_case: CodeGenerations,
-        stream: bool,
-        response_token_length: int,
-    ):
-        async def _stream_generator(
-            prefix: str, suffix: str, stream: bool
-        ) -> AsyncIterator[TextGenModelChunk]:
-            model_chunks = [
-                TextGenModelChunk(text="hello "),
-                TextGenModelChunk(text="world!"),
-            ]
-
-            for chunk in model_chunks:
-                yield chunk
-
-        expected_event_1 = SnowplowEvent(
-            context=None,
-            action="tokens_per_user_request_prompt",
-            label="code_generation",
-            value=1,
-        )
-
-        expected_event_2 = SnowplowEvent(
-            context=None,
-            action="tokens_per_user_request_response",
-            label="code_generation",
-            value=response_token_length,
-        )
-
-        with patch.object(use_case, "tokenization_strategy") as mock, patch.object(
-            use_case, "snowplow_instrumentator"
-        ) as snowplow_mock:
-            mock.estimate_length = Mock(return_value=[4, 5])
-
-            if stream:
-                use_case.model.generate = AsyncMock(side_effect=_stream_generator)
-            else:
-                use_case.model.generate = AsyncMock(
-                    return_value=TextGenModelOutput(
-                        text="output", score=0, safety_attributes=SafetyAttributes()
-                    )
-                )
-
-            actual = await use_case.execute(
-                prefix="any",
-                file_name="bar.py",
-                editor_lang=LanguageId.PYTHON,
-                model_provider=ModelProvider.ANTHROPIC,
-                stream=stream,
-            )
-
-            if stream:
-                async for _ in actual:
-                    _
-
-            mock.estimate_length.assert_called
-
-            snowplow_mock.watch.assert_has_calls(
-                [call(expected_event_1), call(expected_event_2)]
-            )
diff --git a/tests/code_suggestions/tracking/test_snowplow.py b/tests/code_suggestions/tracking/test_snowplow.py
index 99b7872..920baef 100644
--- a/tests/code_suggestions/tracking/test_snowplow.py
+++ b/tests/code_suggestions/tracking/test_snowplow.py
@@ -2,8 +2,9 @@ from dataclasses import asdict
 from unittest import mock
 
 import pytest
-from snowplow_tracker import SelfDescribingJson, Snowplow, StructuredEvent
+from snowplow_tracker import Snowplow
 
+from ai_gateway.instrumentators.base import Telemetry
 from ai_gateway.tracking import (
     RequestCount,
     SnowplowClient,
@@ -49,69 +50,54 @@ class TestSnowplowClient:
         assert tracker_args["namespace"] == configuration.namespace
         assert len(tracker_args["emitters"]) == 1
 
-    @pytest.mark.parametrize(
-        ("inputs"),
-        [
-            (
-                SnowplowEvent(
-                    context=None,
-                    category="code_suggestions",
-                    action="suggestion_requested",
-                    label="some label",
-                    value=1,
-                )
-            ),
-            (
-                SnowplowEvent(
-                    category="code_suggestions",
-                    action="suggestion_requested",
-                    label="some label",
-                    value=1,
-                    context=SnowplowEventContext(
-                        request_counts=[],
-                        prefix_length=2048,
-                        suffix_length=1024,
-                        language="python",
-                        user_agent="vs-code-gitlab-workflow",
-                        gitlab_realm="saas",
-                        gitlab_instance_id="ABCDEF",
-                        gitlab_global_user_id="123XYZ",
-                        gitlab_host_name="gitlab.com",
-                        gitlab_saas_namespace_ids=[12345],
-                        gitlab_saas_duo_pro_namespace_ids=[54321],
-                    ),
+    @mock.patch("snowplow_tracker.events.StructuredEvent.__init__")
+    @mock.patch("snowplow_tracker.Tracker.track")
+    def test_track(self, mock_track, mock_structured_event_init):
+        mock_structured_event_init.return_value = None
+
+        configuration = SnowplowClientConfiguration(
+            namespace="gl",
+            endpoint="https://whitechoc.local",
+            app_id="gitlab_ai_gateway",
+        )
+        context = SnowplowEventContext(
+            request_counts=[
+                RequestCount(
+                    requests=1,
+                    errors=0,
+                    accepts=1,
+                    lang="python",
+                    model_engine="vertex-ai",
+                    model_name="code-gecko",
                 )
-            ),
-        ],
-    )
-    def test_track(self, inputs):
-        with mock.patch("snowplow_tracker.Tracker.track") as mock_track, mock.patch(
-            "snowplow_tracker.events.StructuredEvent.__init__"
-        ) as mock_structured_event_init:
-            configuration = SnowplowClientConfiguration(
-                namespace="gl",
-                endpoint="https://whitechoc.local",
-                app_id="gitlab_ai_gateway",
-            )
-            mock_structured_event_init.return_value = None
-            SnowplowClient(configuration).track(event=inputs)
-
-            mock_track.assert_called_once()
-            mock_structured_event_init.assert_called_once()
-            event_init_args = mock_structured_event_init.call_args[1]
-
-            assert event_init_args["value"] == inputs.value
-            assert event_init_args["category"] == inputs.category
-            assert event_init_args["action"] == inputs.action
-            assert event_init_args["label"] == inputs.label
-
-            if inputs.context is None:
-                return
-
-            context = event_init_args["context"][0]
-            assert isinstance(context, SelfDescribingJson)
-            assert context.to_json()["schema"] == SnowplowClient.SCHEMA
-            assert context.to_json()["data"] == asdict(inputs.context)
+            ],
+            prefix_length=2048,
+            suffix_length=1024,
+            language="python",
+            user_agent="vs-code-gitlab-workflow",
+            gitlab_realm="saas",
+            gitlab_instance_id="ABCDEF",
+            gitlab_global_user_id="123XYZ",
+            gitlab_host_name="gitlab.com",
+            gitlab_saas_namespace_ids=[12345],
+        )
+        event = SnowplowEvent(
+            context=context,
+            category="code_suggestions",
+            action="suggestion_requested",
+        )
+        SnowplowClient(configuration).track(event)
+
+        mock_structured_event_init.assert_called_once()
+
+        init_args = mock_structured_event_init.call_args[1]
+        assert init_args["category"] == event.category
+        assert init_args["action"] == event.action
+
+        context_data = init_args["context"][0].to_json()["data"]
+        assert context_data == asdict(event.context)
+
+        mock_track.assert_called_once()
 
 
 class TestSnowplowInstrumentator:
@@ -128,29 +114,90 @@ class TestSnowplowInstrumentator:
         ),
         [
             (
-                SnowplowEvent(
-                    context=None,
-                    category="code_suggestions",
-                    action="suggestion_requested",
-                    label="some label",
-                    value=1,
-                ),
-                SnowplowEvent(
-                    context=None,
-                    category="code_suggestions",
-                    action="suggestion_requested",
-                    label="some label",
-                    value=1,
-                ),
-            )
+                {
+                    "prefix_length": 11,
+                    "suffix_length": 22,
+                    "language": "ruby",
+                    "user_agent": "vs-code",
+                    "gitlab_realm": "saas",
+                    "gitlab_instance_id": "9ebada7a-f5e2-477a-8609-17797fa95cb9",
+                    "gitlab_global_user_id": "XTuMnZ6XTWkP3yh0ZwXualmOZvm2Gg/bk9jyfkL7Y6k=",
+                    "gitlab_host_name": "gitlab.com",
+                    "gitlab_saas_namespace_ids": ["12345"],
+                },
+                {
+                    "prefix_length": 11,
+                    "suffix_length": 22,
+                    "language": "ruby",
+                    "user_agent": "vs-code",
+                    "gitlab_realm": "saas",
+                    "gitlab_instance_id": "9ebada7a-f5e2-477a-8609-17797fa95cb9",
+                    "gitlab_global_user_id": "XTuMnZ6XTWkP3yh0ZwXualmOZvm2Gg/bk9jyfkL7Y6k=",
+                    "gitlab_host_name": "gitlab.com",
+                    "gitlab_saas_namespace_ids": [12345],
+                },
+            ),
+            (
+                {
+                    "prefix_length": 33,
+                    "suffix_length": 44,
+                    "language": "python",
+                    "user_agent": "web-ide",
+                    "gitlab_realm": "saas",
+                    "gitlab_instance_id": "test",
+                    "gitlab_global_user_id": "test",
+                    "gitlab_host_name": "gitlab.com",
+                    "gitlab_saas_namespace_ids": ["345"],
+                },
+                {
+                    "prefix_length": 33,
+                    "suffix_length": 44,
+                    "language": "python",
+                    "user_agent": "web-ide",
+                    "gitlab_realm": "saas",
+                    "gitlab_instance_id": "test",
+                    "gitlab_global_user_id": "test",
+                    "gitlab_host_name": "gitlab.com",
+                    "gitlab_saas_namespace_ids": [345],
+                },
+            ),
         ],
     )
     def test_watch(self, inputs, expectations):
         mock_client = mock.Mock(spec=SnowplowClient)
         instrumentator = SnowplowInstrumentator(client=mock_client)
 
-        instrumentator.watch(inputs)
+        telemetry_1 = Telemetry(
+            requests=1,
+            accepts=2,
+            errors=3,
+            lang="python",
+            model_engine="vertex",
+            model_name="code-gecko",
+        )
+        telemetry_2 = Telemetry(
+            requests=4,
+            accepts=5,
+            errors=6,
+            lang="golang",
+            model_engine="vertex",
+            model_name="text-bison",
+        )
+
+        test_telemetry = [telemetry_1, telemetry_2]
+
+        instrumentator.watch(telemetry=test_telemetry, **inputs)
 
         mock_client.track.assert_called_once()
 
-        assert mock_client.track.call_args[0][0] == expectations
+        event = mock_client.track.call_args[0][0].context
+
+        assert len(event.request_counts) == 2
+
+        del telemetry_1.__dict__["experiments"]
+        del telemetry_2.__dict__["experiments"]
+        assert event.request_counts[0].__dict__ == telemetry_1.__dict__
+        assert event.request_counts[1].__dict__ == telemetry_2.__dict__
+
+        for k, v in expectations.items():
+            assert getattr(event, k) == v
diff --git a/tests/lints/test_unsafe_dependency_resolution.py b/tests/lints/test_unsafe_dependency_resolution.py
deleted file mode 100644
index 727eb47..0000000
--- a/tests/lints/test_unsafe_dependency_resolution.py
+++ /dev/null
@@ -1,43 +0,0 @@
-import astroid
-import pylint.testutils
-
-from lints import unsafe_dependency_resolution
-
-
-class TestUnsafeDependencyResolution(pylint.testutils.CheckerTestCase):
-    CHECKER_CLASS = unsafe_dependency_resolution.UnsafeDependencyResolution
-
-    def test_finds_unsafe_dependency_resolution(self):
-        node = astroid.extract_node(
-            """
-        Factory[SomeModel] = Depends(Provide[Container.some_factory.provider])
-        """
-        )
-
-        with self.assertAddsMessages(
-            pylint.testutils.MessageTest(
-                msg_id="unsafe-dependency-resolution", node=node.value
-            ),
-            ignore_position=True,
-        ):
-            self.checker.visit_call(node.value)
-
-    def test_ignores_depends_async_def_method(self):
-        node = astroid.extract_node(
-            """
-        Factory[SomeModel] = Depends(async_def_method)
-        """
-        )
-
-        with self.assertNoMessages():
-            self.checker.visit_call(node.value)
-
-    def test_ignores_provide_without_depends(self):
-        node = astroid.extract_node(
-            """
-        Factory[SomeModel] = Provide[Container.some_factory.provider]
-        """
-        )
-
-        with self.assertNoMessages():
-            self.checker.visit_call(node.value)
diff --git a/tests/models/test_container.py b/tests/models/test_container.py
deleted file mode 100644
index cfe3409..0000000
--- a/tests/models/test_container.py
+++ /dev/null
@@ -1,51 +0,0 @@
-import os
-from unittest.mock import AsyncMock, Mock, patch
-
-import pytest
-
-from ai_gateway.models.container import _init_vertex_grpc_client
-
-
-@pytest.mark.parametrize(
-    ("args", "expected_init", "expected_key_to_file"),
-    [
-        (
-            {"endpoint": "test", "json_key": "", "mock_model_responses": False},
-            True,
-            False,
-        ),
-        (
-            {
-                "endpoint": "test",
-                "json_key": '{ "type": "service_account" }',
-                "mock_model_responses": False,
-            },
-            True,
-            True,
-        ),
-        (
-            {"endpoint": "test", "json_key": "", "mock_model_responses": True},
-            False,
-            False,
-        ),
-    ],
-)
-def test_init_vertex_grpc_client(args, expected_init, expected_key_to_file):
-    with patch(
-        # "google.cloud.aiplatform.gapic.PredictionServiceAsyncClient"
-        "ai_gateway.models.container.grpc_connect_vertex"
-    ) as mock_grpc_client:
-        next(_init_vertex_grpc_client(**args))
-
-        if expected_init:
-            mock_grpc_client.assert_called_once_with({"api_endpoint": args["endpoint"]})
-        else:
-            mock_grpc_client.assert_not_called()
-
-        if expected_key_to_file:
-            with open("/tmp/vertex-client.json", "r") as f:
-                assert f.read() == args["json_key"]
-            assert (
-                os.environ["GOOGLE_APPLICATION_CREDENTIALS"]
-                == "/tmp/vertex-client.json"
-            )
diff --git a/tests/test_app.py b/tests/test_app.py
deleted file mode 100644
index 16925cf..0000000
--- a/tests/test_app.py
+++ /dev/null
@@ -1,15 +0,0 @@
-from fastapi import FastAPI
-
-from ai_gateway.app import get_app, get_config
-from ai_gateway.config import Config, ConfigFastApi
-
-
-def test_get_config():
-    config = get_config()
-    assert isinstance(config, Config)
-    assert isinstance(config.fastapi, ConfigFastApi)
-
-
-def test_get_app():
-    app = get_app()
-    assert isinstance(app, FastAPI)
diff --git a/tests/test_config.py b/tests/test_config.py
index 3305e18..f9cabcb 100644
--- a/tests/test_config.py
+++ b/tests/test_config.py
@@ -25,50 +25,29 @@ from ai_gateway.config import (
                 "AIGW_GITLAB_URL": "http://gitlab.test",
                 "AIGW_GITLAB_API_URL": "http://api.gitlab.test",
                 "AIGW_CUSTOMER_PORTAL_URL": "http://customer.gitlab.test",
-                "AIGW_MOCK_MODEL_RESPONSES": "true",
+                "AIGW_USE_FAKE_MODELS": "yes",
             },
             Config(
                 gitlab_url="http://gitlab.test",
                 gitlab_api_url="http://api.gitlab.test",
                 customer_portal_url="http://customer.gitlab.test",
-                # pydantic-settings does not allow omitting the prefix if validation_alias is set for the field
-                aigw_mock_model_responses=True,
+                use_fake_models=True,
             ),
         ),
     ],
 )
 def test_config_base(values: dict, expected: Config):
     with mock.patch.dict(os.environ, values, clear=True):
-        config = Config(_env_file=None, _env_prefix="AIGW_")
+        config = Config(_env_file=None)
 
-        keys = {
+        keys = [
             "gitlab_url",
             "gitlab_api_url",
             "customer_portal_url",
-            "mock_model_responses",
-        }
-
-        actual = config.model_dump(include=keys)
-        assert actual == expected.model_dump(include=keys)
-        assert len(actual) == len(keys)
-
-
-@pytest.mark.parametrize(
-    ("values", "expected"),
-    [
-        # pydantic-settings does not allow omitting the prefix if validation_alias is set for the field
-        ({"AIGW_MOCK_MODEL_RESPONSES": "true"}, Config(aigw_mock_model_responses=True)),
-        (
-            {"AIGW_USE_FAKE_MODELS": "true"},
-            Config(aigw_mock_model_responses=True),
-        ),
-    ],
-)
-def test_mock_model_responses_b_compatibility(values: dict, expected: Config):
-    with mock.patch.dict(os.environ, values, clear=True):
-        config = Config(_env_file=None)
+            "use_fake_models",
+        ]
 
-        assert config.mock_model_responses == expected.mock_model_responses
+        assert config.model_dump(include=keys) == expected.model_dump(include=keys)
 
 
 @pytest.mark.parametrize(
@@ -106,7 +85,6 @@ def test_config_logging(values: dict, expected: ConfigLogging):
                 "AIGW_FASTAPI__DOCS_URL": "docs.test",
                 "AIGW_FASTAPI__OPENAPI_URL": "openapi.test",
                 "AIGW_FASTAPI__REDOC_URL": "redoc.test",
-                "AIGW_FASTAPI__RELOAD": "True",
             },
             ConfigFastApi(
                 api_host="localhost",
@@ -117,7 +95,6 @@ def test_config_logging(values: dict, expected: ConfigLogging):
                 docs_url="docs.test",
                 openapi_url="openapi.test",
                 redoc_url="redoc.test",
-                reload=True,
             ),
         ),
     ],
@@ -238,13 +215,9 @@ def test_config_instrumentator(values: dict, expected: ConfigInstrumentator):
                 "AIGW_VERTEX_TEXT_MODEL__PROJECT": "project",
                 "AIGW_VERTEX_TEXT_MODEL__LOCATION": "location",
                 "AIGW_VERTEX_TEXT_MODEL__ENDPOINT": "endpoint",
-                "AIGW_VERTEX_TEXT_MODEL__JSON_KEY": "secret",
             },
             ConfigVertexTextModel(
-                project="project",
-                location="location",
-                endpoint="endpoint",
-                json_key="secret",
+                project="project", location="location", endpoint="endpoint"
             ),
         ),
     ],
